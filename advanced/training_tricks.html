


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="ko" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="ko" > <!--<![endif]-->
<head>
  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-5SCNQBF5');
  </script>
  <!-- End Google Tag Manager -->

  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="google-site-verification" content="okUst94cAlWSsUsGZTB4xSS4UKTtRV8Nu5XZ9pdd3Aw" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Effective Training Techniques &mdash; PyTorch Lightning &amp; PyTorch Korea User Group 2.0.5 문서</title>
  

  
  
    <link rel="shortcut icon" href="../_static/icon.svg"/>
  
  
  
    <link rel="canonical" href="https://lightning.ai/docs/pytorch/stable//advanced/training_tricks.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../_static/togglebutton.css" type="text/css" />
  <link rel="stylesheet" href="../_static/main.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sphinx_paramlinks.css" type="text/css" />
    <link rel="index" title="색인" href="../genindex.html" />
    <link rel="search" title="검색" href="../search.html" />
    <link rel="next" title="Run on an on-prem cluster (advanced)" href="../clouds/cluster_advanced.html" />
    <link rel="prev" title="Style Guide" href="../starter/style_guide.html" />
  <!-- Google Analytics -->
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-82W25RV60Q"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-82W25RV60Q');
    </script>
  
  <!-- End Google Analytics -->
  

  
  <script src="../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/UCity/UCity-Light.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/UCity/UCity-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/UCity/UCity-Semibold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/Inconsolata/Inconsolata.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <script defer src="https://use.fontawesome.com/releases/v6.1.1/js/all.js" integrity="sha384-xBXmu0dk1bEoiwd71wOonQLyH+VpgR1XcDH3rtxrLww5ajNTuMvBdL5SOiFZnNdp" crossorigin="anonymous"></script>

  <script src="https://unpkg.com/react@18/umd/react.development.js" crossorigin></script>
  <script src="https://unpkg.com/react-dom@18/umd/react-dom.development.js" crossorigin></script>
  <script src="https://unpkg.com/babel-standalone@6/babel.min.js"></script>
  <script src="../_static/js/react/react.jsx" type="text/babel"></script>
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://lightning.ai/docs/pytorch/latest/" aria-label="PyTorch Lightning">
      <!--  <img class="call-to-action-img" src="../_static/images/logo-lightning-icon.png"/> -->
      </a>

      <div class="main-menu">
        <ul>
          <!-- <li>
            <a href="https://lightning.ai/docs/pytorch/latest/starter/introduction.html">Get Started</a>
          </li>

          <li>
            <a href="https://lightning.ai/pages/blog/">Blog</a>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning.ai/docs/pytorch/stable/">
                  <span class="dropdown-title">PyTorch Lightning</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning.ai/docs/fabric/stable/">
                  <span class="dropdown-title">Lightning Fabric</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://torchmetrics.readthedocs.io/en/stable/">
                  <span class="dropdown-title">TorchMetrics</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning-flash.readthedocs.io/en/stable/">
                  <span class="dropdown-title">Lightning Flash</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning-bolts.readthedocs.io/en/stable/">
                  <span class="dropdown-title">Lightning Bolts</span>
                </a>
            </div>
          </li> -->

          <!--<li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://www.pytorchlightning.ai/community">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://lightning.ai/docs/pytorch/latest/#community-examples">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://lightning.ai/forums/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>-->

          <!-- 
          <li>
            <a href="https://lightning.ai/docs/pytorch/latest/past_versions.html">Previous Versions</a>
          </li>
          

          <li>
            <a href="https://github.com/Lightning-AI/lightning">GitHub</a>
          </li> -->

          <li>
            <a href="https://www.lightning.ai/">Lightning AI</a>
          </li>

          <li>
            <a href="https://pytorch.kr/">파이토치 한국 사용자 모임</a>
          </li>

          <li>
            <a href="https://discuss.pytorch.kr/">파이토치 한국어 커뮤니티</a>
          </li>

        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  2.0.5
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Home</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../starter/introduction.html">Lightning in 15 minutes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../starter/installation.html">Install</a></li>
<li class="toctree-l1"><a class="reference internal" href="../upgrade/migration_guide.html">Guide how to upgrade to the 2.0 version</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Level Up</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../levels/core_skills.html">Basic skills</a></li>
<li class="toctree-l1"><a class="reference internal" href="../levels/intermediate.html">Intermediate skills</a></li>
<li class="toctree-l1"><a class="reference internal" href="../levels/advanced.html">Advanced skills</a></li>
<li class="toctree-l1"><a class="reference internal" href="../levels/expert.html">Expert skills</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Core API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../common/lightning_module.html">LightningModule</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/trainer.html">Trainer</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Optional API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_references.html">accelerators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_references.html#callbacks">callbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_references.html#cli">cli</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_references.html#core">core</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_references.html#loggers">loggers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_references.html#profiler">profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_references.html#trainer">trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_references.html#strategies">strategies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_references.html#tuner">tuner</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_references.html#utilities">utilities</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">More</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../community/index.html">Community</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary/index.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/index.html">How-to Guides</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../glossary/index.html">Glossary</a> &gt;</li>
        
      <li>Effective Training Techniques</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/advanced/training_tricks.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <section id="effective-training-techniques">
<span id="training-tricks"></span><h1>Effective Training Techniques<a class="headerlink" href="#effective-training-techniques" title="이 표제에 대한 퍼머링크">¶</a></h1>
<p>Lightning implements various techniques to help during training that can help make the training smoother.</p>
<hr class="docutils" />
<section id="accumulate-gradients">
<h2>Accumulate Gradients<a class="headerlink" href="#accumulate-gradients" title="이 표제에 대한 퍼머링크">¶</a></h2>
<p>Accumulated gradients run K small batches of size <code class="docutils literal notranslate"><span class="pre">N</span></code> before doing a backward pass. The effect is a large effective batch size of size <code class="docutils literal notranslate"><span class="pre">KxN</span></code>, where <code class="docutils literal notranslate"><span class="pre">N</span></code> is the batch size.
Internally it doesn’t stack up the batches and do a forward pass rather it accumulates the gradients for K batches and then do an <code class="docutils literal notranslate"><span class="pre">optimizer.step</span></code> to make sure the
effective batch size is increased but there is no memory overhead.</p>
<div class="admonition warning">
<p class="admonition-title">경고</p>
<p>When using distributed training for eg. DDP, with let’s say with <code class="docutils literal notranslate"><span class="pre">P</span></code> devices, each device accumulates independently i.e. it stores the gradients
after each <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code> and doesn’t sync the gradients across the devices until we call <code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code>. So for each accumulation
step, the effective batch size on each device will remain <code class="docutils literal notranslate"><span class="pre">N*K</span></code> but right before the <code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code>, the gradient sync will make the effective
batch size as <code class="docutils literal notranslate"><span class="pre">P*N*K</span></code>. For DP, since the batch is split across devices, the final effective batch size will be <code class="docutils literal notranslate"><span class="pre">N*K</span></code>.</p>
</div>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT (ie: no accumulated grads)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">accumulate_grad_batches</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Accumulate gradients for 7 batches</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">accumulate_grad_batches</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>
</pre></div>
</div>
<p>Optionally, you can make the <code class="docutils literal notranslate"><span class="pre">accumulate_grad_batches</span></code> value change over time by using the <a class="reference internal" href="../api/lightning.pytorch.callbacks.GradientAccumulationScheduler.html#lightning.pytorch.callbacks.GradientAccumulationScheduler" title="lightning.pytorch.callbacks.gradient_accumulation_scheduler.GradientAccumulationScheduler"><code class="xref py py-class docutils literal notranslate"><span class="pre">GradientAccumulationScheduler</span></code></a>.
Pass in a scheduling dictionary, where the key represents the epoch at which the value for gradient accumulation should be updated.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">lightning.pytorch.callbacks</span> <span class="kn">import</span> <span class="n">GradientAccumulationScheduler</span>

<span class="c1"># till 5th epoch, it will accumulate every 8 batches. From 5th epoch</span>
<span class="c1"># till 9th epoch it will accumulate every 4 batches and after that no accumulation</span>
<span class="c1"># will happen. Note that you need to use zero-indexed epoch keys here</span>
<span class="n">accumulator</span> <span class="o">=</span> <span class="n">GradientAccumulationScheduler</span><span class="p">(</span><span class="n">scheduling</span><span class="o">=</span><span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">:</span> <span class="mi">1</span><span class="p">})</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">callbacks</span><span class="o">=</span><span class="n">accumulator</span><span class="p">)</span>
</pre></div>
</div>
<p>Note: Not all strategies and accelerators support variable gradient accumulation windows.</p>
</section>
<hr class="docutils" />
<section id="gradient-clipping">
<h2>Gradient Clipping<a class="headerlink" href="#gradient-clipping" title="이 표제에 대한 퍼머링크">¶</a></h2>
<p>Gradient clipping can be enabled to avoid exploding gradients. By default, this will clip the gradient norm by calling
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_" title="(PyTorch v2.0에서)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.utils.clip_grad_norm_()</span></code></a> computed over all model parameters together.
If the Trainer’s <code class="docutils literal notranslate"><span class="pre">gradient_clip_algorithm</span></code> is set to <code class="docutils literal notranslate"><span class="pre">'value'</span></code> (<code class="docutils literal notranslate"><span class="pre">'norm'</span></code> by default), this will use instead
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_value_.html#torch.nn.utils.clip_grad_value_" title="(PyTorch v2.0에서)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.utils.clip_grad_value_()</span></code></a> for each parameter instead.</p>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>If using mixed precision, the <code class="docutils literal notranslate"><span class="pre">gradient_clip_val</span></code> does not need to be changed as the gradients are unscaled
before applying the clipping function.</p>
</div>
<div class="admonition seealso">
<p class="admonition-title">더 보기</p>
<p><a class="reference internal" href="../api/lightning.pytorch.trainer.trainer.Trainer.html#lightning.pytorch.trainer.trainer.Trainer" title="lightning.pytorch.trainer.trainer.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a></p>
</div>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT (ie: don&#39;t clip)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">gradient_clip_val</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># clip gradients&#39; global norm to &lt;=0.5 using gradient_clip_algorithm=&#39;norm&#39; by default</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">gradient_clip_val</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># clip gradients&#39; maximum magnitude to &lt;=0.5</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">gradient_clip_val</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">gradient_clip_algorithm</span><span class="o">=</span><span class="s2">&quot;value&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Read more about <a class="reference internal" href="../common/optimization.html#configure-gradient-clipping"><span class="std std-ref">Configuring Gradient Clipping</span></a> for advanced use-cases.</p>
</section>
<hr class="docutils" />
<section id="stochastic-weight-averaging">
<h2>Stochastic Weight Averaging<a class="headerlink" href="#stochastic-weight-averaging" title="이 표제에 대한 퍼머링크">¶</a></h2>
<p>Stochastic Weight Averaging (SWA) can make your models generalize better at virtually no additional cost.
This can be used with both non-trained and trained models. The SWA procedure smooths the loss landscape thus making
it harder to end up in a local minimum during optimization.</p>
<p>For a more detailed explanation of SWA and how it works,
read <a class="reference external" href="https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging">this post</a> by the PyTorch team.</p>
<div class="admonition seealso">
<p class="admonition-title">더 보기</p>
<p>The <a class="reference internal" href="../api/lightning.pytorch.callbacks.StochasticWeightAveraging.html#lightning.pytorch.callbacks.StochasticWeightAveraging" title="lightning.pytorch.callbacks.StochasticWeightAveraging"><code class="xref py py-class docutils literal notranslate"><span class="pre">StochasticWeightAveraging</span></code></a> callback</p>
</div>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Enable Stochastic Weight Averaging using the callback</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">StochasticWeightAveraging</span><span class="p">(</span><span class="n">swa_lrs</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">)])</span>
</pre></div>
</div>
<hr class="docutils" />
</section>
<section id="batch-size-finder">
<span id="id1"></span><h2>Batch Size Finder<a class="headerlink" href="#batch-size-finder" title="이 표제에 대한 퍼머링크">¶</a></h2>
<p>Auto-scaling of batch size can be enabled to find the largest batch size that fits into
memory. Large batch size often yields a better estimation of the gradients, but may also result in
longer training time. Inspired by <a class="reference external" href="https://github.com/BlackHC/toma">https://github.com/BlackHC/toma</a>.</p>
<div class="admonition seealso">
<p class="admonition-title">더 보기</p>
<p><a class="reference internal" href="../api/lightning.pytorch.tuner.tuning.Tuner.html#lightning.pytorch.tuner.tuning.Tuner" title="lightning.pytorch.tuner.tuning.Tuner"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tuner</span></code></a></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">lightning.pytorch.tuner</span> <span class="kn">import</span> <span class="n">Tuner</span>

<span class="c1"># Create a tuner for the trainer</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">tuner</span> <span class="o">=</span> <span class="n">Tuner</span><span class="p">(</span><span class="n">trainer</span><span class="p">)</span>

<span class="c1"># Auto-scale batch size by growing it exponentially (default)</span>
<span class="n">tuner</span><span class="o">.</span><span class="n">scale_batch_size</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;power&quot;</span><span class="p">)</span>

<span class="c1"># Auto-scale batch size with binary search</span>
<span class="n">tuner</span><span class="o">.</span><span class="n">scale_batch_size</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;binsearch&quot;</span><span class="p">)</span>

<span class="c1"># Fit as normal with new batch size</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<p>Currently, this feature supports two modes <code class="docutils literal notranslate"><span class="pre">'power'</span></code> scaling and <code class="docutils literal notranslate"><span class="pre">'binsearch'</span></code>
scaling. In <code class="docutils literal notranslate"><span class="pre">'power'</span></code> scaling, starting from a batch size of 1 keeps doubling
the batch size until an out-of-memory (OOM) error is encountered. Setting the
argument to <code class="docutils literal notranslate"><span class="pre">'binsearch'</span></code> will initially also try doubling the batch size until
it encounters an OOM, after which it will do a binary search that will finetune the
batch size. Additionally, it should be noted that the batch size scaler cannot
search for batch sizes larger than the size of the training dataset.</p>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>This feature expects that a <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> field is either located as a model attribute
i.e. <code class="docutils literal notranslate"><span class="pre">model.batch_size</span></code> or as a field in your <code class="docutils literal notranslate"><span class="pre">hparams</span></code> i.e. <code class="docutils literal notranslate"><span class="pre">model.hparams.batch_size</span></code>.
Similarly it can work with datamodules too. The field should exist and will be updated by
the results of this algorithm. Additionally, your <code class="docutils literal notranslate"><span class="pre">train_dataloader()</span></code> method should depend
on this field for this feature to work i.e.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># using LightningModule</span>
<span class="k">class</span> <span class="nc">LitModel</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">()</span>
        <span class="c1"># or</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>

    <span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">|</span> <span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">LitModel</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">tuner</span> <span class="o">=</span> <span class="n">Tuner</span><span class="p">(</span><span class="n">trainer</span><span class="p">)</span>
<span class="n">tuner</span><span class="o">.</span><span class="n">scale_batch_size</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>


<span class="c1"># using LightningDataModule</span>
<span class="k">class</span> <span class="nc">LitDataModule</span><span class="p">(</span><span class="n">LightningDataModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">()</span>
        <span class="c1"># or</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>

    <span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">|</span> <span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>
<span class="n">datamodule</span> <span class="o">=</span> <span class="n">LitDataModule</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">tuner</span> <span class="o">=</span> <span class="n">Tuner</span><span class="p">(</span><span class="n">trainer</span><span class="p">)</span>
<span class="n">tuner</span><span class="o">.</span><span class="n">scale_batch_size</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">datamodule</span><span class="o">=</span><span class="n">datamodule</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that the <code class="docutils literal notranslate"><span class="pre">train_dataloader</span></code> can be either part of
the <code class="docutils literal notranslate"><span class="pre">LightningModule</span></code> or <code class="docutils literal notranslate"><span class="pre">LightningDataModule</span></code>
as shown above. If both the <code class="docutils literal notranslate"><span class="pre">LightningModule</span></code>
and the <code class="docutils literal notranslate"><span class="pre">LightningDataModule</span></code> contain a <code class="docutils literal notranslate"><span class="pre">train_dataloader</span></code>,
the <code class="docutils literal notranslate"><span class="pre">LightningDataModule</span></code> takes precedence.</p>
</div>
<dl class="simple">
<dt>The algorithm in short works by:</dt><dd><ol class="arabic simple">
<li><p>Dumping the current state of the model and trainer</p></li>
<li><dl class="simple">
<dt>Iteratively until convergence or maximum number of tries <code class="docutils literal notranslate"><span class="pre">max_trials</span></code> (default 25) has been reached:</dt><dd><ul class="simple">
<li><p>Call <code class="docutils literal notranslate"><span class="pre">fit()</span></code> method of trainer. This evaluates <code class="docutils literal notranslate"><span class="pre">steps_per_trial</span></code> (default 3) number of
optimization steps. Each training step can trigger an OOM error if the tensors
(training batch, weights, gradients, etc.) allocated during the steps have a
too large memory footprint.</p></li>
<li><p>If an OOM error is encountered, decrease batch size else increase it.
How much the batch size is increased/decreased is determined by the chosen
strategy.</p></li>
</ul>
</dd>
</dl>
</li>
<li><p>The found batch size is saved to either <code class="docutils literal notranslate"><span class="pre">model.batch_size</span></code> or <code class="docutils literal notranslate"><span class="pre">model.hparams.batch_size</span></code></p></li>
<li><p>Restore the initial state of model and trainer</p></li>
</ol>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">경고</p>
<p>Batch size finder is not yet supported for DDP or any of its variations, it is coming soon.</p>
</div>
<section id="customizing-batch-size-finder">
<h3>Customizing Batch Size Finder<a class="headerlink" href="#customizing-batch-size-finder" title="이 표제에 대한 퍼머링크">¶</a></h3>
<div class="admonition warning">
<p class="admonition-title">경고</p>
<p>This is an <a class="reference internal" href="../versioning.html#experimental-api"><span class="std std-ref">experimental</span></a> feature.</p>
</div>
<ol class="arabic simple">
<li><p>You can also customize the <a class="reference internal" href="../api/lightning.pytorch.callbacks.BatchSizeFinder.html#lightning.pytorch.callbacks.BatchSizeFinder" title="lightning.pytorch.callbacks.batch_size_finder.BatchSizeFinder"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchSizeFinder</span></code></a> callback to run
at different epochs. This feature is useful while fine-tuning models since you can’t always use the same batch size after
unfreezing the backbone.</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">lightning.pytorch.callbacks</span> <span class="kn">import</span> <span class="n">BatchSizeFinder</span>


<span class="k">class</span> <span class="nc">FineTuneBatchSizeFinder</span><span class="p">(</span><span class="n">BatchSizeFinder</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">milestones</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">milestones</span> <span class="o">=</span> <span class="n">milestones</span>

    <span class="k">def</span> <span class="nf">on_fit_start</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span>

    <span class="k">def</span> <span class="nf">on_train_epoch_start</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">trainer</span><span class="o">.</span><span class="n">current_epoch</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">milestones</span> <span class="ow">or</span> <span class="n">trainer</span><span class="o">.</span><span class="n">current_epoch</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scale_batch_size</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">)</span>


<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">FineTuneBatchSizeFinder</span><span class="p">(</span><span class="n">milestones</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">))])</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Run batch size finder for <code class="docutils literal notranslate"><span class="pre">validate</span></code>/<code class="docutils literal notranslate"><span class="pre">test</span></code>/<code class="docutils literal notranslate"><span class="pre">predict</span></code>.</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">lightning.pytorch.callbacks</span> <span class="kn">import</span> <span class="n">BatchSizeFinder</span>


<span class="k">class</span> <span class="nc">EvalBatchSizeFinder</span><span class="p">(</span><span class="n">BatchSizeFinder</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">on_fit_start</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span>

    <span class="k">def</span> <span class="nf">on_test_start</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale_batch_size</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">)</span>


<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">EvalBatchSizeFinder</span><span class="p">()])</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
<hr class="docutils" />
</section>
</section>
<section id="learning-rate-finder">
<span id="id2"></span><h2>Learning Rate Finder<a class="headerlink" href="#learning-rate-finder" title="이 표제에 대한 퍼머링크">¶</a></h2>
<p>For training deep neural networks, selecting a good learning rate is essential
for both better performance and faster convergence. Even optimizers such as
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Adam</span></code></a> that are self-adjusting the learning rate can benefit from more optimal
choices.</p>
<p>To reduce the amount of guesswork concerning choosing a good initial learning
rate, a <cite>learning rate finder</cite> can be used. As described in <a class="reference external" href="https://arxiv.org/abs/1506.01186">this paper</a>
a learning rate finder does a small run where the learning rate is increased
after each processed batch and the corresponding loss is logged. The result of
this is a <code class="docutils literal notranslate"><span class="pre">lr</span></code> vs. <code class="docutils literal notranslate"><span class="pre">loss</span></code> plot that can be used as guidance for choosing an optimal
initial learning rate.</p>
<div class="admonition warning">
<p class="admonition-title">경고</p>
<p>For the moment, this feature only works with models having a single optimizer.</p>
</div>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>With DDP: Since all the processes run in isolation, only process with <code class="docutils literal notranslate"><span class="pre">global_rank=0</span></code> will make the decision to stop the
learning rate finder and broadcast its results to all other ranks. That means, at the end of LR finder, each process will be running with
the learning rate found on <code class="docutils literal notranslate"><span class="pre">global_rank=0</span></code>.</p>
</div>
<section id="using-lightning-s-built-in-lr-finder">
<h3>Using Lightning’s built-in LR finder<a class="headerlink" href="#using-lightning-s-built-in-lr-finder" title="이 표제에 대한 퍼머링크">¶</a></h3>
<p>To enable the learning rate finder, your <a class="reference internal" href="../common/lightning_module.html"><span class="doc">lightning module</span></a> needs to
have a <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code> or <code class="docutils literal notranslate"><span class="pre">lr</span></code> attribute (or as a field in your <code class="docutils literal notranslate"><span class="pre">hparams</span></code> i.e.
<code class="docutils literal notranslate"><span class="pre">hparams.learning_rate</span></code> or <code class="docutils literal notranslate"><span class="pre">hparams.lr</span></code>). Then, create the <a class="reference internal" href="../api/lightning.pytorch.tuner.tuning.Tuner.html#lightning.pytorch.tuner.tuning.Tuner" title="lightning.pytorch.tuner.tuning.Tuner"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tuner</span></code></a> via <code class="docutils literal notranslate"><span class="pre">tuner</span> <span class="pre">=</span> <span class="pre">Tuner(trainer)</span></code>
and call <code class="docutils literal notranslate"><span class="pre">tuner.lr_find(model)</span></code> to run the LR finder.
The suggested <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code> will be written to the console and will be automatically
set to your <a class="reference internal" href="../common/lightning_module.html"><span class="doc">lightning module</span></a>, which can be accessed
via <code class="docutils literal notranslate"><span class="pre">self.learning_rate</span></code> or <code class="docutils literal notranslate"><span class="pre">self.lr</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">lightning.pytorch.tuner</span> <span class="kn">import</span> <span class="n">Tuner</span>


<span class="k">class</span> <span class="nc">LitModel</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">))</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">LitModel</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

<span class="c1"># Create a Tuner</span>
<span class="n">tuner</span> <span class="o">=</span> <span class="n">Tuner</span><span class="p">(</span><span class="n">trainer</span><span class="p">)</span>

<span class="c1"># finds learning rate automatically</span>
<span class="c1"># sets hparams.lr or hparams.learning_rate to that learning rate</span>
<span class="n">tuner</span><span class="o">.</span><span class="n">lr_find</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<p>If your model is using an arbitrary value instead of <code class="docutils literal notranslate"><span class="pre">self.lr</span></code> or <code class="docutils literal notranslate"><span class="pre">self.learning_rate</span></code>, set that value in <code class="docutils literal notranslate"><span class="pre">lr_find</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">LitModel</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">tuner</span> <span class="o">=</span> <span class="n">Tuner</span><span class="p">(</span><span class="n">trainer</span><span class="p">)</span>

<span class="c1"># to set to your own hparams.my_value</span>
<span class="n">tuner</span><span class="o">.</span><span class="n">lr_find</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">attr_name</span><span class="o">=</span><span class="s2">&quot;my_value&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>You can also inspect the results of the learning rate finder or just play around
with the parameters of the algorithm. A typical example of this would look like:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">MyModelClass</span><span class="p">(</span><span class="n">hparams</span><span class="p">)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">()</span>
<span class="n">tuner</span> <span class="o">=</span> <span class="n">Tuner</span><span class="p">(</span><span class="n">trainer</span><span class="p">)</span>

<span class="c1"># Run learning rate finder</span>
<span class="n">lr_finder</span> <span class="o">=</span> <span class="n">tuner</span><span class="o">.</span><span class="n">lr_find</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># Results can be found in</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lr_finder</span><span class="o">.</span><span class="n">results</span><span class="p">)</span>

<span class="c1"># Plot with</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">lr_finder</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">suggest</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Pick point based on plot, or get suggestion</span>
<span class="n">new_lr</span> <span class="o">=</span> <span class="n">lr_finder</span><span class="o">.</span><span class="n">suggestion</span><span class="p">()</span>

<span class="c1"># update hparams of the model</span>
<span class="n">model</span><span class="o">.</span><span class="n">hparams</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">new_lr</span>

<span class="c1"># Fit model</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<p>The figure produced by <code class="docutils literal notranslate"><span class="pre">lr_finder.plot()</span></code> should look something like the figure
below. It is recommended to not pick the learning rate that achieves the lowest
loss, but instead something in the middle of the sharpest downward slope (red point).
This is the point returned py <code class="docutils literal notranslate"><span class="pre">lr_finder.suggestion()</span></code>.</p>
</section>
<section id="customizing-learning-rate-finder">
<h3>Customizing Learning Rate Finder<a class="headerlink" href="#customizing-learning-rate-finder" title="이 표제에 대한 퍼머링크">¶</a></h3>
<div class="admonition warning">
<p class="admonition-title">경고</p>
<p>This is an <a class="reference internal" href="../versioning.html#experimental-api"><span class="std std-ref">experimental</span></a> feature.</p>
</div>
<p>You can also customize the <a class="reference internal" href="../api/lightning.pytorch.callbacks.LearningRateFinder.html#lightning.pytorch.callbacks.LearningRateFinder" title="lightning.pytorch.callbacks.lr_finder.LearningRateFinder"><code class="xref py py-class docutils literal notranslate"><span class="pre">LearningRateFinder</span></code></a> callback to run at different epochs. This feature is useful while fine-tuning models.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">lightning.pytorch.callbacks</span> <span class="kn">import</span> <span class="n">LearningRateFinder</span>


<span class="k">class</span> <span class="nc">FineTuneLearningRateFinder</span><span class="p">(</span><span class="n">LearningRateFinder</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">milestones</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">milestones</span> <span class="o">=</span> <span class="n">milestones</span>

    <span class="k">def</span> <span class="nf">on_fit_start</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span>

    <span class="k">def</span> <span class="nf">on_train_epoch_start</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">trainer</span><span class="o">.</span><span class="n">current_epoch</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">milestones</span> <span class="ow">or</span> <span class="n">trainer</span><span class="o">.</span><span class="n">current_epoch</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lr_find</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">)</span>


<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">FineTuneLearningRateFinder</span><span class="p">(</span><span class="n">milestones</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">))])</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-default">
<img alt="../_images/lr_finder.png" src="../_images/lr_finder.png" />
</figure>
</section>
</section>
<hr class="docutils" />
<section id="advanced-gpu-optimizations">
<h2>Advanced GPU Optimizations<a class="headerlink" href="#advanced-gpu-optimizations" title="이 표제에 대한 퍼머링크">¶</a></h2>
<p>When training on single or multiple GPU machines, Lightning offers a host of advanced optimizations to improve throughput, memory efficiency, and model scaling.
Refer to <a class="reference internal" href="model_parallel.html"><span class="doc">Advanced GPU Optimized Training</span></a> for more details.</p>
<hr class="docutils" />
</section>
<section id="sharing-datasets-across-process-boundaries">
<span id="ddp-spawn-shared-memory"></span><h2>Sharing Datasets Across Process Boundaries<a class="headerlink" href="#sharing-datasets-across-process-boundaries" title="이 표제에 대한 퍼머링크">¶</a></h2>
<p>The <a class="reference internal" href="../api/lightning.pytorch.core.LightningDataModule.html#lightning.pytorch.core.LightningDataModule" title="lightning.pytorch.core.datamodule.LightningDataModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">LightningDataModule</span></code></a> class provides an organized way to decouple data loading from training logic, with <a class="reference internal" href="../api/lightning.pytorch.core.hooks.DataHooks.html#lightning.pytorch.core.hooks.DataHooks.prepare_data" title="lightning.pytorch.core.hooks.DataHooks.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a> being used for downloading and pre-processing the dataset on a single process, and <a class="reference internal" href="../api/lightning.pytorch.core.hooks.DataHooks.html#lightning.pytorch.core.hooks.DataHooks.setup" title="lightning.pytorch.core.hooks.DataHooks.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a> loading the pre-processed data for each process individually:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MNISTDataModule</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningDataModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">prepare_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">MNIST</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data_dir</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stage</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mnist</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data_dir</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">train_loader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">DataLoader</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mnist</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>
</pre></div>
</div>
<p>However, for in-memory datasets, that means that each process will hold a (redundant) replica of the dataset in memory, which may be impractical when using many processes while utilizing datasets that nearly fit into CPU memory, as the memory consumption will scale up linearly with the number of processes.
For example, when training Graph Neural Networks, a common strategy is to load the entire graph into CPU memory for fast access to the entire graph structure and its features, and to then perform neighbor sampling to obtain mini-batches that fit onto the GPU.</p>
<p>A simple way to prevent redundant dataset replicas is to rely on <a class="reference external" href="https://pytorch.org/docs/stable/multiprocessing.html#module-torch.multiprocessing" title="(PyTorch v2.0에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.multiprocessing</span></code></a> to share the <a class="reference external" href="https://pytorch.org/docs/stable/notes/multiprocessing.html">data automatically between spawned processes via shared memory</a>.
For this, all data pre-loading should be done on the main process inside <code class="xref py py-meth docutils literal notranslate"><span class="pre">DataModule.__init__()</span></code>. As a result, all tensor-data will get automatically shared when using the <code class="docutils literal notranslate"><span class="pre">'ddp_spawn'</span></code> strategy.</p>
<div class="admonition warning">
<p class="admonition-title">경고</p>
<p><a class="reference external" href="https://pytorch.org/docs/stable/multiprocessing.html#module-torch.multiprocessing" title="(PyTorch v2.0에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.multiprocessing</span></code></a> will send a handle of each individual tensor to other processes.
In order to prevent any errors due to too many open file handles, try to reduce the number of tensors to share, <em>e.g.</em>, by stacking your data into a single tensor.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MNISTDataModule</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningDataModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mnist</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">T</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">train_loader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">DataLoader</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mnist</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">datamodule</span> <span class="o">=</span> <span class="n">MNISTDataModule</span><span class="p">(</span><span class="s2">&quot;data/MNIST&quot;</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;ddp_spawn&quot;</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">datamodule</span><span class="p">)</span>
</pre></div>
</div>
<p>See the <a class="reference external" href="https://github.com/pyg-team/pytorch_geometric/blob/master/examples/pytorch_lightning/gin.py">graph-level</a> and <a class="reference external" href="https://github.com/pyg-team/pytorch_geometric/blob/master/examples/pytorch_lightning/graph_sage.py">node-level</a> prediction examples in PyTorch Geometric for practical use-cases.</p>
</section>
</section>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../clouds/cluster_advanced.html" class="btn btn-neutral float-right" title="Run on an on-prem cluster (advanced)" accesskey="n" rel="next">Next <img src="../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="../starter/style_guide.html" class="btn btn-neutral" title="Style Guide" accesskey="p" rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright Copyright (c) 2018-2023, Lightning AI et al...

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>
          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Effective Training Techniques</a><ul>
<li><a class="reference internal" href="#accumulate-gradients">Accumulate Gradients</a></li>
<li><a class="reference internal" href="#gradient-clipping">Gradient Clipping</a></li>
<li><a class="reference internal" href="#stochastic-weight-averaging">Stochastic Weight Averaging</a></li>
<li><a class="reference internal" href="#batch-size-finder">Batch Size Finder</a><ul>
<li><a class="reference internal" href="#customizing-batch-size-finder">Customizing Batch Size Finder</a></li>
</ul>
</li>
<li><a class="reference internal" href="#learning-rate-finder">Learning Rate Finder</a><ul>
<li><a class="reference internal" href="#using-lightning-s-built-in-lr-finder">Using Lightning’s built-in LR finder</a></li>
<li><a class="reference internal" href="#customizing-learning-rate-finder">Customizing Learning Rate Finder</a></li>
</ul>
</li>
<li><a class="reference internal" href="#advanced-gpu-optimizations">Advanced GPU Optimizations</a></li>
<li><a class="reference internal" href="#sharing-datasets-across-process-boundaries">Sharing Datasets Across Process Boundaries</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  

  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
         <script src="../_static/jquery.js"></script>
         <script src="../_static/underscore.js"></script>
         <script src="../_static/doctools.js"></script>
         <script src="../_static/clipboard.min.js"></script>
         <script src="../_static/copybutton.js"></script>
         <script src="../_static/copybutton.js"></script>
         <script>let toggleHintShow = 'Click to show';</script>
         <script>let toggleHintHide = 'Click to hide';</script>
         <script>let toggleOpenOnPrint = 'true';</script>
         <script src="../_static/togglebutton.js"></script>
         <script src="../_static/translations.js"></script>
         <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Best practices', 'Optional Extensions', 'Tutorials', 'API References', 'Bolts', 'Examples', 'Partner Domain Frameworks', 'Community'];
</script>



  <!-- Begin Footer -->

  <!-- <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources"> -->
    <!-- <div class="container"> -->
      <!-- <div class="row"> -->
        <!--
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://lightning.ai/docs/pytorch/latest/">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://lightning.ai/docs/pytorch/latest/#tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://lightning.ai/docs/pytorch/latest/#community-examples">View Resources</a>
        </div>
        -->
      <!-- </div> -->
    <!-- </div> -->
  <!-- </div> -->

  <!--
  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://lightning.ai/docs/pytorch/latest/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://lightning.ai/docs/pytorch/latest/">PyTorch</a></li>
            <li><a href="https://lightning.ai/docs/pytorch/latest/starter/introduction.html">Get Started</a></li>
            <li><a href="https://lightning.ai/docs/pytorch/latest/">Features</a></li>
            <li><a href="">Ecosystem</a></li>
            <li><a href="https://lightning.ai/pages/blog/">Blog</a></li>
            <li><a href="https://github.com/Lightning-AI/lightning/blob/master/.github/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://lightning.ai/docs/pytorch/latest/#community-examples">Resources</a></li>
            <li><a href="https://lightning.ai/docs/pytorch/latest/#tutorials">Tutorials</a></li>
            <li><a href="https://lightning.ai/docs/pytorch/latest/">Docs</a></li>
            <li><a href="https://www.pytorchlightning.ai/community" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/Lightning-AI/lightning/issues" target="_blank">Github Issues</a></li>
            <li><a href="" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Stay Connected</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/LightningAI" target="_blank" class="twitter"></a>
            <a href="" target="_blank" class="youtube"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>
  -->

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. Read PyTorch Lightning's <a href="https://pytorchlightning.ai/privacy-policy">Privacy Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://lightning.ai/docs/pytorch/latest/" aria-label="PyTorch Lightning"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <!-- <li>
            <a href="https://lightning.ai/docs/pytorch/latest/starter/introduction.html">Get Started</a>
          </li>

          <li>
            <a href="https://lightning.ai/pages/blog/">Blog</a>
          </li>

          <li class="resources-mobile-menu-title">
            Ecosystem
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://lightning.ai/docs/pytorch/stable/">PyTorch Lightning</a>
            </li>

            <li>
              <a href="https://lightning.ai/docs/fabric/stable/">Lightning Fabric</a>
            </li>

            <li>
              <a href="https://torchmetrics.readthedocs.io/en/stable/">TorchMetrics</a>
            </li>

            <li>
              <a href="https://lightning.ai/docs/fabric/stable/">Fabric</a>
            </li>
          </ul> -->

          <!--<li class="resources-mobile-menu-title">
            Resources
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://lightning.ai/docs/pytorch/latest/#community-examples">Developer Resources</a>
            </li>

            <li>
              <a href="https://lightning.ai/docs/pytorch/latest/">About</a>
            </li>

            <li>
              <a href="">Models (Beta)</a>
            </li>

            <li>
              <a href="https://www.pytorchlightning.ai/community">Community</a>
            </li>

            <li>
              <a href="https://lightning.ai/forums/">Forums</a>
            </li>
          </ul>-->

          <li>
            <a href="https://www.lightning.ai/">Lightning.ai</a>
          </li>

          <li>
            <a href="https://pytorch.kr/">파이토치 한국 사용자 모임</a>
          </li>

          <li>
            <a href="https://discuss.pytorch.kr/">파이토치 한국어 커뮤니티</a>
          </li>

        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>

  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-5SCNQBF5"
  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->
 </body>
</html>