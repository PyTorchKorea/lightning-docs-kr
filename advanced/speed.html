


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="ko" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="ko" > <!--<![endif]-->
<head>
  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-5SCNQBF5');
  </script>
  <!-- End Google Tag Manager -->

  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="google-site-verification" content="okUst94cAlWSsUsGZTB4xSS4UKTtRV8Nu5XZ9pdd3Aw" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Speed Up Model Training &mdash; PyTorch Lightning &amp; PyTorch Korea User Group 2.0.5 문서</title>
  

  
  
    <link rel="shortcut icon" href="../_static/icon.svg"/>
  
  
  
    <link rel="canonical" href="https://lightning.ai/docs/pytorch/stable//advanced/speed.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../_static/togglebutton.css" type="text/css" />
  <link rel="stylesheet" href="../_static/main.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sphinx_paramlinks.css" type="text/css" />
    <link rel="index" title="색인" href="../genindex.html" />
    <link rel="search" title="검색" href="../search.html" />
  <!-- Google Analytics -->
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-82W25RV60Q"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-82W25RV60Q');
    </script>
  
  <!-- End Google Analytics -->
  

  
  <script src="../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/UCity/UCity-Light.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/UCity/UCity-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/UCity/UCity-Semibold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/Inconsolata/Inconsolata.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <script defer src="https://use.fontawesome.com/releases/v6.1.1/js/all.js" integrity="sha384-xBXmu0dk1bEoiwd71wOonQLyH+VpgR1XcDH3rtxrLww5ajNTuMvBdL5SOiFZnNdp" crossorigin="anonymous"></script>

  <script src="https://unpkg.com/react@18/umd/react.development.js" crossorigin></script>
  <script src="https://unpkg.com/react-dom@18/umd/react-dom.development.js" crossorigin></script>
  <script src="https://unpkg.com/babel-standalone@6/babel.min.js"></script>
  <script src="../_static/js/react/react.jsx" type="text/babel"></script>
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://lightning.ai/docs/pytorch/latest/" aria-label="PyTorch Lightning">
      <!--  <img class="call-to-action-img" src="../_static/images/logo-lightning-icon.png"/> -->
      </a>

      <div class="main-menu">
        <ul>
          <!-- <li>
            <a href="https://lightning.ai/docs/pytorch/latest/starter/introduction.html">Get Started</a>
          </li>

          <li>
            <a href="https://lightning.ai/pages/blog/">Blog</a>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning.ai/docs/pytorch/stable/">
                  <span class="dropdown-title">PyTorch Lightning</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning.ai/docs/fabric/stable/">
                  <span class="dropdown-title">Lightning Fabric</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://torchmetrics.readthedocs.io/en/stable/">
                  <span class="dropdown-title">TorchMetrics</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning-flash.readthedocs.io/en/stable/">
                  <span class="dropdown-title">Lightning Flash</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning-bolts.readthedocs.io/en/stable/">
                  <span class="dropdown-title">Lightning Bolts</span>
                </a>
            </div>
          </li> -->

          <!--<li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://www.pytorchlightning.ai/community">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://lightning.ai/docs/pytorch/latest/#community-examples">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://lightning.ai/forums/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>-->

          <!-- 
          <li>
            <a href="https://lightning.ai/docs/pytorch/latest/past_versions.html">Previous Versions</a>
          </li>
          

          <li>
            <a href="https://github.com/Lightning-AI/lightning">GitHub</a>
          </li> -->

          <li>
            <a href="https://www.lightning.ai/">Lightning AI</a>
          </li>

          <li>
            <a href="https://pytorch.kr/">파이토치 한국 사용자 모임</a>
          </li>

          <li>
            <a href="https://discuss.pytorch.kr/">파이토치 한국어 커뮤니티</a>
          </li>

        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  2.0.5
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Home</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../starter/introduction.html">Lightning in 15 minutes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../starter/installation.html">Install</a></li>
<li class="toctree-l1"><a class="reference internal" href="../upgrade/migration_guide.html">Guide how to upgrade to the 2.0 version</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Level Up</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../levels/core_skills.html">Basic skills</a></li>
<li class="toctree-l1"><a class="reference internal" href="../levels/intermediate.html">Intermediate skills</a></li>
<li class="toctree-l1"><a class="reference internal" href="../levels/advanced.html">Advanced skills</a></li>
<li class="toctree-l1"><a class="reference internal" href="../levels/expert.html">Expert skills</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Core API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../common/lightning_module.html">LightningModule</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/trainer.html">Trainer</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Optional API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_references.html">accelerators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_references.html#callbacks">callbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_references.html#cli">cli</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_references.html#core">core</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_references.html#loggers">loggers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_references.html#profiler">profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_references.html#trainer">trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_references.html#strategies">strategies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_references.html#tuner">tuner</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_references.html#utilities">utilities</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">More</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../community/index.html">Community</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary/index.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/index.html">How-to Guides</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>Speed Up Model Training</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/advanced/speed.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <section id="speed-up-model-training">
<span id="training-speedup"></span><h1>Speed Up Model Training<a class="headerlink" href="#speed-up-model-training" title="이 표제에 대한 퍼머링크">¶</a></h1>
<p>When you are limited with the resources, it becomes hard to speed up model training and reduce the training time
without affecting the model’s performance. There are multiple ways you can speed up your model’s time to convergence.</p>
<section id="training-on-accelerators">
<h2>Training on Accelerators<a class="headerlink" href="#training-on-accelerators" title="이 표제에 대한 퍼머링크">¶</a></h2>
<p><strong>Use when:</strong> Whenever possible!</p>
<p>With Lightning, running on GPUs, TPUs, IPUs on multiple nodes is a simple switch of a flag.</p>
<section id="gpu-training">
<h3>GPU Training<a class="headerlink" href="#gpu-training" title="이 표제에 대한 퍼머링크">¶</a></h3>
<p>Lightning supports a variety of plugins to speed up distributed GPU training. Most notably:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../api/lightning.pytorch.strategies.DDPStrategy.html#lightning.pytorch.strategies.DDPStrategy" title="lightning.pytorch.strategies.DDPStrategy"><code class="xref py py-class docutils literal notranslate"><span class="pre">DDPStrategy</span></code></a></p></li>
<li><p><a class="reference internal" href="../api/lightning.pytorch.strategies.FSDPStrategy.html#lightning.pytorch.strategies.FSDPStrategy" title="lightning.pytorch.strategies.FSDPStrategy"><code class="xref py py-class docutils literal notranslate"><span class="pre">FSDPStrategy</span></code></a></p></li>
<li><p><a class="reference internal" href="../api/lightning.pytorch.strategies.DeepSpeedStrategy.html#lightning.pytorch.strategies.DeepSpeedStrategy" title="lightning.pytorch.strategies.DeepSpeedStrategy"><code class="xref py py-class docutils literal notranslate"><span class="pre">DeepSpeedStrategy</span></code></a></p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># run on 1 gpu</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># train on 8 GPUs, using the DDP strategy</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;ddp&quot;</span><span class="p">)</span>

<span class="c1"># train on multiple GPUs across nodes (uses 8 GPUs in total)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_nodes</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
<section id="gpu-training-speedup-tips">
<h4>GPU Training Speedup Tips<a class="headerlink" href="#gpu-training-speedup-tips" title="이 표제에 대한 퍼머링크">¶</a></h4>
<p>When training on single or multiple GPU machines, Lightning offers a host of advanced optimizations to improve throughput, memory efficiency, and model scaling.
Refer to <a class="reference internal" href="model_parallel.html"><span class="doc">Advanced GPU Optimized Training for more details</span></a>.</p>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p><a class="reference internal" href="../api/lightning.pytorch.strategies.DDPStrategy.html#lightning.pytorch.strategies.DDPStrategy" title="lightning.pytorch.strategies.ddp.DDPStrategy"><code class="xref py py-class docutils literal notranslate"><span class="pre">DDPStrategy</span></code></a> only performs two transfer operations for each step, making it the simplest distributed training strategy:</p>
<ol class="arabic simple">
<li><p>Moving data to the device.</p></li>
<li><p>Transfer and sync gradients.</p></li>
</ol>
<a class="reference internal image-reference" href="_static/fetched-s3-assets/ddp.gif"><img alt="Animation showing DDP execution." class="align-center" src="_static/fetched-s3-assets/ddp.gif" style="width: 500px;" /></a>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>For more details on how to tune performance with DDP, please see the <a class="reference internal" href="model_parallel.html#ddp-optimizations"><span class="std std-ref">DDP Optimizations</span></a> section.</p>
<section id="dataloaders">
<h5>DataLoaders<a class="headerlink" href="#dataloaders" title="이 표제에 대한 퍼머링크">¶</a></h5>
<p>When building your DataLoader set <code class="docutils literal notranslate"><span class="pre">num_workers&gt;0</span></code> and <code class="docutils literal notranslate"><span class="pre">pin_memory=True</span></code> (only for GPUs).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Dataloader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="num-workers">
<h5>num_workers<a class="headerlink" href="#num-workers" title="이 표제에 대한 퍼머링크">¶</a></h5>
<p>The question of how many workers to specify in <code class="docutils literal notranslate"><span class="pre">num_workers</span></code> is tricky. Here’s a summary of <a class="reference external" href="https://discuss.pytorch.org/t/guidelines-for-assigning-num-workers-to-dataloader/813">some references</a>, and our suggestions:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">num_workers=0</span></code> means ONLY the main process will load batches (that can be a bottleneck).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_workers=1</span></code> means ONLY one worker (just not the main process) will load data, but it will still be slow.</p></li>
<li><p>The performance of high <code class="docutils literal notranslate"><span class="pre">num_workers</span></code> depends on the batch size and your machine.</p></li>
<li><p>A general place to start is to set <code class="docutils literal notranslate"><span class="pre">num_workers</span></code> equal to the number of CPU cores on that machine. You can get the number of CPU cores in python using <code class="docutils literal notranslate"><span class="pre">os.cpu_count()</span></code>, but note that depending on your batch size, you may overflow RAM memory.</p></li>
</ol>
<div class="admonition warning">
<p class="admonition-title">경고</p>
<p>Increasing <code class="docutils literal notranslate"><span class="pre">num_workers</span></code> will ALSO increase your CPU memory consumption.</p>
</div>
<p>The best thing to do is to increase the <code class="docutils literal notranslate"><span class="pre">num_workers</span></code> slowly and stop once there is no more improvement in your training speed.</p>
<p>For debugging purposes or for dataloaders that load very small datasets, it is desirable to set <code class="docutils literal notranslate"><span class="pre">num_workers=0</span></code>. However, this will always log a warning for every dataloader with <code class="docutils literal notranslate"><span class="pre">num_workers</span> <span class="pre">&lt;=</span> <span class="pre">min(2,</span> <span class="pre">os.cpu_count())</span></code>. In such cases, you can specifically filter this warning by using:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>

<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="s2">&quot;.*Consider increasing the value of the `num_workers` argument*&quot;</span><span class="p">)</span>

<span class="c1"># or to ignore all warnings that could be false positives</span>
<span class="kn">from</span> <span class="nn">lightning.pytorch.utilities.warnings</span> <span class="kn">import</span> <span class="n">PossibleUserWarning</span>

<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="n">PossibleUserWarning</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="spawn">
<h5>Spawn<a class="headerlink" href="#spawn" title="이 표제에 대한 퍼머링크">¶</a></h5>
<p>When using <code class="docutils literal notranslate"><span class="pre">strategy=&quot;ddp_spawn&quot;</span></code> or training on TPUs, the way multiple GPUs/TPU cores are used is by calling <a class="reference external" href="https://pytorch.org/docs/stable/multiprocessing.html#module-torch.multiprocessing" title="(PyTorch v2.0에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.multiprocessing</span></code></a>
<code class="docutils literal notranslate"><span class="pre">.spawn()</span></code> under the hood. The problem is that PyTorch has issues with <code class="docutils literal notranslate"><span class="pre">num_workers&gt;0</span></code> when using <code class="docutils literal notranslate"><span class="pre">.spawn()</span></code>. For this reason, we recommend you
use <code class="docutils literal notranslate"><span class="pre">strategy=&quot;ddp&quot;</span></code> so you can increase the <code class="docutils literal notranslate"><span class="pre">num_workers</span></code>, however since DDP doesn’t work in an interactive environment like IPython/Jupyter notebooks
your script has to be callable like so:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>my_program.py
</pre></div>
</div>
<p>However, using <code class="docutils literal notranslate"><span class="pre">strategy=&quot;ddp_spawn&quot;</span></code> enables to reduce memory usage with In-Memory Dataset and shared memory tensors. For more info, checkout
<a class="reference internal" href="training_tricks.html#ddp-spawn-shared-memory"><span class="std std-ref">Sharing Datasets Across Process Boundaries</span></a> section.</p>
</section>
<section id="persistent-workers">
<h5>Persistent Workers<a class="headerlink" href="#persistent-workers" title="이 표제에 대한 퍼머링크">¶</a></h5>
<p>When using <code class="docutils literal notranslate"><span class="pre">strategy=&quot;ddp_spawn&quot;</span></code> and <code class="docutils literal notranslate"><span class="pre">num_workers&gt;0</span></code>, consider setting <code class="docutils literal notranslate"><span class="pre">persistent_workers=True</span></code> inside your DataLoader since it can result in data-loading bottlenecks and slowdowns.
This is a limitation of Python <code class="docutils literal notranslate"><span class="pre">.spawn()</span></code> and PyTorch.</p>
</section>
</section>
</section>
<section id="tpu-training">
<h3>TPU Training<a class="headerlink" href="#tpu-training" title="이 표제에 대한 퍼머링크">¶</a></h3>
<p>You can set the <code class="docutils literal notranslate"><span class="pre">devices</span></code> trainer argument to 1, [7] (specific core) or eight cores.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># train on 1 TPU core</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;tpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># train on 7th TPU core</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;tpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="p">[</span><span class="mi">7</span><span class="p">])</span>

<span class="c1"># train on 8 TPU cores</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;tpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
</pre></div>
</div>
<p>To train on more than eight cores (a POD),
submit this script using the xla_dist script.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>python -m torch_xla.distributed.xla_dist
--tpu=$TPU_POD_NAME
--conda-env=torch-xla-nightly
--env=XLA_USE_BF16=1
-- python your_trainer_file.py
</pre></div>
</div>
<p>Read more in our <a class="reference internal" href="#training-speedup"><span class="std std-ref">Speed Up Model Training</span></a> and <a class="reference internal" href="../extensions/plugins.html#plugins"><span class="std std-ref">Plugins</span></a> guides.</p>
</section>
</section>
<hr class="docutils" />
<section id="early-stopping">
<h2>Early Stopping<a class="headerlink" href="#early-stopping" title="이 표제에 대한 퍼머링크">¶</a></h2>
<p>Usually, long training epochs can lead to either overfitting or no major improvements in your metrics due to no limited convergence.
Here <a class="reference internal" href="../api/lightning.pytorch.callbacks.EarlyStopping.html#lightning.pytorch.callbacks.EarlyStopping" title="lightning.pytorch.callbacks.early_stopping.EarlyStopping"><code class="xref py py-class docutils literal notranslate"><span class="pre">EarlyStopping</span></code></a> callback can help you stop the training entirely by monitoring a metric of your choice.</p>
<p>You can read more about it <a class="reference internal" href="../common/early_stopping.html#early-stopping"><span class="std std-ref">here</span></a>.</p>
<hr class="docutils" />
</section>
<section id="mixed-precision-16-bit-training">
<span id="speed-amp"></span><h2>Mixed Precision (16-bit) Training<a class="headerlink" href="#mixed-precision-16-bit-training" title="이 표제에 대한 퍼머링크">¶</a></h2>
<p>Lower precision, such as the 16-bit floating-point, enables the training and deployment of large neural networks since they require less memory, enhance data transfer operations since they required
less memory bandwidth and run match operations much faster on GPUs that support Tensor Core.</p>
<p><strong>Use when:</strong></p>
<ul class="simple">
<li><p>You want to optimize for memory usage on a GPU.</p></li>
<li><p>You have a GPU that supports 16-bit precision (NVIDIA pascal architecture or newer).</p></li>
<li><p>Your optimization algorithm (training_step) is numerically stable.</p></li>
<li><p>You want to be the cool person in the lab :p</p></li>
</ul>
<video controls="True" poster="../_static/fetched-s3-assets/thumb_precision.png" preload="auto" width="400"><source src="../_static/fetched-s3-assets/Trainer+flags+9+-+precision_1.mp4" type="video/mp4"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<p>Mixed precision combines the use of both 32 and 16-bit floating points to reduce memory footprint during model training, resulting in improved performance, achieving upto +3X speedups on modern GPUs.</p>
<p>Lightning offers mixed precision training for GPUs and CPUs, as well as bfloat16 mixed precision training for TPUs.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 16-bit precision</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
<p>Read more about <a class="reference internal" href="#speed-amp"><span class="std std-ref">mixed-precision training</span></a>.</p>
</section>
<hr class="docutils" />
<section id="control-training-epochs">
<h2>Control Training Epochs<a class="headerlink" href="#control-training-epochs" title="이 표제에 대한 퍼머링크">¶</a></h2>
<p><strong>Use when:</strong> You run a hyperparameter search to find good initial parameters and want to save time, cost (money), or power (environment).
It can allow you to be more cost efficient and also run more experiments at the same time.</p>
<p>You can use Trainer flags to force training for a minimum number of epochs or limit it to a max number of epochs. Use the <code class="docutils literal notranslate"><span class="pre">min_epochs</span></code> and <code class="docutils literal notranslate"><span class="pre">max_epochs</span></code> Trainer flags to set the number of epochs to run.
Setting <code class="docutils literal notranslate"><span class="pre">min_epochs=N</span></code> makes sure that the training will run for at least <code class="docutils literal notranslate"><span class="pre">N</span></code> epochs. Setting <code class="docutils literal notranslate"><span class="pre">max_epochs=N</span></code> will ensure that training won’t happen after
<code class="docutils literal notranslate"><span class="pre">N</span></code> epochs.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">min_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_epochs</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
<p>If running iteration based training, i.e., infinite / iterable DataLoader, you can also control the number of steps with the <code class="docutils literal notranslate"><span class="pre">min_steps</span></code> and  <code class="docutils literal notranslate"><span class="pre">max_steps</span></code> flags:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">max_steps</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">min_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
<p>You can also interrupt training based on training time:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Stop after 12 hours of training or when reaching 10 epochs (string)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">max_time</span><span class="o">=</span><span class="s2">&quot;00:12:00:00&quot;</span><span class="p">,</span> <span class="n">max_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="c1"># Stop after 1 day and 5 hours (dict)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">max_time</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;days&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;hours&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">})</span>
</pre></div>
</div>
<p>Learn more in our <a class="reference internal" href="../common/trainer.html#trainer-flags"><span class="std std-ref">Trainer flags</span></a> guide.</p>
</section>
<hr class="docutils" />
<section id="control-validation-frequency">
<h2>Control Validation Frequency<a class="headerlink" href="#control-validation-frequency" title="이 표제에 대한 퍼머링크">¶</a></h2>
<section id="check-validation-every-n-epochs">
<h3>Check Validation Every n Epochs<a class="headerlink" href="#check-validation-every-n-epochs" title="이 표제에 대한 퍼머링크">¶</a></h3>
<p><strong>Use when:</strong> You have a small dataset and want to run fewer validation checks.</p>
<p>You can limit validation check to only run every n epochs using the <code class="docutils literal notranslate"><span class="pre">check_val_every_n_epoch</span></code> Trainer flag.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># default</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">check_val_every_n_epoch</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># runs validation after every 7th Epoch</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">check_val_every_n_epoch</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="validation-within-training-epoch">
<h3>Validation Within Training Epoch<a class="headerlink" href="#validation-within-training-epoch" title="이 표제에 대한 퍼머링크">¶</a></h3>
<p><strong>Use when:</strong> You have a large training dataset and want to run mid-epoch validation checks.</p>
<p>For large datasets, it’s often desirable to check validation multiple times within a training epoch.
Pass in a float to check that often within one training epoch. Pass in an int <code class="docutils literal notranslate"><span class="pre">K</span></code> to check every <code class="docutils literal notranslate"><span class="pre">K</span></code> training batch.
Must use an <code class="docutils literal notranslate"><span class="pre">int</span></code> if using an <a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">IterableDataset</span></code></a>.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># default</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">val_check_interval</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="c1"># check every 1/4 th of an epoch</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">val_check_interval</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>

<span class="c1"># check every 100 train batches (ie: for IterableDatasets or fixed frequency)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">val_check_interval</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
<p>Learn more in our <a class="reference internal" href="../common/trainer.html#trainer-flags"><span class="std std-ref">Trainer flags</span></a> guide.</p>
</section>
</section>
<hr class="docutils" />
<section id="preload-data-into-ram">
<h2>Preload Data Into RAM<a class="headerlink" href="#preload-data-into-ram" title="이 표제에 대한 퍼머링크">¶</a></h2>
<p><strong>Use when:</strong> You need access to all samples in a dataset at once.</p>
<p>When your training or preprocessing requires many operations to be performed on entire dataset(s), it can
sometimes be beneficial to store all data in RAM given there is enough space.
However, loading all data at the beginning of the training script has the disadvantage that it can take a long
time, and hence, it slows down the development process. Another downside is that in multiprocessing (e.g., DDP)
the data would get copied in each process.
One can overcome these problems by copying the data into RAM in advance.
Most UNIX-based operating systems provide direct access to tmpfs through a mount point typically named <code class="docutils literal notranslate"><span class="pre">/dev/shm</span></code>.</p>
<p>Increase shared memory if necessary. Refer to the documentation of your OS on how to do this.</p>
<ol class="arabic">
<li><p>Copy training data to shared memory:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cp<span class="w"> </span>-r<span class="w"> </span>/path/to/data/on/disk<span class="w"> </span>/dev/shm/
</pre></div>
</div>
</li>
<li><p>Refer to the new data root in your script or command-line arguments:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">datamodule</span> <span class="o">=</span> <span class="n">MyDataModule</span><span class="p">(</span><span class="n">data_root</span><span class="o">=</span><span class="s2">&quot;/dev/shm/my_data&quot;</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
</section>
<hr class="docutils" />
<section id="model-toggling">
<h2>Model Toggling<a class="headerlink" href="#model-toggling" title="이 표제에 대한 퍼머링크">¶</a></h2>
<p><strong>Use when:</strong> Performing gradient accumulation with multiple optimizers in a
distributed setting.</p>
<p>Here is an explanation of what it does:</p>
<ul class="simple">
<li><p>Considering the current optimizer as A and all other optimizers as B.</p></li>
<li><p>Toggling, which means all parameters from B exclusive to A will have their <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> attribute set to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p>Restoring their original state when exiting the context manager.</p></li>
</ul>
<p>When performing gradient accumulation, there is no need to perform grad synchronization during the accumulation phase.
Setting <code class="docutils literal notranslate"><span class="pre">sync_grad</span></code> to <code class="docutils literal notranslate"><span class="pre">False</span></code> will block this synchronization and improve your training speed.</p>
<p><a class="reference internal" href="../api/lightning.pytorch.core.optimizer.LightningOptimizer.html#lightning.pytorch.core.optimizer.LightningOptimizer" title="lightning.pytorch.core.optimizer.LightningOptimizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code></a> provides a
<a class="reference internal" href="../api/lightning.pytorch.core.optimizer.LightningOptimizer.html#lightning.pytorch.core.optimizer.LightningOptimizer.toggle_model" title="lightning.pytorch.core.optimizer.LightningOptimizer.toggle_model"><code class="xref py py-meth docutils literal notranslate"><span class="pre">toggle_model()</span></code></a> function as a
<a class="reference external" href="https://docs.python.org/3/library/contextlib.html#contextlib.contextmanager" title="(Python v3.11에서)"><code class="xref py py-func docutils literal notranslate"><span class="pre">contextlib.contextmanager()</span></code></a> for advanced users.</p>
<p>Here is an example of an advanced use case:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Scenario for a GAN with gradient accumulation every two batches and optimized for multiple GPUs.</span>
<span class="k">class</span> <span class="nc">SimpleGAN</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">automatic_optimization</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="c1"># Implementation follows the PyTorch tutorial:</span>
        <span class="c1"># https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html</span>
        <span class="n">g_opt</span><span class="p">,</span> <span class="n">d_opt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizers</span><span class="p">()</span>

        <span class="n">X</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="n">X</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">real_label</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">fake_label</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Sync and clear gradients</span>
        <span class="c1"># at the end of accumulation or</span>
        <span class="c1"># at the end of an epoch.</span>
        <span class="n">is_last_batch_to_accumulate</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">is_last_batch</span>

        <span class="n">g_X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_G</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>

        <span class="c1">##########################</span>
        <span class="c1"># Optimize Discriminator #</span>
        <span class="c1">##########################</span>
        <span class="k">with</span> <span class="n">d_opt</span><span class="o">.</span><span class="n">toggle_model</span><span class="p">(</span><span class="n">sync_grad</span><span class="o">=</span><span class="n">is_last_batch_to_accumulate</span><span class="p">):</span>
            <span class="n">d_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">D</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">errD_real</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">d_x</span><span class="p">,</span> <span class="n">real_label</span><span class="p">)</span>

            <span class="n">d_z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">D</span><span class="p">(</span><span class="n">g_X</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
            <span class="n">errD_fake</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">d_z</span><span class="p">,</span> <span class="n">fake_label</span><span class="p">)</span>

            <span class="n">errD</span> <span class="o">=</span> <span class="n">errD_real</span> <span class="o">+</span> <span class="n">errD_fake</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">manual_backward</span><span class="p">(</span><span class="n">errD</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">is_last_batch_to_accumulate</span><span class="p">:</span>
                <span class="n">d_opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
                <span class="n">d_opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="c1">######################</span>
        <span class="c1"># Optimize Generator #</span>
        <span class="c1">######################</span>
        <span class="k">with</span> <span class="n">g_opt</span><span class="o">.</span><span class="n">toggle_model</span><span class="p">(</span><span class="n">sync_grad</span><span class="o">=</span><span class="n">is_last_batch_to_accumulate</span><span class="p">):</span>
            <span class="n">d_z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">D</span><span class="p">(</span><span class="n">g_X</span><span class="p">)</span>
            <span class="n">errG</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">d_z</span><span class="p">,</span> <span class="n">real_label</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">manual_backward</span><span class="p">(</span><span class="n">errG</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">is_last_batch_to_accumulate</span><span class="p">:</span>
                <span class="n">g_opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
                <span class="n">g_opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">({</span><span class="s2">&quot;g_loss&quot;</span><span class="p">:</span> <span class="n">errG</span><span class="p">,</span> <span class="s2">&quot;d_loss&quot;</span><span class="p">:</span> <span class="n">errD</span><span class="p">},</span> <span class="n">prog_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="set-grads-to-none">
<h2>Set Grads to None<a class="headerlink" href="#set-grads-to-none" title="이 표제에 대한 퍼머링크">¶</a></h2>
<p>In order to improve performance, you can override <code class="xref py py-meth docutils literal notranslate"><span class="pre">optimizer_zero_grad()</span></code>.</p>
<p>For a more detailed explanation of the pros / cons of this technique,
read the documentation for <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html#torch.optim.Optimizer.zero_grad" title="(PyTorch v2.0에서)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">zero_grad()</span></code></a> by the PyTorch team.
This is enabled by default on <code class="docutils literal notranslate"><span class="pre">torch&gt;=2.0.0</span></code>.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">optimizer_zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="things-to-avoid">
<h2>Things to Avoid<a class="headerlink" href="#things-to-avoid" title="이 표제에 대한 퍼머링크">¶</a></h2>
<section id="item-numpy-cpu">
<h3>.item(), .numpy(), .cpu()<a class="headerlink" href="#item-numpy-cpu" title="이 표제에 대한 퍼머링크">¶</a></h3>
<p>Don’t call <code class="docutils literal notranslate"><span class="pre">.item()</span></code> anywhere in your code. Use <code class="docutils literal notranslate"><span class="pre">.detach()</span></code> instead to remove the connected graph calls. Lightning
takes a great deal of care to be optimized for this.</p>
</section>
<section id="clear-cache">
<h3>Clear Cache<a class="headerlink" href="#clear-cache" title="이 표제에 대한 퍼머링크">¶</a></h3>
<p>Don’t call <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.cuda.empty_cache.html#torch.cuda.empty_cache" title="(PyTorch v2.0에서)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cuda.empty_cache()</span></code></a> unnecessarily! Every time you call this, ALL your GPUs have to wait to sync.</p>
</section>
<section id="transferring-tensors-to-device">
<h3>Transferring Tensors to Device<a class="headerlink" href="#transferring-tensors-to-device" title="이 표제에 대한 퍼머링크">¶</a></h3>
<p>LightningModules know what device they are on! Construct tensors on the device directly to avoid CPU-&gt;Device transfer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># bad</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

<span class="c1"># good (self is LightningModule)</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<p>For tensors that need to be model attributes, it is best practice to register them as buffers in the module’s
<code class="docutils literal notranslate"><span class="pre">__init__</span></code> method:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># bad</span>
<span class="bp">self</span><span class="o">.</span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># good</span>
<span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;t&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</section>
</section>
</section>


             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright Copyright (c) 2018-2023, Lightning AI et al...

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>
          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Speed Up Model Training</a><ul>
<li><a class="reference internal" href="#training-on-accelerators">Training on Accelerators</a><ul>
<li><a class="reference internal" href="#gpu-training">GPU Training</a><ul>
<li><a class="reference internal" href="#gpu-training-speedup-tips">GPU Training Speedup Tips</a><ul>
<li><a class="reference internal" href="#dataloaders">DataLoaders</a></li>
<li><a class="reference internal" href="#num-workers">num_workers</a></li>
<li><a class="reference internal" href="#spawn">Spawn</a></li>
<li><a class="reference internal" href="#persistent-workers">Persistent Workers</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#tpu-training">TPU Training</a></li>
</ul>
</li>
<li><a class="reference internal" href="#early-stopping">Early Stopping</a></li>
<li><a class="reference internal" href="#mixed-precision-16-bit-training">Mixed Precision (16-bit) Training</a></li>
<li><a class="reference internal" href="#control-training-epochs">Control Training Epochs</a></li>
<li><a class="reference internal" href="#control-validation-frequency">Control Validation Frequency</a><ul>
<li><a class="reference internal" href="#check-validation-every-n-epochs">Check Validation Every n Epochs</a></li>
<li><a class="reference internal" href="#validation-within-training-epoch">Validation Within Training Epoch</a></li>
</ul>
</li>
<li><a class="reference internal" href="#preload-data-into-ram">Preload Data Into RAM</a></li>
<li><a class="reference internal" href="#model-toggling">Model Toggling</a></li>
<li><a class="reference internal" href="#set-grads-to-none">Set Grads to None</a></li>
<li><a class="reference internal" href="#things-to-avoid">Things to Avoid</a><ul>
<li><a class="reference internal" href="#item-numpy-cpu">.item(), .numpy(), .cpu()</a></li>
<li><a class="reference internal" href="#clear-cache">Clear Cache</a></li>
<li><a class="reference internal" href="#transferring-tensors-to-device">Transferring Tensors to Device</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  

  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
         <script src="../_static/jquery.js"></script>
         <script src="../_static/underscore.js"></script>
         <script src="../_static/doctools.js"></script>
         <script src="../_static/clipboard.min.js"></script>
         <script src="../_static/copybutton.js"></script>
         <script src="../_static/copybutton.js"></script>
         <script>let toggleHintShow = 'Click to show';</script>
         <script>let toggleHintHide = 'Click to hide';</script>
         <script>let toggleOpenOnPrint = 'true';</script>
         <script src="../_static/togglebutton.js"></script>
         <script src="../_static/translations.js"></script>
         <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Best practices', 'Optional Extensions', 'Tutorials', 'API References', 'Bolts', 'Examples', 'Partner Domain Frameworks', 'Community'];
</script>



  <!-- Begin Footer -->

  <!-- <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources"> -->
    <!-- <div class="container"> -->
      <!-- <div class="row"> -->
        <!--
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://lightning.ai/docs/pytorch/latest/">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://lightning.ai/docs/pytorch/latest/#tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://lightning.ai/docs/pytorch/latest/#community-examples">View Resources</a>
        </div>
        -->
      <!-- </div> -->
    <!-- </div> -->
  <!-- </div> -->

  <!--
  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://lightning.ai/docs/pytorch/latest/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://lightning.ai/docs/pytorch/latest/">PyTorch</a></li>
            <li><a href="https://lightning.ai/docs/pytorch/latest/starter/introduction.html">Get Started</a></li>
            <li><a href="https://lightning.ai/docs/pytorch/latest/">Features</a></li>
            <li><a href="">Ecosystem</a></li>
            <li><a href="https://lightning.ai/pages/blog/">Blog</a></li>
            <li><a href="https://github.com/Lightning-AI/lightning/blob/master/.github/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://lightning.ai/docs/pytorch/latest/#community-examples">Resources</a></li>
            <li><a href="https://lightning.ai/docs/pytorch/latest/#tutorials">Tutorials</a></li>
            <li><a href="https://lightning.ai/docs/pytorch/latest/">Docs</a></li>
            <li><a href="https://www.pytorchlightning.ai/community" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/Lightning-AI/lightning/issues" target="_blank">Github Issues</a></li>
            <li><a href="" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Stay Connected</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/LightningAI" target="_blank" class="twitter"></a>
            <a href="" target="_blank" class="youtube"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>
  -->

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. Read PyTorch Lightning's <a href="https://pytorchlightning.ai/privacy-policy">Privacy Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://lightning.ai/docs/pytorch/latest/" aria-label="PyTorch Lightning"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <!-- <li>
            <a href="https://lightning.ai/docs/pytorch/latest/starter/introduction.html">Get Started</a>
          </li>

          <li>
            <a href="https://lightning.ai/pages/blog/">Blog</a>
          </li>

          <li class="resources-mobile-menu-title">
            Ecosystem
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://lightning.ai/docs/pytorch/stable/">PyTorch Lightning</a>
            </li>

            <li>
              <a href="https://lightning.ai/docs/fabric/stable/">Lightning Fabric</a>
            </li>

            <li>
              <a href="https://torchmetrics.readthedocs.io/en/stable/">TorchMetrics</a>
            </li>

            <li>
              <a href="https://lightning.ai/docs/fabric/stable/">Fabric</a>
            </li>
          </ul> -->

          <!--<li class="resources-mobile-menu-title">
            Resources
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://lightning.ai/docs/pytorch/latest/#community-examples">Developer Resources</a>
            </li>

            <li>
              <a href="https://lightning.ai/docs/pytorch/latest/">About</a>
            </li>

            <li>
              <a href="">Models (Beta)</a>
            </li>

            <li>
              <a href="https://www.pytorchlightning.ai/community">Community</a>
            </li>

            <li>
              <a href="https://lightning.ai/forums/">Forums</a>
            </li>
          </ul>-->

          <li>
            <a href="https://www.lightning.ai/">Lightning.ai</a>
          </li>

          <li>
            <a href="https://pytorch.kr/">파이토치 한국 사용자 모임</a>
          </li>

          <li>
            <a href="https://discuss.pytorch.kr/">파이토치 한국어 커뮤니티</a>
          </li>

        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>

  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-5SCNQBF5"
  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->
 </body>
</html>