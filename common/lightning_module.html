


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="ko" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="ko" > <!--<![endif]-->
<head>
  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-5SCNQBF5');
  </script>
  <!-- End Google Tag Manager -->

  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="google-site-verification" content="okUst94cAlWSsUsGZTB4xSS4UKTtRV8Nu5XZ9pdd3Aw" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>LightningModule &mdash; PyTorch Lightning &amp; PyTorch Korea User Group 2.0.5 문서</title>
  

  
  
    <link rel="shortcut icon" href="../_static/icon.svg"/>
  
  
  
    <link rel="canonical" href="https://lightning.ai/docs/pytorch/stable//common/lightning_module.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../_static/togglebutton.css" type="text/css" />
  <link rel="stylesheet" href="../_static/main.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sphinx_paramlinks.css" type="text/css" />
    <link rel="index" title="색인" href="../genindex.html" />
    <link rel="search" title="검색" href="../search.html" />
    <link rel="next" title="Trainer" href="trainer.html" />
    <link rel="prev" title="Expert skills" href="../levels/expert.html" />
  <!-- Google Analytics -->
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-82W25RV60Q"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-82W25RV60Q');
    </script>
  
  <!-- End Google Analytics -->
  

  
  <script src="../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/UCity/UCity-Light.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/UCity/UCity-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/UCity/UCity-Semibold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/Inconsolata/Inconsolata.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <script defer src="https://use.fontawesome.com/releases/v6.1.1/js/all.js" integrity="sha384-xBXmu0dk1bEoiwd71wOonQLyH+VpgR1XcDH3rtxrLww5ajNTuMvBdL5SOiFZnNdp" crossorigin="anonymous"></script>

  <script src="https://unpkg.com/react@18/umd/react.development.js" crossorigin></script>
  <script src="https://unpkg.com/react-dom@18/umd/react-dom.development.js" crossorigin></script>
  <script src="https://unpkg.com/babel-standalone@6/babel.min.js"></script>
  <script src="../_static/js/react/react.jsx" type="text/babel"></script>
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://lightning.ai/docs/pytorch/latest/" aria-label="PyTorch Lightning">
      <!--  <img class="call-to-action-img" src="../_static/images/logo-lightning-icon.png"/> -->
      </a>

      <div class="main-menu">
        <ul>
          <!-- <li>
            <a href="https://lightning.ai/docs/pytorch/latest/starter/introduction.html">Get Started</a>
          </li>

          <li>
            <a href="https://lightning.ai/pages/blog/">Blog</a>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning.ai/docs/pytorch/stable/">
                  <span class="dropdown-title">PyTorch Lightning</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning.ai/docs/fabric/stable/">
                  <span class="dropdown-title">Lightning Fabric</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://torchmetrics.readthedocs.io/en/stable/">
                  <span class="dropdown-title">TorchMetrics</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning-flash.readthedocs.io/en/stable/">
                  <span class="dropdown-title">Lightning Flash</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning-bolts.readthedocs.io/en/stable/">
                  <span class="dropdown-title">Lightning Bolts</span>
                </a>
            </div>
          </li> -->

          <!--<li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://www.pytorchlightning.ai/community">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://lightning.ai/docs/pytorch/latest/#community-examples">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://lightning.ai/forums/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>-->

          <!-- 
          <li>
            <a href="https://lightning.ai/docs/pytorch/latest/past_versions.html">Previous Versions</a>
          </li>
          

          <li>
            <a href="https://github.com/Lightning-AI/lightning">GitHub</a>
          </li> -->

          <li>
            <a href="https://www.lightning.ai/">Lightning AI</a>
          </li>

          <li>
            <a href="https://pytorch.kr/">파이토치 한국 사용자 모임</a>
          </li>

          <li>
            <a href="https://discuss.pytorch.kr/">파이토치 한국어 커뮤니티</a>
          </li>

        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  2.0.5
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Home</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../starter/introduction.html">Lightning in 15 minutes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../starter/installation.html">Install</a></li>
<li class="toctree-l1"><a class="reference internal" href="../upgrade/migration_guide.html">Guide how to upgrade to the 2.0 version</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Level Up</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../levels/core_skills.html">Basic skills</a></li>
<li class="toctree-l1"><a class="reference internal" href="../levels/intermediate.html">Intermediate skills</a></li>
<li class="toctree-l1"><a class="reference internal" href="../levels/advanced.html">Advanced skills</a></li>
<li class="toctree-l1"><a class="reference internal" href="../levels/expert.html">Expert skills</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Core API</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">LightningModule</a></li>
<li class="toctree-l1"><a class="reference internal" href="trainer.html">Trainer</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Optional API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_references.html">accelerators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_references.html#callbacks">callbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_references.html#cli">cli</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_references.html#core">core</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_references.html#loggers">loggers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_references.html#profiler">profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_references.html#trainer">trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_references.html#strategies">strategies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_references.html#tuner">tuner</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_references.html#utilities">utilities</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">More</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../community/index.html">Community</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary/index.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html">How-to Guides</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>LightningModule</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/common/lightning_module.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <section id="lightningmodule">
<span id="lightning-module"></span><h1>LightningModule<a class="headerlink" href="#lightningmodule" title="이 표제에 대한 퍼머링크">¶</a></h1>
<p>A <a class="reference internal" href="../api/lightning.pytorch.core.LightningModule.html#lightning.pytorch.core.LightningModule" title="lightning.pytorch.core.module.LightningModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code></a> organizes your PyTorch code into 6 sections:</p>
<ul class="simple">
<li><p>Initialization (<code class="docutils literal notranslate"><span class="pre">__init__</span></code> and <code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code>).</p></li>
<li><p>Train Loop (<code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code>)</p></li>
<li><p>Validation Loop (<code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code>)</p></li>
<li><p>Test Loop (<code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code>)</p></li>
<li><p>Prediction Loop (<code class="xref py py-meth docutils literal notranslate"><span class="pre">predict_step()</span></code>)</p></li>
<li><p>Optimizers and LR Schedulers (<code class="xref py py-meth docutils literal notranslate"><span class="pre">configure_optimizers()</span></code>)</p></li>
</ul>
<p>When you convert to use Lightning, the code IS NOT abstracted - just organized.
All the other code that’s not in the <a class="reference internal" href="../api/lightning.pytorch.core.LightningModule.html#lightning.pytorch.core.LightningModule" title="lightning.pytorch.core.module.LightningModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code></a>
has been automated for you by the <a class="reference internal" href="../api/lightning.pytorch.trainer.trainer.Trainer.html#lightning.pytorch.trainer.trainer.Trainer" title="lightning.pytorch.trainer.trainer.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a>.</p>
<div class="line-block">
<div class="line"><br /></div>
</div>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">MyLightningModuleNet</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">()</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<p>There are no <code class="docutils literal notranslate"><span class="pre">.cuda()</span></code> or <code class="docutils literal notranslate"><span class="pre">.to(device)</span></code> calls required. Lightning does these for you.</p>
<div class="line-block">
<div class="line"><br /></div>
</div>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># don&#39;t do in Lightning</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># do this instead</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span>  <span class="c1"># leave it alone!</span>

<span class="c1"># or to init a new tensor</span>
<span class="n">new_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">new_x</span> <span class="o">=</span> <span class="n">new_x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<p>When running under a distributed strategy, Lightning handles the distributed sampler for you by default.</p>
<div class="line-block">
<div class="line"><br /></div>
</div>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Don&#39;t do in Lightning...</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">sampler</span> <span class="o">=</span> <span class="n">DistributedSampler</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">DataLoader</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">sampler</span><span class="p">)</span>

<span class="c1"># do this instead</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">DataLoader</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<p>A <a class="reference internal" href="../api/lightning.pytorch.core.LightningModule.html#lightning.pytorch.core.LightningModule" title="lightning.pytorch.core.module.LightningModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code></a> is a <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code></a> but with added functionality. Use it as such!</p>
<div class="line-block">
<div class="line"><br /></div>
</div>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span><span class="n">PATH</span><span class="p">)</span>
<span class="n">net</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<p>Thus, to use Lightning, you just need to organize your code which takes about 30 minutes,
(and let’s be real, you probably should do anyway).</p>
<hr class="docutils" />
<section id="starter-example">
<h2>Starter Example<a class="headerlink" href="#starter-example" title="이 표제에 대한 퍼머링크">¶</a></h2>
<p>Here are the only required methods.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">lightning.pytorch</span> <span class="k">as</span> <span class="nn">pl</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>


<span class="k">class</span> <span class="nc">LitModel</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
</pre></div>
</div>
<p>Which you can train by doing:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">MNIST</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">(),</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()))</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span><span class="n">max_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LitModel</span><span class="p">()</span>

<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_dataloaders</span><span class="o">=</span><span class="n">train_loader</span><span class="p">)</span>
</pre></div>
</div>
<p>The LightningModule has many convenience methods, but the core ones you need to know about are:</p>
<table class="colwidths-given docutils align-default">
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code> and <code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></p></td>
<td><p>Define initialization here</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></p></td>
<td><p>To run data through your model only (separate from <code class="docutils literal notranslate"><span class="pre">training_step</span></code>)</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></p></td>
<td><p>the complete training step</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></p></td>
<td><p>the complete validation step</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></p></td>
<td><p>the complete test step</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">predict_step()</span></code></p></td>
<td><p>the complete prediction step</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">configure_optimizers()</span></code></p></td>
<td><p>define optimizers and LR schedulers</p></td>
</tr>
</tbody>
</table>
</section>
<hr class="docutils" />
<section id="training">
<h2>Training<a class="headerlink" href="#training" title="이 표제에 대한 퍼머링크">¶</a></h2>
<section id="training-loop">
<h3>Training Loop<a class="headerlink" href="#training-loop" title="이 표제에 대한 퍼머링크">¶</a></h3>
<p>To activate the training loop, override the <code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code> method.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LitClassifier</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>

    <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<p>Under the hood, Lightning does the following (pseudocode):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># put model in train mode and enable gradient calculation</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">training_step</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>

    <span class="c1"># clear gradients</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># backward</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># update parameters</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="train-epoch-level-metrics">
<h3>Train Epoch-level Metrics<a class="headerlink" href="#train-epoch-level-metrics" title="이 표제에 대한 퍼머링크">¶</a></h3>
<p>If you want to calculate epoch-level metrics and log them, use <code class="xref py py-meth docutils literal notranslate"><span class="pre">log()</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># logs metrics for each training_step,</span>
    <span class="c1"># and the average across the epoch, to the progress bar and logger</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;train_loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">on_step</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">on_epoch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">prog_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">logger</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<p>The <code class="xref py py-meth docutils literal notranslate"><span class="pre">log()</span></code> method automatically reduces the
requested metrics across a complete epoch and devices. Here’s the pseudocode of what it does under the hood:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">):</span>
    <span class="c1"># forward</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">training_step</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>
    <span class="n">outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>

    <span class="c1"># clear gradients</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="c1"># backward</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="c1"># update parameters</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="c1"># note: in reality, we do this incrementally, instead of keeping all outputs in memory</span>
<span class="n">epoch_metric</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">outs</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="train-epoch-level-operations">
<h3>Train Epoch-level Operations<a class="headerlink" href="#train-epoch-level-operations" title="이 표제에 대한 퍼머링크">¶</a></h3>
<p>In the case that you need to make use of all the outputs from each <code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code>,
override the <code class="xref py py-meth docutils literal notranslate"><span class="pre">on_train_epoch_end()</span></code> method.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">training_step_outputs</span> <span class="o">=</span> <span class="p">[]</span>


<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="o">...</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">training_step_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>


<span class="k">def</span> <span class="nf">on_train_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">all_preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_step_outputs</span><span class="p">)</span>
    <span class="c1"># do something with all preds</span>
    <span class="o">...</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">training_step_outputs</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>  <span class="c1"># free memory</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="validation">
<h2>Validation<a class="headerlink" href="#validation" title="이 표제에 대한 퍼머링크">¶</a></h2>
<section id="validation-loop">
<h3>Validation Loop<a class="headerlink" href="#validation-loop" title="이 표제에 대한 퍼머링크">¶</a></h3>
<p>To activate the validation loop while training, override the <code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code> method.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LitModel</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;val_loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
<p>Under the hood, Lightning does the following (pseudocode):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># ...</span>
<span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">training_step</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="c1"># ...</span>

    <span class="k">if</span> <span class="n">validate_at_some_point</span><span class="p">:</span>
        <span class="c1"># disable grads + batchnorm + dropout</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

        <span class="c1"># ----------------- VAL LOOP ---------------</span>
        <span class="k">for</span> <span class="n">val_batch_idx</span><span class="p">,</span> <span class="n">val_batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">val_dataloader</span><span class="p">):</span>
            <span class="n">val_out</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">validation_step</span><span class="p">(</span><span class="n">val_batch</span><span class="p">,</span> <span class="n">val_batch_idx</span><span class="p">)</span>
        <span class="c1"># ----------------- VAL LOOP ---------------</span>

        <span class="c1"># enable grads + batchnorm + dropout</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
<p>You can also run just the validation loop on your validation dataloaders by overriding <code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code>
and calling <a class="reference internal" href="../api/lightning.pytorch.trainer.trainer.Trainer.html#lightning.pytorch.trainer.trainer.Trainer.validate" title="lightning.pytorch.trainer.trainer.Trainer.validate"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validate()</span></code></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">()</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">validate</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>It is recommended to validate on single device to ensure each sample/batch gets evaluated exactly once.
This is helpful to make sure benchmarking for research papers is done the right way. Otherwise, in a
multi-device setting, samples could occur duplicated when <a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.distributed.DistributedSampler" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistributedSampler</span></code></a>
is used, for eg. with <code class="docutils literal notranslate"><span class="pre">strategy=&quot;ddp&quot;</span></code>. It replicates some samples on some devices to make sure all devices have
same batch size in case of uneven inputs.</p>
</div>
</section>
<section id="validation-epoch-level-metrics">
<h3>Validation Epoch-level Metrics<a class="headerlink" href="#validation-epoch-level-metrics" title="이 표제에 대한 퍼머링크">¶</a></h3>
<p>In the case that you need to make use of all the outputs from each <code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code>,
override the <code class="xref py py-meth docutils literal notranslate"><span class="pre">on_validation_epoch_end()</span></code> method.
Note that this method is called before <code class="xref py py-meth docutils literal notranslate"><span class="pre">on_train_epoch_end()</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">validation_step_outputs</span> <span class="o">=</span> <span class="p">[]</span>


<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="o">...</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">validation_step_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pred</span>


<span class="k">def</span> <span class="nf">on_validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">all_preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">validation_step_outputs</span><span class="p">)</span>
    <span class="c1"># do something with all preds</span>
    <span class="o">...</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">validation_step_outputs</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>  <span class="c1"># free memory</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="testing">
<h2>Testing<a class="headerlink" href="#testing" title="이 표제에 대한 퍼머링크">¶</a></h2>
<section id="test-loop">
<h3>Test Loop<a class="headerlink" href="#test-loop" title="이 표제에 대한 퍼머링크">¶</a></h3>
<p>The process for enabling a test loop is the same as the process for enabling a validation loop. Please refer to
the section above for details. For this you need to override the <code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code> method.</p>
<p>The only difference is that the test loop is only called when <a class="reference internal" href="../api/lightning.pytorch.trainer.trainer.Trainer.html#lightning.pytorch.trainer.trainer.Trainer.test" title="lightning.pytorch.trainer.trainer.Trainer.test"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test()</span></code></a> is used.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">()</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># automatically loads the best weights for you</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<p>There are two ways to call <code class="docutils literal notranslate"><span class="pre">test()</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># call after training</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">()</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># automatically auto-loads the best weights from the previous run</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">dataloaders</span><span class="o">=</span><span class="n">test_dataloader</span><span class="p">)</span>

<span class="c1"># or call with pretrained model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span><span class="n">PATH</span><span class="p">)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">()</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloaders</span><span class="o">=</span><span class="n">test_dataloader</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>It is recommended to validate on single device to ensure each sample/batch gets evaluated exactly once.
This is helpful to make sure benchmarking for research papers is done the right way. Otherwise, in a
multi-device setting, samples could occur duplicated when <a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.distributed.DistributedSampler" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistributedSampler</span></code></a>
is used, for eg. with <code class="docutils literal notranslate"><span class="pre">strategy=&quot;ddp&quot;</span></code>. It replicates some samples on some devices to make sure all devices have
same batch size in case of uneven inputs.</p>
</div>
</section>
</section>
<hr class="docutils" />
<section id="inference">
<h2>Inference<a class="headerlink" href="#inference" title="이 표제에 대한 퍼머링크">¶</a></h2>
<section id="prediction-loop">
<h3>Prediction Loop<a class="headerlink" href="#prediction-loop" title="이 표제에 대한 퍼머링크">¶</a></h3>
<p>By default, the <code class="xref py py-meth docutils literal notranslate"><span class="pre">predict_step()</span></code> method runs the
<code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code> method. In order to customize this behaviour,
simply override the <code class="xref py py-meth docutils literal notranslate"><span class="pre">predict_step()</span></code> method.</p>
<p>For the example let’s override <code class="docutils literal notranslate"><span class="pre">predict_step</span></code> and try out <a class="reference external" href="https://arxiv.org/pdf/1506.02142.pdf">Monte Carlo Dropout</a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LitMCdropoutModel</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">mc_iteration</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mc_iteration</span> <span class="o">=</span> <span class="n">mc_iteration</span>

    <span class="k">def</span> <span class="nf">predict_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="c1"># enable Monte Carlo Dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

        <span class="c1"># take average of `self.mc_iteration` iterations</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mc_iteration</span><span class="p">)])</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pred</span>
</pre></div>
</div>
<p>Under the hood, Lightning does the following (pseudocode):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># disable grads + batchnorm + dropout</span>
<span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">all_preds</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">predict_dataloader</span><span class="p">):</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_step</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>
    <span class="n">all_preds</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span>
</pre></div>
</div>
<p>There are two ways to call <code class="docutils literal notranslate"><span class="pre">predict()</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># call after training</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">()</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># automatically auto-loads the best weights from the previous run</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">dataloaders</span><span class="o">=</span><span class="n">predict_dataloader</span><span class="p">)</span>

<span class="c1"># or call with pretrained model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span><span class="n">PATH</span><span class="p">)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">()</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloaders</span><span class="o">=</span><span class="n">test_dataloader</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="inference-in-research">
<h3>Inference in Research<a class="headerlink" href="#inference-in-research" title="이 표제에 대한 퍼머링크">¶</a></h3>
<p>If you want to perform inference with the system, you can add a <code class="docutils literal notranslate"><span class="pre">forward</span></code> method to the LightningModule.</p>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>When using forward, you are responsible to call <code class="xref py py-func docutils literal notranslate"><span class="pre">eval()</span></code> and use the <code class="xref py py-func docutils literal notranslate"><span class="pre">no_grad()</span></code> context manager.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Autoencoder</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">Autoencoder</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">reconstruction</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span>
</pre></div>
</div>
<p>The advantage of adding a forward is that in complex systems, you can do a much more involved inference procedure,
such as text generation:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Seq2Seq</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">hidden_states</span><span class="p">:</span>
            <span class="c1"># decode</span>
            <span class="o">...</span>
        <span class="k">return</span> <span class="n">decoded</span>
</pre></div>
</div>
<p>In the case where you want to scale your inference, you should be using
<code class="xref py py-meth docutils literal notranslate"><span class="pre">predict_step()</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Autoencoder</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="c1"># this calls forward</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>


<span class="n">data_module</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Autoencoder</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">data_module</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="inference-in-production">
<h3>Inference in Production<a class="headerlink" href="#inference-in-production" title="이 표제에 대한 퍼머링크">¶</a></h3>
<p>For cases like production, you might want to iterate different models inside a LightningModule.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchmetrics.functional</span> <span class="kn">import</span> <span class="n">accuracy</span>


<span class="k">class</span> <span class="nc">ClassificationTask</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>

    <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="n">loss</span><span class="p">,</span> <span class="n">acc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shared_eval_step</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>
        <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;val_acc&quot;</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span> <span class="s2">&quot;val_loss&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">metrics</span>

    <span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="n">loss</span><span class="p">,</span> <span class="n">acc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shared_eval_step</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>
        <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;test_acc&quot;</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span> <span class="s2">&quot;test_loss&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">metrics</span>

    <span class="k">def</span> <span class="nf">_shared_eval_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">acc</span> <span class="o">=</span> <span class="n">accuracy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">acc</span>

    <span class="k">def</span> <span class="nf">predict_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y_hat</span>

    <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
</pre></div>
</div>
<p>Then pass in any arbitrary model to be fit with this task</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="p">[</span><span class="n">resnet50</span><span class="p">(),</span> <span class="n">vgg16</span><span class="p">(),</span> <span class="n">BidirectionalRNN</span><span class="p">()]:</span>
    <span class="n">task</span> <span class="o">=</span> <span class="n">ClassificationTask</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

    <span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">task</span><span class="p">,</span> <span class="n">train_dataloaders</span><span class="o">=</span><span class="n">train_dataloader</span><span class="p">,</span> <span class="n">val_dataloaders</span><span class="o">=</span><span class="n">val_dataloader</span><span class="p">)</span>
</pre></div>
</div>
<p>Tasks can be arbitrarily complex such as implementing GAN training, self-supervised or even RL.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GANTask</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">generator</span><span class="p">,</span> <span class="n">discriminator</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">generator</span> <span class="o">=</span> <span class="n">generator</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">discriminator</span> <span class="o">=</span> <span class="n">discriminator</span>

    <span class="o">...</span>
</pre></div>
</div>
<p>When used like this, the model can be separated from the Task and thus used in production without needing to keep it in
a <code class="docutils literal notranslate"><span class="pre">LightningModule</span></code>.</p>
<p>The following example shows how you can run inference in the Python runtime:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">task</span> <span class="o">=</span> <span class="n">ClassificationTask</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">task</span><span class="p">,</span> <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">val_dataloader</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">(</span><span class="s2">&quot;best_model.ckpt&quot;</span><span class="p">)</span>

<span class="c1"># use model after training or load weights and drop into the production system</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ClassificationTask</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span><span class="s2">&quot;best_model.ckpt&quot;</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>Check out <a class="reference internal" href="../deploy/production.html#production-inference"><span class="std std-ref">Inference in Production</span></a> guide to learn about the possible ways to perform inference in production.</p>
</section>
</section>
<hr class="docutils" />
<section id="save-hyperparameters">
<h2>Save Hyperparameters<a class="headerlink" href="#save-hyperparameters" title="이 표제에 대한 퍼머링크">¶</a></h2>
<p>Often times we train many versions of a model. You might share that model or come back to it a few months later at which
point it is very useful to know how that model was trained (i.e.: what learning rate, neural network, etc…).</p>
<p>Lightning has a standardized way of saving the information for you in checkpoints and YAML files. The goal here is to
improve readability and reproducibility.</p>
<section id="id1">
<h3>save_hyperparameters<a class="headerlink" href="#id1" title="이 표제에 대한 퍼머링크">¶</a></h3>
<p>Use <code class="xref py py-meth docutils literal notranslate"><span class="pre">save_hyperparameters()</span></code> within your
<a class="reference internal" href="../api/lightning.pytorch.core.LightningModule.html#lightning.pytorch.core.LightningModule" title="lightning.pytorch.core.module.LightningModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code></a>’s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method. It will enable Lightning to store all the
provided arguments under the <code class="docutils literal notranslate"><span class="pre">self.hparams</span></code> attribute. These hyperparameters will also be stored within the model
checkpoint, which simplifies model re-instantiation after training.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LitMNIST</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer_1_dim</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># call this to save (layer_1_dim=128, learning_rate=1e-4) to the checkpoint</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">()</span>

        <span class="c1"># equivalent</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">(</span><span class="s2">&quot;layer_1_dim&quot;</span><span class="p">,</span> <span class="s2">&quot;learning_rate&quot;</span><span class="p">)</span>

        <span class="c1"># Now possible to access layer_1_dim from hparams</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="o">.</span><span class="n">layer_1_dim</span>
</pre></div>
</div>
<p>In addition, loggers that support it will automatically log the contents of <code class="docutils literal notranslate"><span class="pre">self.hparams</span></code>.</p>
</section>
<section id="excluding-hyperparameters">
<h3>Excluding hyperparameters<a class="headerlink" href="#excluding-hyperparameters" title="이 표제에 대한 퍼머링크">¶</a></h3>
<p>By default, every parameter of the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method will be considered a hyperparameter to the LightningModule.
However, sometimes some parameters need to be excluded from saving, for example when they are not serializable. Those
parameters should be provided back when reloading the LightningModule. In this case, exclude them explicitly:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LitMNIST</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss_fx</span><span class="p">,</span> <span class="n">generator_network</span><span class="p">,</span> <span class="n">layer_1_dim</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_1_dim</span> <span class="o">=</span> <span class="n">layer_1_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_fx</span> <span class="o">=</span> <span class="n">loss_fx</span>

        <span class="c1"># call this to save only (layer_1_dim=128) to the checkpoint</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">(</span><span class="s2">&quot;layer_1_dim&quot;</span><span class="p">)</span>

        <span class="c1"># equivalent</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">(</span><span class="n">ignore</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;loss_fx&quot;</span><span class="p">,</span> <span class="s2">&quot;generator_network&quot;</span><span class="p">])</span>
</pre></div>
</div>
</section>
<section id="load-from-checkpoint">
<h3>load_from_checkpoint<a class="headerlink" href="#load-from-checkpoint" title="이 표제에 대한 퍼머링크">¶</a></h3>
<p>LightningModules that have hyperparameters automatically saved with
<code class="xref py py-meth docutils literal notranslate"><span class="pre">save_hyperparameters()</span></code> can conveniently be loaded and instantiated
directly from a checkpoint with <code class="xref py py-meth docutils literal notranslate"><span class="pre">load_from_checkpoint()</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># to load specify the other args</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LitMNIST</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span><span class="n">PATH</span><span class="p">,</span> <span class="n">loss_fx</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">SomeOtherLoss</span><span class="p">,</span> <span class="n">generator_network</span><span class="o">=</span><span class="n">MyGenerator</span><span class="p">())</span>
</pre></div>
</div>
<p>If parameters were excluded, they need to be provided at the time of loading:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the excluded parameters were `loss_fx` and `generator_network`</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LitMNIST</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span><span class="n">PATH</span><span class="p">,</span> <span class="n">loss_fx</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">SomeOtherLoss</span><span class="p">,</span> <span class="n">generator_network</span><span class="o">=</span><span class="n">MyGenerator</span><span class="p">())</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="child-modules">
<h2>Child Modules<a class="headerlink" href="#child-modules" title="이 표제에 대한 퍼머링크">¶</a></h2>
<p>Research projects tend to test different approaches to the same dataset.
This is very easy to do in Lightning with inheritance.</p>
<p>For example, imagine we now want to train an <code class="docutils literal notranslate"><span class="pre">AutoEncoder</span></code> to use as a feature extractor for images.
The only things that change in the <code class="docutils literal notranslate"><span class="pre">LitAutoEncoder</span></code> model are the init, forward, training, validation and test step.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="o">...</span>


<span class="k">class</span> <span class="nc">Decoder</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="o">...</span>


<span class="k">class</span> <span class="nc">AutoEncoder</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">Decoder</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>


<span class="k">class</span> <span class="nc">LitAutoEncoder</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">auto_encoder</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">auto_encoder</span> <span class="o">=</span> <span class="n">auto_encoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metric</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">auto_encoder</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="n">x_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">auto_encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">metric</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_hat</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_shared_eval</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="s2">&quot;val&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_shared_eval</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="s2">&quot;test&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_shared_eval</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">prefix</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="n">x_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">auto_encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">metric</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_hat</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">_loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
<p>and we can train this using the <code class="docutils literal notranslate"><span class="pre">Trainer</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">auto_encoder</span> <span class="o">=</span> <span class="n">AutoEncoder</span><span class="p">()</span>
<span class="n">lightning_module</span> <span class="o">=</span> <span class="n">LitAutoEncoder</span><span class="p">(</span><span class="n">auto_encoder</span><span class="p">)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">()</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">lightning_module</span><span class="p">,</span> <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">val_dataloader</span><span class="p">)</span>
</pre></div>
</div>
<p>And remember that the forward method should define the practical use of a <a class="reference internal" href="../api/lightning.pytorch.core.LightningModule.html#lightning.pytorch.core.LightningModule" title="lightning.pytorch.core.module.LightningModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code></a>.
In this case, we want to use the <code class="docutils literal notranslate"><span class="pre">LitAutoEncoder</span></code> to extract image representations:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">some_images</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
<span class="n">representations</span> <span class="o">=</span> <span class="n">lightning_module</span><span class="p">(</span><span class="n">some_images</span><span class="p">)</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="lightningmodule-api">
<h2>LightningModule API<a class="headerlink" href="#lightningmodule-api" title="이 표제에 대한 퍼머링크">¶</a></h2>
<section id="methods">
<h3>Methods<a class="headerlink" href="#methods" title="이 표제에 대한 퍼머링크">¶</a></h3>
<section id="all-gather">
<h4>all_gather<a class="headerlink" href="#all-gather" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">all_gather</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_grads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lightning/pytorch/core/module.html#LightningModule.all_gather"><span class="viewcode-link"><span class="pre">[소스]</span></span></a></dt>
<dd><p>Gather tensors or collections of tensors from multiple processes.</p>
<p>This method needs to be called on all processes. Failing to do so will cause your program to stall forever.</p>
<dl class="field-list simple">
<dt class="field-odd">매개변수</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.all_gather.params.data"></span><strong>data</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.all_gather.params.data">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code></a>[<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.List" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Tuple" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code></a>]) – int, float, tensor of shape (batch, …), or a (possibly nested) collection thereof.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.all_gather.params.group"></span><strong>group</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.all_gather.params.group">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></a>]) – the process group to gather results from. Defaults to all processes (world)</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.all_gather.params.sync_grads"></span><strong>sync_grads</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.all_gather.params.sync_grads">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></a>) – flag that allows users to synchronize gradients for the all_gather operation</p></li>
</ul>
</dd>
<dt class="field-even">반환 형식</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code></a>[<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.List" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Tuple" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code></a>]</p>
</dd>
<dt class="field-odd">반환</dt>
<dd class="field-odd"><p>A tensor of shape (world_size, batch, …), or if the input was a collection
the output will also be a collection with tensors of this shape.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="configure-callbacks">
<h4>configure_callbacks<a class="headerlink" href="#configure-callbacks" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">configure_callbacks</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lightning/pytorch/core/module.html#LightningModule.configure_callbacks"><span class="viewcode-link"><span class="pre">[소스]</span></span></a></dt>
<dd><p>Configure model-specific callbacks. When the model gets attached, e.g., when <code class="docutils literal notranslate"><span class="pre">.fit()</span></code> or <code class="docutils literal notranslate"><span class="pre">.test()</span></code>
gets called, the list or a callback returned here will be merged with the list of callbacks passed to the
Trainer’s <code class="docutils literal notranslate"><span class="pre">callbacks</span></code> argument. If a callback returned here has the same type as one or several callbacks
already present in the Trainer’s callbacks list, it will take priority and replace them. In addition,
Lightning will make sure <a class="reference internal" href="../api/lightning.pytorch.callbacks.ModelCheckpoint.html#lightning.pytorch.callbacks.ModelCheckpoint" title="lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModelCheckpoint</span></code></a> callbacks
run last.</p>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Sequence" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code></a>[<a class="reference internal" href="../api/lightning.pytorch.callbacks.Callback.html#lightning.pytorch.callbacks.Callback" title="lightning.pytorch.callbacks.callback.Callback"><code class="xref py py-class docutils literal notranslate"><span class="pre">Callback</span></code></a>], <a class="reference internal" href="../api/lightning.pytorch.callbacks.Callback.html#lightning.pytorch.callbacks.Callback" title="lightning.pytorch.callbacks.callback.Callback"><code class="xref py py-class docutils literal notranslate"><span class="pre">Callback</span></code></a>]</p>
</dd>
<dt class="field-even">반환</dt>
<dd class="field-even"><p>A callback or a list of callbacks which will extend the list of callbacks in the Trainer.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">configure_callbacks</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">early_stop</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s2">&quot;val_acc&quot;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;max&quot;</span><span class="p">)</span>
    <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s2">&quot;val_loss&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">early_stop</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">]</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="configure-optimizers">
<h4>configure_optimizers<a class="headerlink" href="#configure-optimizers" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">configure_optimizers</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lightning/pytorch/core/module.html#LightningModule.configure_optimizers"><span class="viewcode-link"><span class="pre">[소스]</span></span></a></dt>
<dd><p>Choose what optimizers and learning-rate schedulers to use in your optimization. Normally you’d need
one. But in the case of GANs or similar you might have multiple. Optimization with multiple optimizers only
works in the manual optimization mode.</p>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></a></p>
</dd>
<dt class="field-even">반환</dt>
<dd class="field-even"><p><p>Any of these 6 options.</p>
<ul class="simple">
<li><p><strong>Single optimizer</strong>.</p></li>
<li><p><strong>List or Tuple</strong> of optimizers.</p></li>
<li><p><strong>Two lists</strong> - The first list has multiple optimizers, and the second has multiple LR schedulers
(or multiple <code class="docutils literal notranslate"><span class="pre">lr_scheduler_config</span></code>).</p></li>
<li><p><strong>Dictionary</strong>, with an <code class="docutils literal notranslate"><span class="pre">&quot;optimizer&quot;</span></code> key, and (optionally) a <code class="docutils literal notranslate"><span class="pre">&quot;lr_scheduler&quot;</span></code>
key whose value is a single LR scheduler or <code class="docutils literal notranslate"><span class="pre">lr_scheduler_config</span></code>.</p></li>
<li><p><strong>None</strong> - Fit will run without any optimizer.</p></li>
</ul>
</p>
</dd>
</dl>
<p>The <code class="docutils literal notranslate"><span class="pre">lr_scheduler_config</span></code> is a dictionary which contains the scheduler and its associated configuration.
The default configuration is shown below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">lr_scheduler_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="c1"># REQUIRED: The scheduler instance</span>
    <span class="s2">&quot;scheduler&quot;</span><span class="p">:</span> <span class="n">lr_scheduler</span><span class="p">,</span>
    <span class="c1"># The unit of the scheduler&#39;s step size, could also be &#39;step&#39;.</span>
    <span class="c1"># &#39;epoch&#39; updates the scheduler on epoch end whereas &#39;step&#39;</span>
    <span class="c1"># updates it after a optimizer update.</span>
    <span class="s2">&quot;interval&quot;</span><span class="p">:</span> <span class="s2">&quot;epoch&quot;</span><span class="p">,</span>
    <span class="c1"># How many epochs/steps should pass between calls to</span>
    <span class="c1"># `scheduler.step()`. 1 corresponds to updating the learning</span>
    <span class="c1"># rate after every epoch/step.</span>
    <span class="s2">&quot;frequency&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="c1"># Metric to to monitor for schedulers like `ReduceLROnPlateau`</span>
    <span class="s2">&quot;monitor&quot;</span><span class="p">:</span> <span class="s2">&quot;val_loss&quot;</span><span class="p">,</span>
    <span class="c1"># If set to `True`, will enforce that the value specified &#39;monitor&#39;</span>
    <span class="c1"># is available when the scheduler is updated, thus stopping</span>
    <span class="c1"># training if not found. If set to `False`, it will only produce a warning</span>
    <span class="s2">&quot;strict&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="c1"># If using the `LearningRateMonitor` callback to monitor the</span>
    <span class="c1"># learning rate progress, this keyword can be used to specify</span>
    <span class="c1"># a custom logged name</span>
    <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>When there are schedulers in which the <code class="docutils literal notranslate"><span class="pre">.step()</span></code> method is conditioned on a value, such as the
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.lr_scheduler.ReduceLROnPlateau</span></code></a> scheduler, Lightning requires that the
<code class="docutils literal notranslate"><span class="pre">lr_scheduler_config</span></code> contains the keyword <code class="docutils literal notranslate"><span class="pre">&quot;monitor&quot;</span></code> set to the metric name that the scheduler
should be conditioned on.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># The ReduceLROnPlateau scheduler requires a monitor</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;optimizer&quot;</span><span class="p">:</span> <span class="n">optimizer</span><span class="p">,</span>
        <span class="s2">&quot;lr_scheduler&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;scheduler&quot;</span><span class="p">:</span> <span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="o">...</span><span class="p">),</span>
            <span class="s2">&quot;monitor&quot;</span><span class="p">:</span> <span class="s2">&quot;metric_to_track&quot;</span><span class="p">,</span>
            <span class="s2">&quot;frequency&quot;</span><span class="p">:</span> <span class="s2">&quot;indicates how often the metric is updated&quot;</span>
            <span class="c1"># If &quot;monitor&quot; references validation metrics, then &quot;frequency&quot; should be set to a</span>
            <span class="c1"># multiple of &quot;trainer.check_val_every_n_epoch&quot;.</span>
        <span class="p">},</span>
    <span class="p">}</span>


<span class="c1"># In the case of two optimizers, only one using the ReduceLROnPlateau scheduler</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">optimizer1</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">optimizer2</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">scheduler1</span> <span class="o">=</span> <span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">optimizer1</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
    <span class="n">scheduler2</span> <span class="o">=</span> <span class="n">LambdaLR</span><span class="p">(</span><span class="n">optimizer2</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="p">{</span>
            <span class="s2">&quot;optimizer&quot;</span><span class="p">:</span> <span class="n">optimizer1</span><span class="p">,</span>
            <span class="s2">&quot;lr_scheduler&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;scheduler&quot;</span><span class="p">:</span> <span class="n">scheduler1</span><span class="p">,</span>
                <span class="s2">&quot;monitor&quot;</span><span class="p">:</span> <span class="s2">&quot;metric_to_track&quot;</span><span class="p">,</span>
            <span class="p">},</span>
        <span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;optimizer&quot;</span><span class="p">:</span> <span class="n">optimizer2</span><span class="p">,</span> <span class="s2">&quot;lr_scheduler&quot;</span><span class="p">:</span> <span class="n">scheduler2</span><span class="p">},</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>Metrics can be made available to monitor by simply logging it using
<code class="docutils literal notranslate"><span class="pre">self.log('metric_to_track',</span> <span class="pre">metric_val)</span></code> in your <a class="reference internal" href="../api/lightning.pytorch.core.LightningModule.html#lightning.pytorch.core.LightningModule" title="lightning.pytorch.core.module.LightningModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code></a>.</p>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>Some things to know:</p>
<ul class="simple">
<li><p>Lightning calls <code class="docutils literal notranslate"><span class="pre">.backward()</span></code> and <code class="docutils literal notranslate"><span class="pre">.step()</span></code> automatically in case of automatic optimization.</p></li>
<li><p>If a learning rate scheduler is specified in <code class="docutils literal notranslate"><span class="pre">configure_optimizers()</span></code> with key
<code class="docutils literal notranslate"><span class="pre">&quot;interval&quot;</span></code> (default “epoch”) in the scheduler configuration, Lightning will call
the scheduler’s <code class="docutils literal notranslate"><span class="pre">.step()</span></code> method automatically in case of automatic optimization.</p></li>
<li><p>If you use 16-bit precision (<code class="docutils literal notranslate"><span class="pre">precision=16</span></code>), Lightning will automatically handle the optimizer.</p></li>
<li><p>If you use <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.optim.LBFGS.html#torch.optim.LBFGS" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.LBFGS</span></code></a>, Lightning handles the closure function automatically for you.</p></li>
<li><p>If you use multiple optimizers, you will have to switch to ‘manual optimization’ mode and step them
yourself.</p></li>
<li><p>If you need to control how often the optimizer steps, override the <code class="xref py py-meth docutils literal notranslate"><span class="pre">optimizer_step()</span></code> hook.</p></li>
</ul>
</div>
</dd></dl>

</section>
<section id="forward">
<h4>forward<a class="headerlink" href="#forward" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lightning/pytorch/core/module.html#LightningModule.forward"><span class="viewcode-link"><span class="pre">[소스]</span></span></a></dt>
<dd><p>Same as <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.forward" title="(PyTorch v2.0에서)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.forward()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">매개변수</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.forward.params.*args"></span><strong>*args</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.forward.params.*args">¶</a> – Whatever you decide to pass into the forward method.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.forward.params.**kwargs"></span><strong>**kwargs</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.forward.params.**kwargs">¶</a> – Keyword arguments are also possible.</p></li>
</ul>
</dd>
<dt class="field-even">반환 형식</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></a></p>
</dd>
<dt class="field-odd">반환</dt>
<dd class="field-odd"><p>Your model’s output</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="freeze">
<h4>freeze<a class="headerlink" href="#freeze" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">freeze</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lightning/pytorch/core/module.html#LightningModule.freeze"><span class="viewcode-link"><span class="pre">[소스]</span></span></a></dt>
<dd><p>Freeze all params for inference.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">MyLightningModule</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="log">
<span id="lm-log"></span><h4>log<a class="headerlink" href="#log" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">log</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prog_bar</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logger</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_step</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_epoch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce_fx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_graph</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist_group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_dataloader_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric_attribute</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rank_zero_only</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lightning/pytorch/core/module.html#LightningModule.log"><span class="viewcode-link"><span class="pre">[소스]</span></span></a></dt>
<dd><p>Log a key, value pair.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s1">&#39;train_loss&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
<p>The default behavior per hook is documented here: <a class="reference internal" href="../extensions/logging.html#automatic-logging"><span class="std std-ref">Automatic Logging</span></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">매개변수</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.log.params.name"></span><strong>name</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.log.params.name">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a>) – key to log.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.log.params.value"></span><strong>value</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.log.params.value">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code></a>[<a class="reference external" href="https://torchmetrics.readthedocs.io/en/stable/pages/implement.html#torchmetrics.Metric" title="(PyTorch-Metrics v1.0.1에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Metric</span></code></a>, <a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a>]) – value to log. Can be a <code class="docutils literal notranslate"><span class="pre">float</span></code>, <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>, or a <code class="docutils literal notranslate"><span class="pre">Metric</span></code>.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.log.params.prog_bar"></span><strong>prog_bar</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.log.params.prog_bar">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></a>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code> logs to the progress bar.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.log.params.logger"></span><strong>logger</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.log.params.logger">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></a>]) – if <code class="docutils literal notranslate"><span class="pre">True</span></code> logs to the logger.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.log.params.on_step"></span><strong>on_step</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.log.params.on_step">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></a>]) – if <code class="docutils literal notranslate"><span class="pre">True</span></code> logs at this step. The default value is determined by the hook.
See <a class="reference internal" href="../extensions/logging.html#automatic-logging"><span class="std std-ref">Automatic Logging</span></a> for details.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.log.params.on_epoch"></span><strong>on_epoch</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.log.params.on_epoch">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></a>]) – if <code class="docutils literal notranslate"><span class="pre">True</span></code> logs epoch accumulated metrics. The default value is determined by the hook.
See <a class="reference internal" href="../extensions/logging.html#automatic-logging"><span class="std std-ref">Automatic Logging</span></a> for details.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.log.params.reduce_fx"></span><strong>reduce_fx</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.log.params.reduce_fx">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code></a>]) – reduction function over step values for end of epoch. <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.mean()</span></code> by default.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.log.params.enable_graph"></span><strong>enable_graph</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.log.params.enable_graph">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></a>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, will not auto detach the graph.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.log.params.sync_dist"></span><strong>sync_dist</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.log.params.sync_dist">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></a>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, reduces the metric across devices. Use with care as this may lead to a significant
communication overhead.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.log.params.sync_dist_group"></span><strong>sync_dist_group</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.log.params.sync_dist_group">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></a>]) – the DDP group to sync across.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.log.params.add_dataloader_idx"></span><strong>add_dataloader_idx</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.log.params.add_dataloader_idx">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></a>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, appends the index of the current dataloader to
the name (when using multiple dataloaders). If False, user needs to give unique names for
each dataloader to not mix the values.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.log.params.batch_size"></span><strong>batch_size</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.log.params.batch_size">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a>]) – Current batch_size. This will be directly inferred from the loaded batch,
but for some data structures you might need to explicitly provide it.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.log.params.metric_attribute"></span><strong>metric_attribute</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.log.params.metric_attribute">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a>]) – To restore the metric state, Lightning requires the reference of the
<a class="reference external" href="https://torchmetrics.readthedocs.io/en/stable/pages/implement.html#torchmetrics.Metric" title="(PyTorch-Metrics v1.0.1에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchmetrics.Metric</span></code></a> in your model. This is found automatically if it is a model attribute.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.log.params.rank_zero_only"></span><strong>rank_zero_only</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.log.params.rank_zero_only">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></a>) – Whether the value will be logged only on rank 0. This will prevent synchronization which
would produce a deadlock as not all processes would perform this log call.</p></li>
</ul>
</dd>
<dt class="field-even">반환 형식</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="log-dict">
<h4>log_dict<a class="headerlink" href="#log-dict" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">log_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dictionary</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prog_bar</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logger</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_step</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_epoch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce_fx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_graph</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist_group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_dataloader_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rank_zero_only</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lightning/pytorch/core/module.html#LightningModule.log_dict"><span class="viewcode-link"><span class="pre">[소스]</span></span></a></dt>
<dd><p>Log a dictionary of values at once.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">values</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;acc&#39;</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="s1">&#39;metric_n&#39;</span><span class="p">:</span> <span class="n">metric_n</span><span class="p">}</span>
<span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">매개변수</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.log_dict.params.dictionary"></span><strong>dictionary</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.log_dict.params.dictionary">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Mapping" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Mapping</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code></a>[<a class="reference external" href="https://torchmetrics.readthedocs.io/en/stable/pages/implement.html#torchmetrics.Metric" title="(PyTorch-Metrics v1.0.1에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Metric</span></code></a>, <a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a>]]) – key value pairs.
The values can be a <code class="docutils literal notranslate"><span class="pre">float</span></code>, <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="docutils literal notranslate"><span class="pre">Metric</span></code>, or <code class="docutils literal notranslate"><span class="pre">MetricCollection</span></code>.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.log_dict.params.prog_bar"></span><strong>prog_bar</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.log_dict.params.prog_bar">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></a>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code> logs to the progress base.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.log_dict.params.logger"></span><strong>logger</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.log_dict.params.logger">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></a>]) – if <code class="docutils literal notranslate"><span class="pre">True</span></code> logs to the logger.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.log_dict.params.on_step"></span><strong>on_step</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.log_dict.params.on_step">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></a>]) – if <code class="docutils literal notranslate"><span class="pre">True</span></code> logs at this step.
<code class="docutils literal notranslate"><span class="pre">None</span></code> auto-logs for training_step but not validation/test_step.
The default value is determined by the hook.
See <a class="reference internal" href="../extensions/logging.html#automatic-logging"><span class="std std-ref">Automatic Logging</span></a> for details.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.log_dict.params.on_epoch"></span><strong>on_epoch</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.log_dict.params.on_epoch">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></a>]) – if <code class="docutils literal notranslate"><span class="pre">True</span></code> logs epoch accumulated metrics.
<code class="docutils literal notranslate"><span class="pre">None</span></code> auto-logs for val/test step but not <code class="docutils literal notranslate"><span class="pre">training_step</span></code>.
The default value is determined by the hook.
See <a class="reference internal" href="../extensions/logging.html#automatic-logging"><span class="std std-ref">Automatic Logging</span></a> for details.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.log_dict.params.reduce_fx"></span><strong>reduce_fx</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.log_dict.params.reduce_fx">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code></a>]) – reduction function over step values for end of epoch. <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.mean()</span></code> by default.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.log_dict.params.enable_graph"></span><strong>enable_graph</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.log_dict.params.enable_graph">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></a>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, will not auto-detach the graph</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.log_dict.params.sync_dist"></span><strong>sync_dist</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.log_dict.params.sync_dist">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></a>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, reduces the metric across GPUs/TPUs. Use with care as this may lead to a significant
communication overhead.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.log_dict.params.sync_dist_group"></span><strong>sync_dist_group</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.log_dict.params.sync_dist_group">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></a>]) – the ddp group to sync across.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.log_dict.params.add_dataloader_idx"></span><strong>add_dataloader_idx</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.log_dict.params.add_dataloader_idx">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></a>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, appends the index of the current dataloader to
the name (when using multiple). If <code class="docutils literal notranslate"><span class="pre">False</span></code>, user needs to give unique names for
each dataloader to not mix values.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.log_dict.params.batch_size"></span><strong>batch_size</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.log_dict.params.batch_size">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a>]) – Current batch size. This will be directly inferred from the loaded batch,
but some data structures might need to explicitly provide it.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.log_dict.params.rank_zero_only"></span><strong>rank_zero_only</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.log_dict.params.rank_zero_only">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></a>) – Whether the value will be logged only on rank 0. This will prevent synchronization which
would produce a deadlock as not all processes would perform this log call.</p></li>
</ul>
</dd>
<dt class="field-even">반환 형식</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="lr-schedulers">
<h4>lr_schedulers<a class="headerlink" href="#lr-schedulers" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">lr_schedulers</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lightning/pytorch/core/module.html#LightningModule.lr_schedulers"><span class="viewcode-link"><span class="pre">[소스]</span></span></a></dt>
<dd><p>Returns the learning rate scheduler(s) that are being used during training. Useful for manual
optimization.</p>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.List" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code></a>[<code class="xref py py-class docutils literal notranslate"><span class="pre">LRScheduler</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">ReduceLROnPlateau</span></code>]], <code class="xref py py-class docutils literal notranslate"><span class="pre">LRScheduler</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">ReduceLROnPlateau</span></code>]</p>
</dd>
<dt class="field-even">반환</dt>
<dd class="field-even"><p>A single scheduler, or a list of schedulers in case multiple ones are present, or <code class="docutils literal notranslate"><span class="pre">None</span></code> if no
schedulers were returned in <code class="xref py py-meth docutils literal notranslate"><span class="pre">configure_optimizers()</span></code>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="manual-backward">
<h4>manual_backward<a class="headerlink" href="#manual-backward" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">manual_backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lightning/pytorch/core/module.html#LightningModule.manual_backward"><span class="viewcode-link"><span class="pre">[소스]</span></span></a></dt>
<dd><p>Call this directly from your <code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code> when doing optimizations manually. By using this,
Lightning can ensure that all the proper scaling gets applied when using mixed precision.</p>
<p>See <a class="reference internal" href="optimization.html#id2"><span class="std std-ref">manual optimization</span></a> for more examples.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizers</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">...</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="c1"># automatically applies scaling, etc...</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">manual_backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">매개변수</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.manual_backward.params.loss"></span><strong>loss</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.manual_backward.params.loss">¶</a> (<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>) – The tensor on which to compute gradients. Must have a graph attached.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.manual_backward.params.*args"></span><strong>*args</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.manual_backward.params.*args">¶</a> – Additional positional arguments to be forwarded to <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html#torch.Tensor.backward" title="(PyTorch v2.0에서)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">backward()</span></code></a></p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.manual_backward.params.**kwargs"></span><strong>**kwargs</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.manual_backward.params.**kwargs">¶</a> – Additional keyword arguments to be forwarded to <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html#torch.Tensor.backward" title="(PyTorch v2.0에서)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">backward()</span></code></a></p></li>
</ul>
</dd>
<dt class="field-even">반환 형식</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="optimizers">
<h4>optimizers<a class="headerlink" href="#optimizers" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">optimizers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">use_pl_optimizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Literal" title="(Python v3.11에서)"><span class="pre">Literal</span></a><span class="p"><span class="pre">[</span></span><span class="k"><span class="pre">True</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(Python v3.11에서)"><span class="pre">Union</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="../api/lightning.pytorch.core.optimizer.LightningOptimizer.html#lightning.pytorch.core.optimizer.LightningOptimizer" title="lightning.pytorch.core.optimizer.LightningOptimizer"><span class="pre">lightning.pytorch.core.optimizer.LightningOptimizer</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.List" title="(Python v3.11에서)"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="../api/lightning.pytorch.core.optimizer.LightningOptimizer.html#lightning.pytorch.core.optimizer.LightningOptimizer" title="lightning.pytorch.core.optimizer.LightningOptimizer"><span class="pre">lightning.pytorch.core.optimizer.LightningOptimizer</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/lightning/pytorch/core/module.html#LightningModule.optimizers"><span class="viewcode-link"><span class="pre">[소스]</span></span></a></dt>
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">optimizers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">use_pl_optimizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Literal" title="(Python v3.11에서)"><span class="pre">Literal</span></a><span class="p"><span class="pre">[</span></span><span class="k"><span class="pre">False</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(Python v3.11에서)"><span class="pre">Union</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(PyTorch v2.0에서)"><span class="pre">torch.optim.optimizer.Optimizer</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.List" title="(Python v3.11에서)"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(PyTorch v2.0에서)"><span class="pre">torch.optim.optimizer.Optimizer</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span></dt>
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">optimizers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">use_pl_optimizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(Python v3.11에서)"><span class="pre">bool</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(Python v3.11에서)"><span class="pre">Union</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(PyTorch v2.0에서)"><span class="pre">torch.optim.optimizer.Optimizer</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="../api/lightning.pytorch.core.optimizer.LightningOptimizer.html#lightning.pytorch.core.optimizer.LightningOptimizer" title="lightning.pytorch.core.optimizer.LightningOptimizer"><span class="pre">lightning.pytorch.core.optimizer.LightningOptimizer</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">lightning.fabric.wrappers._FabricOptimizer</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.List" title="(Python v3.11에서)"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(PyTorch v2.0에서)"><span class="pre">torch.optim.optimizer.Optimizer</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.List" title="(Python v3.11에서)"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="../api/lightning.pytorch.core.optimizer.LightningOptimizer.html#lightning.pytorch.core.optimizer.LightningOptimizer" title="lightning.pytorch.core.optimizer.LightningOptimizer"><span class="pre">lightning.pytorch.core.optimizer.LightningOptimizer</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.List" title="(Python v3.11에서)"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><span class="pre">lightning.fabric.wrappers._FabricOptimizer</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span></dt>
<dd><p>Returns the optimizer(s) that are being used during training. Useful for manual optimization.</p>
<dl class="field-list simple">
<dt class="field-odd">매개변수</dt>
<dd class="field-odd"><p><span class="target" id="lightning.pytorch.core.module.LightningModule.optimizers.params.use_pl_optimizer"></span><strong>use_pl_optimizer</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.optimizers.params.use_pl_optimizer">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></a>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, will wrap the optimizer(s) in a
<a class="reference internal" href="../api/lightning.pytorch.core.optimizer.LightningOptimizer.html#lightning.pytorch.core.optimizer.LightningOptimizer" title="lightning.pytorch.core.optimizer.LightningOptimizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code></a> for automatic handling of precision and
profiling.</p>
</dd>
<dt class="field-even">반환 형식</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code></a>[<a class="reference external" href="https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code></a>, <a class="reference internal" href="../api/lightning.pytorch.core.optimizer.LightningOptimizer.html#lightning.pytorch.core.optimizer.LightningOptimizer" title="lightning.pytorch.core.optimizer.LightningOptimizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">_FabricOptimizer</span></code>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.List" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code></a>[<a class="reference external" href="https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code></a>], <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.List" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code></a>[<a class="reference internal" href="../api/lightning.pytorch.core.optimizer.LightningOptimizer.html#lightning.pytorch.core.optimizer.LightningOptimizer" title="lightning.pytorch.core.optimizer.LightningOptimizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code></a>], <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.List" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code></a>[<code class="xref py py-class docutils literal notranslate"><span class="pre">_FabricOptimizer</span></code>]]</p>
</dd>
<dt class="field-odd">반환</dt>
<dd class="field-odd"><p>A single optimizer, or a list of optimizers in case multiple ones are present.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="print">
<h4>print<a class="headerlink" href="#print" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">print</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lightning/pytorch/core/module.html#LightningModule.print"><span class="viewcode-link"><span class="pre">[소스]</span></span></a></dt>
<dd><p>Prints only from process 0. Use this in any distributed mode to log only once.</p>
<dl class="field-list simple">
<dt class="field-odd">매개변수</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.print.params.*args"></span><strong>*args</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.print.params.*args">¶</a> – The thing to print. The same as for Python’s built-in print function.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.print.params.**kwargs"></span><strong>**kwargs</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.print.params.**kwargs">¶</a> – The same as for Python’s built-in print function.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;in forward&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="predict-step">
<h4>predict_step<a class="headerlink" href="#predict-step" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">predict_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lightning/pytorch/core/module.html#LightningModule.predict_step"><span class="viewcode-link"><span class="pre">[소스]</span></span></a></dt>
<dd><p>Step function called during <a class="reference internal" href="../api/lightning.pytorch.trainer.trainer.Trainer.html#lightning.pytorch.trainer.trainer.Trainer.predict" title="lightning.pytorch.trainer.trainer.Trainer.predict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">predict()</span></code></a>. By default, it
calls <code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code>. Override to add any processing logic.</p>
<p>The <code class="xref py py-meth docutils literal notranslate"><span class="pre">predict_step()</span></code> is used
to scale inference on multi-devices.</p>
<p>To prevent an OOM error, it is possible to use <a class="reference internal" href="../api/lightning.pytorch.callbacks.BasePredictionWriter.html#lightning.pytorch.callbacks.BasePredictionWriter" title="lightning.pytorch.callbacks.BasePredictionWriter"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePredictionWriter</span></code></a>
callback to write the predictions to disk or database after each batch or on epoch end.</p>
<p>The <a class="reference internal" href="../api/lightning.pytorch.callbacks.BasePredictionWriter.html#lightning.pytorch.callbacks.BasePredictionWriter" title="lightning.pytorch.callbacks.BasePredictionWriter"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePredictionWriter</span></code></a> should be used while using a spawn
based accelerator. This happens for <code class="docutils literal notranslate"><span class="pre">Trainer(strategy=&quot;ddp_spawn&quot;)</span></code>
or training on 8 TPU cores with <code class="docutils literal notranslate"><span class="pre">Trainer(accelerator=&quot;tpu&quot;,</span> <span class="pre">devices=8)</span></code> as predictions won’t be returned.</p>
<p>Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyModel</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">predict_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

<span class="n">dm</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dm</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">매개변수</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.predict_step.params.batch"></span><strong>batch</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.predict_step.params.batch">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></a>) – Current batch.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.predict_step.params.batch_idx"></span><strong>batch_idx</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.predict_step.params.batch_idx">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a>) – Index of current batch.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.predict_step.params.dataloader_idx"></span><strong>dataloader_idx</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.predict_step.params.dataloader_idx">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a>) – Index of the current dataloader.</p></li>
</ul>
</dd>
<dt class="field-even">반환 형식</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></a></p>
</dd>
<dt class="field-odd">반환</dt>
<dd class="field-odd"><p>Predicted output</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id2">
<h4>save_hyperparameters<a class="headerlink" href="#id2" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">save_hyperparameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">frame</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logger</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Save arguments to <code class="docutils literal notranslate"><span class="pre">hparams</span></code> attribute.</p>
<dl class="field-list simple">
<dt class="field-odd">매개변수</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.save_hyperparameters.params.args"></span><strong>args</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.save_hyperparameters.params.args">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></a>) – single object of <cite>dict</cite>, <cite>NameSpace</cite> or <cite>OmegaConf</cite>
or string names or arguments from class <code class="docutils literal notranslate"><span class="pre">__init__</span></code></p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.save_hyperparameters.params.ignore"></span><strong>ignore</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.save_hyperparameters.params.ignore">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Sequence" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a>], <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a>]) – an argument name or a list of argument names from
class <code class="docutils literal notranslate"><span class="pre">__init__</span></code> to be ignored</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.save_hyperparameters.params.frame"></span><strong>frame</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.save_hyperparameters.params.frame">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code></a>[<code class="xref py py-class docutils literal notranslate"><span class="pre">frame</span></code>]) – a frame object. Default is None</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.save_hyperparameters.params.logger"></span><strong>logger</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.save_hyperparameters.params.logger">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></a>) – Whether to send the hyperparameters to the logger. Default: True</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">lightning.pytorch.core.mixins</span> <span class="kn">import</span> <span class="n">HyperparametersMixin</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">ManuallyArgsModel</span><span class="p">(</span><span class="n">HyperparametersMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="p">,</span> <span class="n">arg3</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="c1"># manually assign arguments</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">(</span><span class="s1">&#39;arg1&#39;</span><span class="p">,</span> <span class="s1">&#39;arg3&#39;</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="gp">... </span>        <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ManuallyArgsModel</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;abc&#39;</span><span class="p">,</span> <span class="mf">3.14</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">hparams</span>
<span class="go">&quot;arg1&quot;: 1</span>
<span class="go">&quot;arg3&quot;: 3.14</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">lightning.pytorch.core.mixins</span> <span class="kn">import</span> <span class="n">HyperparametersMixin</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">AutomaticArgsModel</span><span class="p">(</span><span class="n">HyperparametersMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="p">,</span> <span class="n">arg3</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="c1"># equivalent automatic</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">()</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="gp">... </span>        <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutomaticArgsModel</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;abc&#39;</span><span class="p">,</span> <span class="mf">3.14</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">hparams</span>
<span class="go">&quot;arg1&quot;: 1</span>
<span class="go">&quot;arg2&quot;: abc</span>
<span class="go">&quot;arg3&quot;: 3.14</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">lightning.pytorch.core.mixins</span> <span class="kn">import</span> <span class="n">HyperparametersMixin</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">SingleArgModel</span><span class="p">(</span><span class="n">HyperparametersMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="c1"># manually assign single argument</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="gp">... </span>        <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">SingleArgModel</span><span class="p">(</span><span class="n">Namespace</span><span class="p">(</span><span class="n">p1</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">p2</span><span class="o">=</span><span class="s1">&#39;abc&#39;</span><span class="p">,</span> <span class="n">p3</span><span class="o">=</span><span class="mf">3.14</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">hparams</span>
<span class="go">&quot;p1&quot;: 1</span>
<span class="go">&quot;p2&quot;: abc</span>
<span class="go">&quot;p3&quot;: 3.14</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">lightning.pytorch.core.mixins</span> <span class="kn">import</span> <span class="n">HyperparametersMixin</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">ManuallyArgsModel</span><span class="p">(</span><span class="n">HyperparametersMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="p">,</span> <span class="n">arg3</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="c1"># pass argument(s) to ignore as a string or in a list</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">(</span><span class="n">ignore</span><span class="o">=</span><span class="s1">&#39;arg2&#39;</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="gp">... </span>        <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ManuallyArgsModel</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;abc&#39;</span><span class="p">,</span> <span class="mf">3.14</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">hparams</span>
<span class="go">&quot;arg1&quot;: 1</span>
<span class="go">&quot;arg3&quot;: 3.14</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="toggle-optimizer">
<h4>toggle_optimizer<a class="headerlink" href="#toggle-optimizer" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">toggle_optimizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lightning/pytorch/core/module.html#LightningModule.toggle_optimizer"><span class="viewcode-link"><span class="pre">[소스]</span></span></a></dt>
<dd><p>Makes sure only the gradients of the current optimizer’s parameters are calculated in the training step
to prevent dangling gradients in multiple-optimizer setup.</p>
<p>It works with <code class="xref py py-meth docutils literal notranslate"><span class="pre">untoggle_optimizer()</span></code> to make sure <code class="docutils literal notranslate"><span class="pre">param_requires_grad_state</span></code> is properly reset.</p>
<dl class="field-list simple">
<dt class="field-odd">매개변수</dt>
<dd class="field-odd"><p><span class="target" id="lightning.pytorch.core.module.LightningModule.toggle_optimizer.params.optimizer"></span><strong>optimizer</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.toggle_optimizer.params.optimizer">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code></a>[<a class="reference external" href="https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code></a>, <a class="reference internal" href="../api/lightning.pytorch.core.optimizer.LightningOptimizer.html#lightning.pytorch.core.optimizer.LightningOptimizer" title="lightning.pytorch.core.optimizer.LightningOptimizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code></a>]) – The optimizer to toggle.</p>
</dd>
<dt class="field-even">반환 형식</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="test-step">
<h4>test_step<a class="headerlink" href="#test-step" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">test_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lightning/pytorch/core/module.html#LightningModule.test_step"><span class="viewcode-link"><span class="pre">[소스]</span></span></a></dt>
<dd><p>Operates on a single batch of data from the test set. In this step you’d normally generate examples or
calculate anything of interest such as accuracy.</p>
<dl class="field-list simple">
<dt class="field-odd">매개변수</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.test_step.params.batch"></span><strong>batch</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.test_step.params.batch">¶</a> – The output of your <a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code></a>.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.test_step.params.batch_idx"></span><strong>batch_idx</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.test_step.params.batch_idx">¶</a> – The index of this batch.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.test_step.params.dataloader_id"></span><strong>dataloader_id</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.test_step.params.dataloader_id">¶</a> – The index of the dataloader that produced this batch.
(only if multiple test dataloaders used).</p></li>
</ul>
</dd>
<dt class="field-even">반환 형식</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code></a>[<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></a>], <a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a>]</p>
</dd>
<dt class="field-odd">반환</dt>
<dd class="field-odd"><p><p>Any of.</p>
<blockquote>
<div><ul class="simple">
<li><p>Any object or value</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> - Testing will skip to the next batch</p></li>
</ul>
</div></blockquote>
</p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># if you have one test dataloader:</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="o">...</span>


<span class="c1"># if you have multiple test dataloaders:</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="o">...</span>
</pre></div>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 1: A single test dataset</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="c1"># implement your own</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># log 6 example images</span>
    <span class="c1"># or generated text... or whatever</span>
    <span class="n">sample_imgs</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="mi">6</span><span class="p">]</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">make_grid</span><span class="p">(</span><span class="n">sample_imgs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_image</span><span class="p">(</span><span class="s1">&#39;example_images&#39;</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># calculate acc</span>
    <span class="n">labels_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">test_acc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">labels_hat</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span><span class="p">)</span>

    <span class="c1"># log the outputs!</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">({</span><span class="s1">&#39;test_loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;test_acc&#39;</span><span class="p">:</span> <span class="n">test_acc</span><span class="p">})</span>
</pre></div>
</div>
<p>If you pass in multiple test dataloaders, <code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code> will have an additional argument. We recommend
setting the default value of 0 so that you can quickly switch between single and multiple dataloaders.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 2: multiple test dataloaders</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="c1"># dataloader_idx tells you which dataset this is.</span>
    <span class="o">...</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>If you don’t need to test you don’t need to implement this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>When the <code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code> is called, the model has been put in eval mode and
PyTorch gradients have been disabled. At the end of the test epoch, the model goes back
to training mode and gradients are enabled.</p>
</div>
</dd></dl>

</section>
<section id="to-onnx">
<h4>to_onnx<a class="headerlink" href="#to-onnx" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">to_onnx</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_sample</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lightning/pytorch/core/module.html#LightningModule.to_onnx"><span class="viewcode-link"><span class="pre">[소스]</span></span></a></dt>
<dd><p>Saves the model in ONNX format.</p>
<dl class="field-list simple">
<dt class="field-odd">매개변수</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.to_onnx.params.file_path"></span><strong>file_path</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.to_onnx.params.file_path">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/pathlib.html#pathlib.Path" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Path</span></code></a>]) – The path of the file the onnx model should be saved to.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.to_onnx.params.input_sample"></span><strong>input_sample</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.to_onnx.params.input_sample">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></a>]) – An input for tracing. Default: None (Use self.example_input_array)</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.to_onnx.params.**kwargs"></span><strong>**kwargs</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.to_onnx.params.**kwargs">¶</a> – Will be passed to torch.onnx.export function.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SimpleModel</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SimpleModel</span><span class="p">()</span>
<span class="n">input_sample</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">to_onnx</span><span class="p">(</span><span class="s2">&quot;export.onnx&quot;</span><span class="p">,</span> <span class="n">input_sample</span><span class="p">,</span> <span class="n">export_params</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="to-torchscript">
<h4>to_torchscript<a class="headerlink" href="#to-torchscript" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">to_torchscript</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'script'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">example_inputs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lightning/pytorch/core/module.html#LightningModule.to_torchscript"><span class="viewcode-link"><span class="pre">[소스]</span></span></a></dt>
<dd><p>By default compiles the whole model to a <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ScriptModule</span></code></a>. If you want to use tracing,
please provided the argument <code class="docutils literal notranslate"><span class="pre">method='trace'</span></code> and make sure that either the <cite>example_inputs</cite> argument is
provided, or the model has <code class="xref py py-attr docutils literal notranslate"><span class="pre">example_input_array</span></code> set. If you would like to customize the modules that
are scripted you should override this method. In case you want to return multiple modules, we recommend
using a dictionary.</p>
<dl class="field-list simple">
<dt class="field-odd">매개변수</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.to_torchscript.params.file_path"></span><strong>file_path</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.to_torchscript.params.file_path">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/pathlib.html#pathlib.Path" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Path</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a>]) – Path where to save the torchscript. Default: None (no file saved).</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.to_torchscript.params.method"></span><strong>method</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.to_torchscript.params.method">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a>]) – Whether to use TorchScript’s script or trace method. Default: ‘script’</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.to_torchscript.params.example_inputs"></span><strong>example_inputs</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.to_torchscript.params.example_inputs">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></a>]) – An input to be used to do tracing when method is set to ‘trace’.
Default: None (uses <code class="xref py py-attr docutils literal notranslate"><span class="pre">example_input_array</span></code>)</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.to_torchscript.params.**kwargs"></span><strong>**kwargs</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.to_torchscript.params.**kwargs">¶</a> – Additional arguments that will be passed to the <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.jit.script.html#torch.jit.script" title="(PyTorch v2.0에서)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.jit.script()</span></code></a> or
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.jit.trace.html#torch.jit.trace" title="(PyTorch v2.0에서)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.jit.trace()</span></code></a> function.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">참고</p>
<ul class="simple">
<li><p>Requires the implementation of the
<code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code> method.</p></li>
<li><p>The exported script will be set to evaluation mode.</p></li>
<li><p>It is recommended that you install the latest supported version of PyTorch
to use this feature without limitations. See also the <a class="reference external" href="https://pytorch.org/docs/stable/jit.html#module-torch.jit" title="(PyTorch v2.0에서)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.jit</span></code></a>
documentation for supported features.</p></li>
</ul>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SimpleModel</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)))</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SimpleModel</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">to_torchscript</span><span class="p">(</span><span class="n">file_path</span><span class="o">=</span><span class="s2">&quot;model.pt&quot;</span><span class="p">)</span>

<span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">to_torchscript</span><span class="p">(</span>
    <span class="n">file_path</span><span class="o">=</span><span class="s2">&quot;model_trace.pt&quot;</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;trace&#39;</span><span class="p">,</span> <span class="n">example_inputs</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">))</span>
<span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code></a>[<code class="xref py py-class docutils literal notranslate"><span class="pre">ScriptModule</span></code>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">ScriptModule</span></code>]]</p>
</dd>
<dt class="field-even">반환</dt>
<dd class="field-even"><p>This LightningModule as a torchscript, regardless of whether <cite>file_path</cite> is
defined or not.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="training-step">
<h4>training_step<a class="headerlink" href="#training-step" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">training_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lightning/pytorch/core/module.html#LightningModule.training_step"><span class="viewcode-link"><span class="pre">[소스]</span></span></a></dt>
<dd><p>Here you compute and return the training loss and some additional metrics for e.g. the progress bar or
logger.</p>
<dl class="field-list simple">
<dt class="field-odd">매개변수</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.training_step.params.batch"></span><strong>batch</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.training_step.params.batch">¶</a> (<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a> | (<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>, …) | [<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>, …]) – The output of your <a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code></a>. A tensor, tuple or list.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.training_step.params.batch_idx"></span><strong>batch_idx</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.training_step.params.batch_idx">¶</a> (<code class="docutils literal notranslate"><span class="pre">int</span></code>) – Integer displaying index of this batch</p></li>
</ul>
</dd>
<dt class="field-even">반환 형식</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code></a>[<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></a>]]</p>
</dd>
<dt class="field-odd">반환</dt>
<dd class="field-odd"><p><p>Any of.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a> - The loss tensor</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dict</span></code> - A dictionary. Can include any keys, but must include the key <code class="docutils literal notranslate"><span class="pre">'loss'</span></code></p></li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">None</span></code> - Training will skip to the next batch. This is only for automatic optimization.</dt><dd><p>This is not supported for multi-GPU, TPU, IPU, or DeepSpeed.</p>
</dd>
</dl>
</li>
</ul>
</p>
</dd>
</dl>
<p>In this step you’d normally do the forward pass and calculate the loss for a batch.
You can also do fancier things like multiple forward passes or something model specific.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<p>To use multiple optimizers, you can switch to ‘manual optimization’ and control their stepping:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">automatic_optimization</span> <span class="o">=</span> <span class="kc">False</span>


<span class="c1"># Multiple optimizers (e.g.: GANs)</span>
<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">opt1</span><span class="p">,</span> <span class="n">opt2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizers</span><span class="p">()</span>

    <span class="c1"># do training_step with encoder</span>
    <span class="o">...</span>
    <span class="n">opt1</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="c1"># do training_step with decoder</span>
    <span class="o">...</span>
    <span class="n">opt2</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>When <code class="docutils literal notranslate"><span class="pre">accumulate_grad_batches</span></code> &gt; 1, the loss returned here will be automatically
normalized by <code class="docutils literal notranslate"><span class="pre">accumulate_grad_batches</span></code> internally.</p>
</div>
</dd></dl>

</section>
<section id="unfreeze">
<h4>unfreeze<a class="headerlink" href="#unfreeze" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">unfreeze</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lightning/pytorch/core/module.html#LightningModule.unfreeze"><span class="viewcode-link"><span class="pre">[소스]</span></span></a></dt>
<dd><p>Unfreeze all parameters for training.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">MyLightningModule</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">unfreeze</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="untoggle-optimizer">
<h4>untoggle_optimizer<a class="headerlink" href="#untoggle-optimizer" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">untoggle_optimizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lightning/pytorch/core/module.html#LightningModule.untoggle_optimizer"><span class="viewcode-link"><span class="pre">[소스]</span></span></a></dt>
<dd><p>Resets the state of required gradients that were toggled with <code class="xref py py-meth docutils literal notranslate"><span class="pre">toggle_optimizer()</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">매개변수</dt>
<dd class="field-odd"><p><span class="target" id="lightning.pytorch.core.module.LightningModule.untoggle_optimizer.params.optimizer"></span><strong>optimizer</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.untoggle_optimizer.params.optimizer">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code></a>[<a class="reference external" href="https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code></a>, <a class="reference internal" href="../api/lightning.pytorch.core.optimizer.LightningOptimizer.html#lightning.pytorch.core.optimizer.LightningOptimizer" title="lightning.pytorch.core.optimizer.LightningOptimizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code></a>]) – The optimizer to untoggle.</p>
</dd>
<dt class="field-even">반환 형식</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="validation-step">
<h4>validation_step<a class="headerlink" href="#validation-step" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">validation_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lightning/pytorch/core/module.html#LightningModule.validation_step"><span class="viewcode-link"><span class="pre">[소스]</span></span></a></dt>
<dd><p>Operates on a single batch of data from the validation set. In this step you’d might generate examples
or calculate anything of interest like accuracy.</p>
<dl class="field-list simple">
<dt class="field-odd">매개변수</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.validation_step.params.batch"></span><strong>batch</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.validation_step.params.batch">¶</a> – The output of your <a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code></a>.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.validation_step.params.batch_idx"></span><strong>batch_idx</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.validation_step.params.batch_idx">¶</a> – The index of this batch.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.validation_step.params.dataloader_idx"></span><strong>dataloader_idx</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.validation_step.params.dataloader_idx">¶</a> – The index of the dataloader that produced this batch.
(only if multiple val dataloaders used)</p></li>
</ul>
</dd>
<dt class="field-even">반환 형식</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code></a>[<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></a>], <a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a>]</p>
</dd>
<dt class="field-odd">반환</dt>
<dd class="field-odd"><p><ul class="simple">
<li><p>Any object or value</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> - Validation will skip to the next batch</p></li>
</ul>
</p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># if you have one val dataloader:</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="o">...</span>


<span class="c1"># if you have multiple val dataloaders:</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="o">...</span>
</pre></div>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 1: A single validation dataset</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="c1"># implement your own</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># log 6 example images</span>
    <span class="c1"># or generated text... or whatever</span>
    <span class="n">sample_imgs</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="mi">6</span><span class="p">]</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">make_grid</span><span class="p">(</span><span class="n">sample_imgs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_image</span><span class="p">(</span><span class="s1">&#39;example_images&#39;</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># calculate acc</span>
    <span class="n">labels_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">val_acc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">labels_hat</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span><span class="p">)</span>

    <span class="c1"># log the outputs!</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">({</span><span class="s1">&#39;val_loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;val_acc&#39;</span><span class="p">:</span> <span class="n">val_acc</span><span class="p">})</span>
</pre></div>
</div>
<p>If you pass in multiple val dataloaders, <code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code> will have an additional argument. We recommend
setting the default value of 0 so that you can quickly switch between single and multiple dataloaders.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 2: multiple validation dataloaders</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="c1"># dataloader_idx tells you which dataset this is.</span>
    <span class="o">...</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>If you don’t need to validate you don’t need to implement this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>When the <code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code> is called, the model has been put in eval mode
and PyTorch gradients have been disabled. At the end of validation,
the model goes back to training mode and gradients are enabled.</p>
</div>
</dd></dl>

</section>
</section>
<hr class="docutils" />
<section id="properties">
<h3>Properties<a class="headerlink" href="#properties" title="이 표제에 대한 퍼머링크">¶</a></h3>
<p>These are properties available in a LightningModule.</p>
<section id="current-epoch">
<h4>current_epoch<a class="headerlink" href="#current-epoch" title="이 표제에 대한 퍼머링크">¶</a></h4>
<p>The number of epochs run.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">current_epoch</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="o">...</span>
</pre></div>
</div>
</section>
<section id="device">
<h4>device<a class="headerlink" href="#device" title="이 표제에 대한 퍼머링크">¶</a></h4>
<p>The device the module is on. Use it to keep your code device agnostic.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="global-rank">
<h4>global_rank<a class="headerlink" href="#global-rank" title="이 표제에 대한 퍼머링크">¶</a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">global_rank</span></code> is the index of the current process across all nodes and devices.
Lightning will perform some operations such as logging, weight checkpointing only when <code class="docutils literal notranslate"><span class="pre">global_rank=0</span></code>. You
usually do not need to use this property, but it is useful to know how to access it if needed.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># do something only once across all the nodes</span>
        <span class="o">...</span>
</pre></div>
</div>
</section>
<section id="global-step">
<h4>global_step<a class="headerlink" href="#global-step" title="이 표제에 대한 퍼머링크">¶</a></h4>
<p>The number of optimizer steps taken (does not reset each epoch).
This includes multiple optimizers (if enabled).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">log_image</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">global_step</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="hparams">
<h4>hparams<a class="headerlink" href="#hparams" title="이 표제에 대한 퍼머링크">¶</a></h4>
<p>The arguments passed through <code class="docutils literal notranslate"><span class="pre">LightningModule.__init__()</span></code> and saved by calling
<code class="xref py py-meth docutils literal notranslate"><span class="pre">save_hyperparameters()</span></code> could be accessed by the <code class="docutils literal notranslate"><span class="pre">hparams</span></code> attribute.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="logger">
<h4>logger<a class="headerlink" href="#logger" title="이 표제에 대한 퍼머링크">¶</a></h4>
<p>The current logger being used (tensorboard or other supported logger)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># the generic logger (same no matter if tensorboard or other supported logger)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span>

    <span class="c1"># the particular logger</span>
    <span class="n">tensorboard_logger</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span>
</pre></div>
</div>
</section>
<section id="loggers">
<h4>loggers<a class="headerlink" href="#loggers" title="이 표제에 대한 퍼머링크">¶</a></h4>
<p>The list of loggers currently being used by the Trainer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># List of Logger objects</span>
    <span class="n">loggers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loggers</span>
    <span class="k">for</span> <span class="n">logger</span> <span class="ow">in</span> <span class="n">loggers</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">log_metrics</span><span class="p">({</span><span class="s2">&quot;foo&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">})</span>
</pre></div>
</div>
</section>
<section id="local-rank">
<h4>local_rank<a class="headerlink" href="#local-rank" title="이 표제에 대한 퍼머링크">¶</a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">local_rank</span></code> is the index of the current process across all the devices for the current node.
You usually do not need to use this property, but it is useful to know how to access it if needed.
For example, if using 10 machines (or nodes), the GPU at index 0 on each machine has local_rank = 0.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># do something only once across each node</span>
        <span class="o">...</span>
</pre></div>
</div>
</section>
<section id="precision">
<h4>precision<a class="headerlink" href="#precision" title="이 표제에 대한 퍼머링크">¶</a></h4>
<p>The type of precision used:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">precision</span> <span class="o">==</span> <span class="mi">16</span><span class="p">:</span>
        <span class="o">...</span>
</pre></div>
</div>
</section>
<section id="trainer">
<h4>trainer<a class="headerlink" href="#trainer" title="이 표제에 대한 퍼머링크">¶</a></h4>
<p>Pointer to the trainer</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">max_steps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">max_steps</span>
    <span class="n">any_flag</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">any_flag</span>
</pre></div>
</div>
</section>
<section id="prepare-data-per-node">
<h4>prepare_data_per_node<a class="headerlink" href="#prepare-data-per-node" title="이 표제에 대한 퍼머링크">¶</a></h4>
<p>If set to <code class="docutils literal notranslate"><span class="pre">True</span></code> will call <code class="docutils literal notranslate"><span class="pre">prepare_data()</span></code> on LOCAL_RANK=0 for every node.
If set to <code class="docutils literal notranslate"><span class="pre">False</span></code> will only call from NODE_RANK=0, LOCAL_RANK=0.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LitModel</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prepare_data_per_node</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
</section>
<section id="automatic-optimization">
<h4>automatic_optimization<a class="headerlink" href="#automatic-optimization" title="이 표제에 대한 퍼머링크">¶</a></h4>
<p>When set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, Lightning does not automate the optimization process. This means you are responsible for handling
your optimizers. However, we do take care of precision and any accelerators used.</p>
<p>See <a class="reference internal" href="optimization.html#id2"><span class="std std-ref">manual optimization</span></a> for details.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">automatic_optimization</span> <span class="o">=</span> <span class="kc">False</span>


<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizers</span><span class="p">(</span><span class="n">use_pl_optimizer</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="o">...</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">manual_backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<p>Manual optimization is most useful for research topics like reinforcement learning, sparse coding, and GAN research.
It is required when you are using 2+ optimizers because with automatic optimization, you can only use one optimizer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">automatic_optimization</span> <span class="o">=</span> <span class="kc">False</span>


<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># access your optimizers with use_pl_optimizer=False. Default is True</span>
    <span class="n">opt_a</span><span class="p">,</span> <span class="n">opt_b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizers</span><span class="p">(</span><span class="n">use_pl_optimizer</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">gen_loss</span> <span class="o">=</span> <span class="o">...</span>
    <span class="n">opt_a</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">manual_backward</span><span class="p">(</span><span class="n">gen_loss</span><span class="p">)</span>
    <span class="n">opt_a</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="n">disc_loss</span> <span class="o">=</span> <span class="o">...</span>
    <span class="n">opt_b</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">manual_backward</span><span class="p">(</span><span class="n">disc_loss</span><span class="p">)</span>
    <span class="n">opt_b</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="example-input-array">
<h4>example_input_array<a class="headerlink" href="#example-input-array" title="이 표제에 대한 퍼머링크">¶</a></h4>
<p>Set and access example_input_array, which basically represents a single batch.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">example_input_array</span> <span class="o">=</span> <span class="o">...</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">generator</span> <span class="o">=</span> <span class="o">...</span>


<span class="k">def</span> <span class="nf">on_train_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># generate some images using the example_input_array</span>
    <span class="n">gen_images</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generator</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">example_input_array</span><span class="p">)</span>
</pre></div>
</div>
<hr class="docutils" />
</section>
</section>
<section id="hooks">
<span id="lightning-hooks"></span><h3>Hooks<a class="headerlink" href="#hooks" title="이 표제에 대한 퍼머링크">¶</a></h3>
<p>This is the pseudocode to describe the structure of <code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code>.
The inputs and outputs of each function are not represented for simplicity. Please check each function’s API reference
for more information.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">global_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># prepare data is called on GLOBAL_ZERO only</span>
        <span class="n">prepare_data</span><span class="p">()</span>

    <span class="n">configure_callbacks</span><span class="p">()</span>

    <span class="k">with</span> <span class="n">parallel</span><span class="p">(</span><span class="n">devices</span><span class="p">):</span>
        <span class="c1"># devices can be GPUs, TPUs, ...</span>
        <span class="n">train_on_device</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">train_on_device</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="c1"># called PER DEVICE</span>
    <span class="n">setup</span><span class="p">(</span><span class="s2">&quot;fit&quot;</span><span class="p">)</span>
    <span class="n">configure_optimizers</span><span class="p">()</span>
    <span class="n">on_fit_start</span><span class="p">()</span>

    <span class="c1"># the sanity check runs here</span>

    <span class="n">on_train_start</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">epochs</span><span class="p">:</span>
        <span class="n">fit_loop</span><span class="p">()</span>
    <span class="n">on_train_end</span><span class="p">()</span>

    <span class="n">on_fit_end</span><span class="p">()</span>
    <span class="n">teardown</span><span class="p">(</span><span class="s2">&quot;fit&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">fit_loop</span><span class="p">():</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">on_train_epoch_start</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_dataloader</span><span class="p">():</span>
        <span class="n">on_train_batch_start</span><span class="p">()</span>

        <span class="n">on_before_batch_transfer</span><span class="p">()</span>
        <span class="n">transfer_batch_to_device</span><span class="p">()</span>
        <span class="n">on_after_batch_transfer</span><span class="p">()</span>

        <span class="n">out</span> <span class="o">=</span> <span class="n">training_step</span><span class="p">()</span>

        <span class="n">on_before_zero_grad</span><span class="p">()</span>
        <span class="n">optimizer_zero_grad</span><span class="p">()</span>

        <span class="n">on_before_backward</span><span class="p">()</span>
        <span class="n">backward</span><span class="p">()</span>
        <span class="n">on_after_backward</span><span class="p">()</span>

        <span class="n">on_before_optimizer_step</span><span class="p">()</span>
        <span class="n">configure_gradient_clipping</span><span class="p">()</span>
        <span class="n">optimizer_step</span><span class="p">()</span>

        <span class="n">on_train_batch_end</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">should_check_val</span><span class="p">:</span>
            <span class="n">val_loop</span><span class="p">()</span>

    <span class="n">on_train_epoch_end</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">val_loop</span><span class="p">():</span>
    <span class="n">on_validation_model_eval</span><span class="p">()</span>  <span class="c1"># calls `model.eval()`</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">on_validation_start</span><span class="p">()</span>
    <span class="n">on_validation_epoch_start</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">val_dataloader</span><span class="p">()):</span>
        <span class="n">on_validation_batch_start</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>

        <span class="n">batch</span> <span class="o">=</span> <span class="n">on_before_batch_transfer</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">transfer_batch_to_device</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">on_after_batch_transfer</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="n">validation_step</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>

        <span class="n">on_validation_batch_end</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>

    <span class="n">on_validation_epoch_end</span><span class="p">()</span>
    <span class="n">on_validation_end</span><span class="p">()</span>

    <span class="c1"># set up for train</span>
    <span class="n">on_validation_model_train</span><span class="p">()</span>  <span class="c1"># calls `model.train()`</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<section id="backward">
<h4>backward<a class="headerlink" href="#backward" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lightning/pytorch/core/module.html#LightningModule.backward"><span class="viewcode-link"><span class="pre">[소스]</span></span></a></dt>
<dd><p>Called to perform backward on the loss returned in <code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code>. Override this hook with your
own implementation if you need to.</p>
<dl class="field-list simple">
<dt class="field-odd">매개변수</dt>
<dd class="field-odd"><p><span class="target" id="lightning.pytorch.core.module.LightningModule.backward.params.loss"></span><strong>loss</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.backward.params.loss">¶</a> (<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>) – The loss tensor returned by <code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code>. If gradient accumulation is used, the loss here
holds the normalized value (scaled by 1 / accumulation steps).</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">):</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-before-backward">
<h4>on_before_backward<a class="headerlink" href="#on-before-backward" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_before_backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Called before <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">매개변수</dt>
<dd class="field-odd"><p><span class="target" id="lightning.pytorch.core.module.LightningModule.on_before_backward.params.loss"></span><strong>loss</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.on_before_backward.params.loss">¶</a> (<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>) – Loss divided by number of batches for gradient accumulation and scaled if using AMP.</p>
</dd>
<dt class="field-even">반환 형식</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-after-backward">
<h4>on_after_backward<a class="headerlink" href="#on-after-backward" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_after_backward</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Called after <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code> and before optimizers are stepped.</p>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>If using native AMP, the gradients will not be unscaled at this point.
Use the <code class="docutils literal notranslate"><span class="pre">on_before_optimizer_step</span></code> if you need the unscaled gradients.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-before-zero-grad">
<h4>on_before_zero_grad<a class="headerlink" href="#on-before-zero-grad" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_before_zero_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Called after <code class="docutils literal notranslate"><span class="pre">training_step()</span></code> and before <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>.</p>
<p>Called in the training loop after taking an optimizer step and before zeroing grads.
Good place to inspect weight information with weights updated.</p>
<p>This is where it is called:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">optimizer</span> <span class="ow">in</span> <span class="n">optimizers</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">training_step</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

    <span class="n">model</span><span class="o">.</span><span class="n">on_before_zero_grad</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span> <span class="c1"># &lt; ---- called here</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">매개변수</dt>
<dd class="field-odd"><p><span class="target" id="lightning.pytorch.core.module.LightningModule.on_before_zero_grad.params.optimizer"></span><strong>optimizer</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.on_before_zero_grad.params.optimizer">¶</a> (<a class="reference external" href="https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code></a>) – The optimizer for which grads should be zeroed.</p>
</dd>
<dt class="field-even">반환 형식</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-fit-start">
<h4>on_fit_start<a class="headerlink" href="#on-fit-start" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_fit_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Called at the very beginning of fit.</p>
<p>If on DDP it is called on every process</p>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-fit-end">
<h4>on_fit_end<a class="headerlink" href="#on-fit-end" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_fit_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Called at the very end of fit.</p>
<p>If on DDP it is called on every process</p>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-load-checkpoint">
<h4>on_load_checkpoint<a class="headerlink" href="#on-load-checkpoint" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_load_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Called by Lightning to restore your model. If you saved something with <code class="xref py py-meth docutils literal notranslate"><span class="pre">on_save_checkpoint()</span></code> this
is your chance to restore this.</p>
<dl class="field-list simple">
<dt class="field-odd">매개변수</dt>
<dd class="field-odd"><p><span class="target" id="lightning.pytorch.core.module.LightningModule.on_load_checkpoint.params.checkpoint"></span><strong>checkpoint</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.on_load_checkpoint.params.checkpoint">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></a>]) – Loaded checkpoint</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_load_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">):</span>
    <span class="c1"># 99% of the time you don&#39;t need to implement this method</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">something_cool_i_want_to_save</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;something_cool_i_want_to_save&#39;</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>Lightning auto-restores global step, epoch, and train state including amp scaling.
There is no need for you to restore anything regarding training.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-save-checkpoint">
<h4>on_save_checkpoint<a class="headerlink" href="#on-save-checkpoint" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_save_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want
to save.</p>
<dl class="field-list simple">
<dt class="field-odd">매개변수</dt>
<dd class="field-odd"><p><span class="target" id="lightning.pytorch.core.module.LightningModule.on_save_checkpoint.params.checkpoint"></span><strong>checkpoint</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.on_save_checkpoint.params.checkpoint">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></a>]) – The full checkpoint dictionary before it gets dumped to a file.
Implementations of this hook can insert additional data into this dictionary.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">):</span>
    <span class="c1"># 99% of use cases you don&#39;t need to implement this method</span>
    <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;something_cool_i_want_to_save&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">my_cool_pickable_object</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>Lightning saves all aspects of training (epoch, global step, etc…)
including amp scaling.
There is no need for you to store anything about training.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id3">
<h4>load_from_checkpoint<a class="headerlink" href="#id3" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">load_from_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">map_location</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hparams_file</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lightning/pytorch/core/module.html#LightningModule.load_from_checkpoint"><span class="viewcode-link"><span class="pre">[소스]</span></span></a></dt>
<dd><p>Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint
it stores the arguments passed to <code class="docutils literal notranslate"><span class="pre">__init__</span></code>  in the checkpoint under <code class="docutils literal notranslate"><span class="pre">&quot;hyper_parameters&quot;</span></code>.</p>
<p>Any arguments specified through **kwargs will override args stored in <code class="docutils literal notranslate"><span class="pre">&quot;hyper_parameters&quot;</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">매개변수</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.load_from_checkpoint.params.checkpoint_path"></span><strong>checkpoint_path</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.load_from_checkpoint.params.checkpoint_path">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/pathlib.html#pathlib.Path" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Path</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.IO" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">IO</span></code></a>]) – Path to checkpoint. This can also be a URL, or file-like object</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.load_from_checkpoint.params.map_location"></span><strong>map_location</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.load_from_checkpoint.params.map_location">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code></a>[<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">device</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code></a>[[<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code></a>[<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">device</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a>]], <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code></a>[<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">device</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a>]], <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code></a>[<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">device</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a>], <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code></a>[<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">device</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a>]], <a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a>]) – If your checkpoint saved a GPU model and you now load on CPUs
or a different number of GPUs, use this to map to the new setup.
The behaviour is the same as in <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.load.html#torch.load" title="(PyTorch v2.0에서)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.load()</span></code></a>.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.load_from_checkpoint.params.hparams_file"></span><strong>hparams_file</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.load_from_checkpoint.params.hparams_file">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/pathlib.html#pathlib.Path" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Path</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a>]) – <p>Optional path to a <code class="docutils literal notranslate"><span class="pre">.yaml</span></code> or <code class="docutils literal notranslate"><span class="pre">.csv</span></code> file with hierarchical structure
as in this example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">drop_prob</span><span class="p">:</span> <span class="mf">0.2</span>
<span class="n">dataloader</span><span class="p">:</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="mi">32</span>
</pre></div>
</div>
<p>You most likely won’t need this since Lightning will always save the hyperparameters
to the checkpoint.
However, if your checkpoint weights don’t have the hyperparameters saved,
use this method to pass in a <code class="docutils literal notranslate"><span class="pre">.yaml</span></code> file with the hparams you’d like to use.
These will be converted into a <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></a> and passed into your
<a class="reference internal" href="../api/lightning.pytorch.core.LightningModule.html#lightning.pytorch.core.LightningModule" title="lightning.pytorch.core.module.LightningModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code></a> for use.</p>
<p>If your model’s <code class="docutils literal notranslate"><span class="pre">hparams</span></code> argument is <a class="reference external" href="https://docs.python.org/3/library/argparse.html#argparse.Namespace" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Namespace</span></code></a>
and <code class="docutils literal notranslate"><span class="pre">.yaml</span></code> file has hierarchical structure, you need to refactor your model to treat
<code class="docutils literal notranslate"><span class="pre">hparams</span></code> as <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></a>.</p>
</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.load_from_checkpoint.params.strict"></span><strong>strict</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.load_from_checkpoint.params.strict">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></a>) – Whether to strictly enforce that the keys in <code class="xref py py-attr docutils literal notranslate"><span class="pre">checkpoint_path</span></code> match the keys
returned by this module’s state dict.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.load_from_checkpoint.params.**kwargs"></span><strong>**kwargs</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.load_from_checkpoint.params.**kwargs">¶</a> – Any extra keyword args needed to init the model. Can also be used to override saved
hyperparameter values.</p></li>
</ul>
</dd>
<dt class="field-even">반환 형식</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Self</span></code></p>
</dd>
<dt class="field-odd">반환</dt>
<dd class="field-odd"><p><a class="reference internal" href="../api/lightning.pytorch.core.LightningModule.html#lightning.pytorch.core.LightningModule" title="lightning.pytorch.core.module.LightningModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code></a> instance with loaded weights and hyperparameters (if available).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p><code class="docutils literal notranslate"><span class="pre">load_from_checkpoint</span></code> is a <strong>class</strong> method. You should use your <a class="reference internal" href="../api/lightning.pytorch.core.LightningModule.html#lightning.pytorch.core.LightningModule" title="lightning.pytorch.core.module.LightningModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code></a>
<strong>class</strong> to call it instead of the <a class="reference internal" href="../api/lightning.pytorch.core.LightningModule.html#lightning.pytorch.core.LightningModule" title="lightning.pytorch.core.module.LightningModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code></a> instance.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># load weights without mapping ...</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span><span class="s1">&#39;path/to/checkpoint.ckpt&#39;</span><span class="p">)</span>

<span class="c1"># or load weights mapping all weights from GPU 1 to GPU 0 ...</span>
<span class="n">map_location</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">:</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">}</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span>
    <span class="s1">&#39;path/to/checkpoint.ckpt&#39;</span><span class="p">,</span>
    <span class="n">map_location</span><span class="o">=</span><span class="n">map_location</span>
<span class="p">)</span>

<span class="c1"># or load weights and hyperparameters from separate files.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span>
    <span class="s1">&#39;path/to/checkpoint.ckpt&#39;</span><span class="p">,</span>
    <span class="n">hparams_file</span><span class="o">=</span><span class="s1">&#39;/path/to/hparams_file.yaml&#39;</span>
<span class="p">)</span>

<span class="c1"># override some of the params with new values</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span>
    <span class="n">PATH</span><span class="p">,</span>
    <span class="n">num_layers</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">pretrained_ckpt_path</span><span class="o">=</span><span class="n">NEW_PATH</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># predict</span>
<span class="n">pretrained_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">pretrained_model</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">pretrained_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="on-train-start">
<h4>on_train_start<a class="headerlink" href="#on-train-start" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_train_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Called at the beginning of training after sanity check.</p>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-train-end">
<h4>on_train_end<a class="headerlink" href="#on-train-end" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_train_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Called at the end of training before logger experiment is closed.</p>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-validation-start">
<h4>on_validation_start<a class="headerlink" href="#on-validation-start" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_validation_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Called at the beginning of validation.</p>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-validation-end">
<h4>on_validation_end<a class="headerlink" href="#on-validation-end" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_validation_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Called at the end of validation.</p>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-test-batch-start">
<h4>on_test_batch_start<a class="headerlink" href="#on-test-batch-start" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_test_batch_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Called in the test loop before anything happens for that batch.</p>
<dl class="field-list simple">
<dt class="field-odd">매개변수</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.on_test_batch_start.params.batch"></span><strong>batch</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.on_test_batch_start.params.batch">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></a>) – The batched data as it is returned by the test DataLoader.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.on_test_batch_start.params.batch_idx"></span><strong>batch_idx</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.on_test_batch_start.params.batch_idx">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a>) – the index of the batch</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.on_test_batch_start.params.dataloader_idx"></span><strong>dataloader_idx</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.on_test_batch_start.params.dataloader_idx">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">반환 형식</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-test-batch-end">
<h4>on_test_batch_end<a class="headerlink" href="#on-test-batch-end" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_test_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Called in the test loop after the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">매개변수</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.on_test_batch_end.params.outputs"></span><strong>outputs</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.on_test_batch_end.params.outputs">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code></a>[<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></a>], <a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a>]) – The outputs of test_step(x)</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.on_test_batch_end.params.batch"></span><strong>batch</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.on_test_batch_end.params.batch">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></a>) – The batched data as it is returned by the test DataLoader.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.on_test_batch_end.params.batch_idx"></span><strong>batch_idx</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.on_test_batch_end.params.batch_idx">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a>) – the index of the batch</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.on_test_batch_end.params.dataloader_idx"></span><strong>dataloader_idx</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.on_test_batch_end.params.dataloader_idx">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">반환 형식</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-test-epoch-start">
<h4>on_test_epoch_start<a class="headerlink" href="#on-test-epoch-start" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_test_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Called in the test loop at the very beginning of the epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-test-epoch-end">
<h4>on_test_epoch_end<a class="headerlink" href="#on-test-epoch-end" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_test_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Called in the test loop at the very end of the epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-test-start">
<h4>on_test_start<a class="headerlink" href="#on-test-start" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_test_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Called at the beginning of testing.</p>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-test-end">
<h4>on_test_end<a class="headerlink" href="#on-test-end" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_test_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Called at the end of testing.</p>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-predict-batch-start">
<h4>on_predict_batch_start<a class="headerlink" href="#on-predict-batch-start" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_predict_batch_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Called in the predict loop before anything happens for that batch.</p>
<dl class="field-list simple">
<dt class="field-odd">매개변수</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.on_predict_batch_start.params.batch"></span><strong>batch</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.on_predict_batch_start.params.batch">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></a>) – The batched data as it is returned by the test DataLoader.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.on_predict_batch_start.params.batch_idx"></span><strong>batch_idx</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.on_predict_batch_start.params.batch_idx">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a>) – the index of the batch</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.on_predict_batch_start.params.dataloader_idx"></span><strong>dataloader_idx</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.on_predict_batch_start.params.dataloader_idx">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">반환 형식</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-predict-batch-end">
<h4>on_predict_batch_end<a class="headerlink" href="#on-predict-batch-end" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_predict_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Called in the predict loop after the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">매개변수</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.on_predict_batch_end.params.outputs"></span><strong>outputs</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.on_predict_batch_end.params.outputs">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></a>]) – The outputs of predict_step(x)</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.on_predict_batch_end.params.batch"></span><strong>batch</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.on_predict_batch_end.params.batch">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></a>) – The batched data as it is returned by the prediction DataLoader.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.on_predict_batch_end.params.batch_idx"></span><strong>batch_idx</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.on_predict_batch_end.params.batch_idx">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a>) – the index of the batch</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.on_predict_batch_end.params.dataloader_idx"></span><strong>dataloader_idx</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.on_predict_batch_end.params.dataloader_idx">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">반환 형식</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-predict-epoch-start">
<h4>on_predict_epoch_start<a class="headerlink" href="#on-predict-epoch-start" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_predict_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Called at the beginning of predicting.</p>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-predict-epoch-end">
<h4>on_predict_epoch_end<a class="headerlink" href="#on-predict-epoch-end" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_predict_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Called at the end of predicting.</p>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-predict-start">
<h4>on_predict_start<a class="headerlink" href="#on-predict-start" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_predict_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Called at the beginning of predicting.</p>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-predict-end">
<h4>on_predict_end<a class="headerlink" href="#on-predict-end" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_predict_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Called at the end of predicting.</p>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-train-batch-start">
<h4>on_train_batch_start<a class="headerlink" href="#on-train-batch-start" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_train_batch_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Called in the training loop before anything happens for that batch.</p>
<p>If you return -1 here, you will skip training for the rest of the current epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">매개변수</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.on_train_batch_start.params.batch"></span><strong>batch</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.on_train_batch_start.params.batch">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></a>) – The batched data as it is returned by the training DataLoader.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.on_train_batch_start.params.batch_idx"></span><strong>batch_idx</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.on_train_batch_start.params.batch_idx">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a>) – the index of the batch</p></li>
</ul>
</dd>
<dt class="field-even">반환 형식</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a>]</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-train-batch-end">
<h4>on_train_batch_end<a class="headerlink" href="#on-train-batch-end" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_train_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Called in the training loop after the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">매개변수</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.on_train_batch_end.params.outputs"></span><strong>outputs</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.on_train_batch_end.params.outputs">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code></a>[<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></a>]]) – The outputs of training_step(x)</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.on_train_batch_end.params.batch"></span><strong>batch</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.on_train_batch_end.params.batch">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></a>) – The batched data as it is returned by the training DataLoader.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.on_train_batch_end.params.batch_idx"></span><strong>batch_idx</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.on_train_batch_end.params.batch_idx">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a>) – the index of the batch</p></li>
</ul>
</dd>
<dt class="field-even">반환 형식</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-train-epoch-start">
<h4>on_train_epoch_start<a class="headerlink" href="#on-train-epoch-start" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_train_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Called in the training loop at the very beginning of the epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-train-epoch-end">
<h4>on_train_epoch_end<a class="headerlink" href="#on-train-epoch-end" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_train_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Called in the training loop at the very end of the epoch.</p>
<p>To access all batch outputs at the end of the epoch, you can cache step outputs as an attribute of the
<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code> and access them in this hook:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyLightningModule</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training_step_outputs</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="o">...</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training_step_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">on_train_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># do something with all training_step outputs, for example:</span>
        <span class="n">epoch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_step_outputs</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;training_epoch_mean&quot;</span><span class="p">,</span> <span class="n">epoch_mean</span><span class="p">)</span>
        <span class="c1"># free up the memory</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training_step_outputs</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-validation-batch-start">
<h4>on_validation_batch_start<a class="headerlink" href="#on-validation-batch-start" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_validation_batch_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Called in the validation loop before anything happens for that batch.</p>
<dl class="field-list simple">
<dt class="field-odd">매개변수</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.on_validation_batch_start.params.batch"></span><strong>batch</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.on_validation_batch_start.params.batch">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></a>) – The batched data as it is returned by the validation DataLoader.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.on_validation_batch_start.params.batch_idx"></span><strong>batch_idx</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.on_validation_batch_start.params.batch_idx">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a>) – the index of the batch</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.on_validation_batch_start.params.dataloader_idx"></span><strong>dataloader_idx</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.on_validation_batch_start.params.dataloader_idx">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">반환 형식</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-validation-batch-end">
<h4>on_validation_batch_end<a class="headerlink" href="#on-validation-batch-end" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_validation_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Called in the validation loop after the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">매개변수</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.on_validation_batch_end.params.outputs"></span><strong>outputs</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.on_validation_batch_end.params.outputs">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code></a>[<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></a>], <a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a>]) – The outputs of validation_step(x)</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.on_validation_batch_end.params.batch"></span><strong>batch</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.on_validation_batch_end.params.batch">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></a>) – The batched data as it is returned by the validation DataLoader.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.on_validation_batch_end.params.batch_idx"></span><strong>batch_idx</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.on_validation_batch_end.params.batch_idx">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a>) – the index of the batch</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.on_validation_batch_end.params.dataloader_idx"></span><strong>dataloader_idx</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.on_validation_batch_end.params.dataloader_idx">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">반환 형식</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-validation-epoch-start">
<h4>on_validation_epoch_start<a class="headerlink" href="#on-validation-epoch-start" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_validation_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Called in the validation loop at the very beginning of the epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-validation-epoch-end">
<h4>on_validation_epoch_end<a class="headerlink" href="#on-validation-epoch-end" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_validation_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Called in the validation loop at the very end of the epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="configure-model">
<h4>configure_model<a class="headerlink" href="#configure-model" title="이 표제에 대한 퍼머링크">¶</a></h4>
</section>
<section id="on-validation-model-eval">
<h4>on_validation_model_eval<a class="headerlink" href="#on-validation-model-eval" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_validation_model_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Sets the model to eval during the val loop.</p>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-validation-model-train">
<h4>on_validation_model_train<a class="headerlink" href="#on-validation-model-train" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_validation_model_train</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Sets the model to train during the val loop.</p>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-test-model-eval">
<h4>on_test_model_eval<a class="headerlink" href="#on-test-model-eval" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_test_model_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Sets the model to eval during the test loop.</p>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-test-model-train">
<h4>on_test_model_train<a class="headerlink" href="#on-test-model-train" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_test_model_train</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Sets the model to train during the test loop.</p>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-before-optimizer-step">
<h4>on_before_optimizer_step<a class="headerlink" href="#on-before-optimizer-step" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_before_optimizer_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Called before <code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code>.</p>
<p>If using gradient accumulation, the hook is called once the gradients have been accumulated.
See: <code class="xref py py-paramref docutils literal notranslate"><span class="pre">accumulate_grad_batches</span></code>.</p>
<p>If using AMP, the loss will be unscaled before calling this hook.
See these <a class="reference external" href="https://pytorch.org/docs/stable/notes/amp_examples.html#working-with-unscaled-gradients">docs</a>
for more information on the scaling of gradients.</p>
<p>If clipping gradients, the gradients will not have been clipped yet.</p>
<dl class="field-list simple">
<dt class="field-odd">매개변수</dt>
<dd class="field-odd"><p><span class="target" id="lightning.pytorch.core.module.LightningModule.on_before_optimizer_step.params.optimizer"></span><strong>optimizer</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.on_before_optimizer_step.params.optimizer">¶</a> (<a class="reference external" href="https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code></a>) – Current optimizer being used.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_before_optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
    <span class="c1"># example to inspect gradient information in tensorboard</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span> <span class="o">%</span> <span class="mi">25</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># don&#39;t make the tf file huge</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_histogram</span><span class="p">(</span>
                <span class="n">tag</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">v</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">global_step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span>
            <span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="configure-gradient-clipping">
<h4>configure_gradient_clipping<a class="headerlink" href="#configure-gradient-clipping" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">configure_gradient_clipping</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_clip_val</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_clip_algorithm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lightning/pytorch/core/module.html#LightningModule.configure_gradient_clipping"><span class="viewcode-link"><span class="pre">[소스]</span></span></a></dt>
<dd><p>Perform gradient clipping for the optimizer parameters. Called before <code class="xref py py-meth docutils literal notranslate"><span class="pre">optimizer_step()</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">매개변수</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.configure_gradient_clipping.params.optimizer"></span><strong>optimizer</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.configure_gradient_clipping.params.optimizer">¶</a> (<a class="reference external" href="https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code></a>) – Current optimizer being used.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.configure_gradient_clipping.params.gradient_clip_val"></span><strong>gradient_clip_val</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.configure_gradient_clipping.params.gradient_clip_val">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a>]) – The value at which to clip gradients. By default, value passed in Trainer
will be available here.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.configure_gradient_clipping.params.gradient_clip_algorithm"></span><strong>gradient_clip_algorithm</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.configure_gradient_clipping.params.gradient_clip_algorithm">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a>]) – The gradient clipping algorithm to use. By default, value
passed in Trainer will be available here.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">configure_gradient_clipping</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">gradient_clip_val</span><span class="p">,</span> <span class="n">gradient_clip_algorithm</span><span class="p">):</span>
    <span class="c1"># Implement your own custom logic to clip gradients</span>
    <span class="c1"># You can call `self.clip_gradients` with your settings:</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">clip_gradients</span><span class="p">(</span>
        <span class="n">optimizer</span><span class="p">,</span>
        <span class="n">gradient_clip_val</span><span class="o">=</span><span class="n">gradient_clip_val</span><span class="p">,</span>
        <span class="n">gradient_clip_algorithm</span><span class="o">=</span><span class="n">gradient_clip_algorithm</span>
    <span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="optimizer-step">
<h4>optimizer_step<a class="headerlink" href="#optimizer-step" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">optimizer_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_closure</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lightning/pytorch/core/module.html#LightningModule.optimizer_step"><span class="viewcode-link"><span class="pre">[소스]</span></span></a></dt>
<dd><p>Override this method to adjust the default way the <a class="reference internal" href="../api/lightning.pytorch.trainer.trainer.Trainer.html#lightning.pytorch.trainer.trainer.Trainer" title="lightning.pytorch.trainer.trainer.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a>
calls the optimizer.</p>
<p>By default, Lightning calls <code class="docutils literal notranslate"><span class="pre">step()</span></code> and <code class="docutils literal notranslate"><span class="pre">zero_grad()</span></code> as shown in the example.
This method (and <code class="docutils literal notranslate"><span class="pre">zero_grad()</span></code>) won’t be called during the accumulation phase when
<code class="docutils literal notranslate"><span class="pre">Trainer(accumulate_grad_batches</span> <span class="pre">!=</span> <span class="pre">1)</span></code>. Overriding this hook has no benefit with manual optimization.</p>
<dl class="field-list simple">
<dt class="field-odd">매개변수</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.optimizer_step.params.epoch"></span><strong>epoch</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.optimizer_step.params.epoch">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a>) – Current epoch</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.optimizer_step.params.batch_idx"></span><strong>batch_idx</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.optimizer_step.params.batch_idx">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a>) – Index of current batch</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.optimizer_step.params.optimizer"></span><strong>optimizer</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.optimizer_step.params.optimizer">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code></a>[<a class="reference external" href="https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code></a>, <a class="reference internal" href="../api/lightning.pytorch.core.optimizer.LightningOptimizer.html#lightning.pytorch.core.optimizer.LightningOptimizer" title="lightning.pytorch.core.optimizer.LightningOptimizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code></a>]) – A PyTorch optimizer</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.optimizer_step.params.optimizer_closure"></span><strong>optimizer_closure</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.optimizer_step.params.optimizer_closure">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code></a>[[], <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></a>]]) – The optimizer closure. This closure must be executed as it includes the
calls to <code class="docutils literal notranslate"><span class="pre">training_step()</span></code>, <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>, and <code class="docutils literal notranslate"><span class="pre">backward()</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT</span>
<span class="k">def</span> <span class="nf">optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_closure</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="o">=</span><span class="n">optimizer_closure</span><span class="p">)</span>

<span class="c1"># Learning rate warm-up</span>
<span class="k">def</span> <span class="nf">optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_closure</span><span class="p">):</span>
    <span class="c1"># update params</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="o">=</span><span class="n">optimizer_closure</span><span class="p">)</span>

    <span class="c1"># manually warm up lr without a scheduler</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span> <span class="o">&lt;</span> <span class="mi">500</span><span class="p">:</span>
        <span class="n">lr_scale</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mf">500.0</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">pg</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="n">pg</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr_scale</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="optimizer-zero-grad">
<h4>optimizer_zero_grad<a class="headerlink" href="#optimizer-zero-grad" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">optimizer_zero_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lightning/pytorch/core/module.html#LightningModule.optimizer_zero_grad"><span class="viewcode-link"><span class="pre">[소스]</span></span></a></dt>
<dd><p>Override this method to change the default behaviour of <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">매개변수</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.optimizer_zero_grad.params.epoch"></span><strong>epoch</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.optimizer_zero_grad.params.epoch">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a>) – Current epoch</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.optimizer_zero_grad.params.batch_idx"></span><strong>batch_idx</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.optimizer_zero_grad.params.batch_idx">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a>) – Index of current batch</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.optimizer_zero_grad.params.optimizer"></span><strong>optimizer</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.optimizer_zero_grad.params.optimizer">¶</a> (<a class="reference external" href="https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code></a>) – A PyTorch optimizer</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT</span>
<span class="k">def</span> <span class="nf">optimizer_zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

<span class="c1"># Set gradients to `None` instead of zero to improve performance (not required on `torch&gt;=2.0.0`).</span>
<span class="k">def</span> <span class="nf">optimizer_zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html#torch.optim.Optimizer.zero_grad" title="(PyTorch v2.0에서)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.optim.Optimizer.zero_grad()</span></code></a> for the explanation of the above example.</p>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="prepare-data">
<h4>prepare_data<a class="headerlink" href="#prepare-data" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">prepare_data</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Use this to download and prepare data. Downloading and saving data with multiple processes (distributed
settings) will result in corrupted data. Lightning ensures this method is called only within a single
process, so you can safely add your downloading logic within.</p>
<div class="admonition warning">
<p class="admonition-title">경고</p>
<p>DO NOT set state to the model (use <code class="docutils literal notranslate"><span class="pre">setup</span></code> instead)
since this is NOT called on every device</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">prepare_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># good</span>
    <span class="n">download_data</span><span class="p">()</span>
    <span class="n">tokenize</span><span class="p">()</span>
    <span class="n">etc</span><span class="p">()</span>

    <span class="c1"># bad</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">split</span> <span class="o">=</span> <span class="n">data_split</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">some_state</span> <span class="o">=</span> <span class="n">some_other_state</span><span class="p">()</span>
</pre></div>
</div>
<p>In a distributed environment, <code class="docutils literal notranslate"><span class="pre">prepare_data</span></code> can be called in two ways
(using <a class="reference internal" href="#prepare-data-per-node"><span class="std std-ref">prepare_data_per_node</span></a>)</p>
<ol class="arabic simple">
<li><p>Once per node. This is the default and is only called on LOCAL_RANK=0.</p></li>
<li><p>Once in total. Only called on GLOBAL_RANK=0.</p></li>
</ol>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT</span>
<span class="c1"># called once per node on LOCAL_RANK=0 of that node</span>
<span class="k">class</span> <span class="nc">LitDataModule</span><span class="p">(</span><span class="n">LightningDataModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prepare_data_per_node</span> <span class="o">=</span> <span class="kc">True</span>


<span class="c1"># call on GLOBAL_RANK=0 (great for shared file systems)</span>
<span class="k">class</span> <span class="nc">LitDataModule</span><span class="p">(</span><span class="n">LightningDataModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prepare_data_per_node</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
<p>This is called before requesting the dataloaders:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">prepare_data</span><span class="p">()</span>
<span class="n">initialize_distributed</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">stage</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train_dataloader</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">val_dataloader</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">test_dataloader</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">predict_dataloader</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="setup">
<h4>setup<a class="headerlink" href="#setup" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">setup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stage</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Called at the beginning of fit (train + validate), validate, test, or predict. This is a good hook when
you need to build models dynamically or adjust something about them. This hook is called on every process
when using DDP.</p>
<dl class="field-list simple">
<dt class="field-odd">매개변수</dt>
<dd class="field-odd"><p><span class="target" id="lightning.pytorch.core.module.LightningModule.setup.params.stage"></span><strong>stage</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.setup.params.stage">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a>) – either <code class="docutils literal notranslate"><span class="pre">'fit'</span></code>, <code class="docutils literal notranslate"><span class="pre">'validate'</span></code>, <code class="docutils literal notranslate"><span class="pre">'test'</span></code>, or <code class="docutils literal notranslate"><span class="pre">'predict'</span></code></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LitModel</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">prepare_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">download_data</span><span class="p">()</span>
        <span class="n">tokenize</span><span class="p">()</span>

        <span class="c1"># don&#39;t do this</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">something</span> <span class="o">=</span> <span class="k">else</span>

    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stage</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">num_classes</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="teardown">
<h4>teardown<a class="headerlink" href="#teardown" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">teardown</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stage</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Called at the end of fit (train + validate), validate, test, or predict.</p>
<dl class="field-list simple">
<dt class="field-odd">매개변수</dt>
<dd class="field-odd"><p><span class="target" id="lightning.pytorch.core.module.LightningModule.teardown.params.stage"></span><strong>stage</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.teardown.params.stage">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a>) – either <code class="docutils literal notranslate"><span class="pre">'fit'</span></code>, <code class="docutils literal notranslate"><span class="pre">'validate'</span></code>, <code class="docutils literal notranslate"><span class="pre">'test'</span></code>, or <code class="docutils literal notranslate"><span class="pre">'predict'</span></code></p>
</dd>
<dt class="field-even">반환 형식</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(Python v3.11에서)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="train-dataloader">
<h4>train_dataloader<a class="headerlink" href="#train-dataloader" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">train_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>An iterable or collection of iterables specifying training samples.</p>
<p>For more information about multiple dataloaders, see this <span class="xref std std-ref">section</span>.</p>
<p>The dataloader you return will not be reloaded unless you set
<code class="xref py py-paramref docutils literal notranslate"><span class="pre">reload_dataloaders_every_n_epochs</span></code> to
a positive integer.</p>
<p>For data processing use the following pattern:</p>
<blockquote>
<div><ul class="simple">
<li><p>download in <code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></p></li>
<li><p>process and split in <code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></p></li>
</ul>
</div></blockquote>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">경고</p>
<p>do not assign state in prepare_data</p>
</div>
<ul class="simple">
<li><p><a class="reference internal" href="../api/lightning.pytorch.trainer.trainer.Trainer.html#lightning.pytorch.trainer.trainer.Trainer.fit" title="lightning.pytorch.trainer.trainer.Trainer.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></a></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>Lightning tries to add the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="val-dataloader">
<h4>val_dataloader<a class="headerlink" href="#val-dataloader" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">val_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>An iterable or collection of iterables specifying validation samples.</p>
<p>For more information about multiple dataloaders, see this <span class="xref std std-ref">section</span>.</p>
<p>The dataloader you return will not be reloaded unless you set
<code class="xref py py-paramref docutils literal notranslate"><span class="pre">reload_dataloaders_every_n_epochs</span></code> to
a positive integer.</p>
<p>It’s recommended that all data downloads and preparation happen in <code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code>.</p>
<ul class="simple">
<li><p><a class="reference internal" href="../api/lightning.pytorch.trainer.trainer.Trainer.html#lightning.pytorch.trainer.trainer.Trainer.fit" title="lightning.pytorch.trainer.trainer.Trainer.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></a></p></li>
<li><p><a class="reference internal" href="../api/lightning.pytorch.trainer.trainer.Trainer.html#lightning.pytorch.trainer.trainer.Trainer.validate" title="lightning.pytorch.trainer.trainer.Trainer.validate"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validate()</span></code></a></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>Lightning tries to add the correct sampler for distributed and arbitrary hardware
There is no need to set it yourself.</p>
</div>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>If you don’t need a validation dataset and a <code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code>, you don’t need to
implement this method.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="test-dataloader">
<h4>test_dataloader<a class="headerlink" href="#test-dataloader" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">test_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>An iterable or collection of iterables specifying test samples.</p>
<p>For more information about multiple dataloaders, see this <span class="xref std std-ref">section</span>.</p>
<p>For data processing use the following pattern:</p>
<blockquote>
<div><ul class="simple">
<li><p>download in <code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></p></li>
<li><p>process and split in <code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></p></li>
</ul>
</div></blockquote>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">경고</p>
<p>do not assign state in prepare_data</p>
</div>
<ul class="simple">
<li><p><a class="reference internal" href="../api/lightning.pytorch.trainer.trainer.Trainer.html#lightning.pytorch.trainer.trainer.Trainer.test" title="lightning.pytorch.trainer.trainer.Trainer.test"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test()</span></code></a></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>Lightning tries to add the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p>
</div>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>If you don’t need a test dataset and a <code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code>, you don’t need to implement
this method.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="predict-dataloader">
<h4>predict_dataloader<a class="headerlink" href="#predict-dataloader" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">predict_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>An iterable or collection of iterables specifying prediction samples.</p>
<p>For more information about multiple dataloaders, see this <span class="xref std std-ref">section</span>.</p>
<p>It’s recommended that all data downloads and preparation happen in <code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code>.</p>
<ul class="simple">
<li><p><a class="reference internal" href="../api/lightning.pytorch.trainer.trainer.Trainer.html#lightning.pytorch.trainer.trainer.Trainer.predict" title="lightning.pytorch.trainer.trainer.Trainer.predict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">predict()</span></code></a></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>Lightning tries to add the correct sampler for distributed and arbitrary hardware
There is no need to set it yourself.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">반환 형식</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></a></p>
</dd>
<dt class="field-even">반환</dt>
<dd class="field-even"><p>A <a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code></a> or a sequence of them specifying prediction samples.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="transfer-batch-to-device">
<h4>transfer_batch_to_device<a class="headerlink" href="#transfer-batch-to-device" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">transfer_batch_to_device</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Override this hook if your <a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code></a> returns tensors wrapped in a custom
data structure.</p>
<p>The data types listed below (and any arbitrary nesting of them) are supported out of the box:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a> or anything that implements <cite>.to(…)</cite></p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">list</span></code></a></p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></a></p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></a></p></li>
</ul>
<p>For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, …).</p>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>This hook should only transfer the data and not modify it, nor should it move the data to
any other device than the one passed in as argument (unless you know what you are doing).
To check the current state of execution of this hook you can use
<code class="docutils literal notranslate"><span class="pre">self.trainer.training/testing/validating/predicting</span></code> so that you can
add different logic as per your requirement.</p>
</div>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>This hook only runs on single GPU training and DDP (no data-parallel).
Data-Parallel support will come in near future.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">매개변수</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.transfer_batch_to_device.params.batch"></span><strong>batch</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.transfer_batch_to_device.params.batch">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></a>) – A batch of data that needs to be transferred to a new device.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.transfer_batch_to_device.params.device"></span><strong>device</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.transfer_batch_to_device.params.device">¶</a> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(PyTorch v2.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">device</span></code></a>) – The target device as defined in PyTorch.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.transfer_batch_to_device.params.dataloader_idx"></span><strong>dataloader_idx</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.transfer_batch_to_device.params.dataloader_idx">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a>) – The index of the dataloader to which the batch belongs.</p></li>
</ul>
</dd>
<dt class="field-even">반환 형식</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></a></p>
</dd>
<dt class="field-odd">반환</dt>
<dd class="field-odd"><p>A reference to the data on the new device.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">transfer_batch_to_device</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">CustomBatch</span><span class="p">):</span>
        <span class="c1"># move all tensors in your custom data structure to the device</span>
        <span class="n">batch</span><span class="o">.</span><span class="n">samples</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">samples</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">batch</span><span class="o">.</span><span class="n">targets</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">targets</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">dataloader_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># skip device transfer for the first dataloader or anything you wish</span>
        <span class="k">pass</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">transfer_batch_to_device</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">batch</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">예외 발생</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>MisconfigurationException</strong> – If using data-parallel, <code class="docutils literal notranslate"><span class="pre">Trainer(strategy='dp')</span></code>.</p></li>
<li><p><strong>MisconfigurationException</strong> – If using IPUs, <code class="docutils literal notranslate"><span class="pre">Trainer(accelerator='ipu')</span></code>.</p></li>
</ul>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">더 보기</p>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">move_data_to_device()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">apply_to_collection()</span></code></p></li>
</ul>
</div>
</dd></dl>

</section>
<section id="on-before-batch-transfer">
<h4>on_before_batch_transfer<a class="headerlink" href="#on-before-batch-transfer" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_before_batch_transfer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Override to alter or apply batch augmentations to your batch before it is transferred to the device.</p>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>To check the current state of execution of this hook you can use
<code class="docutils literal notranslate"><span class="pre">self.trainer.training/testing/validating/predicting</span></code> so that you can
add different logic as per your requirement.</p>
</div>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>This hook only runs on single GPU training and DDP (no data-parallel).
Data-Parallel support will come in near future.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">매개변수</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.on_before_batch_transfer.params.batch"></span><strong>batch</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.on_before_batch_transfer.params.batch">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></a>) – A batch of data that needs to be altered or augmented.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.on_before_batch_transfer.params.dataloader_idx"></span><strong>dataloader_idx</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.on_before_batch_transfer.params.dataloader_idx">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a>) – The index of the dataloader to which the batch belongs.</p></li>
</ul>
</dd>
<dt class="field-even">반환 형식</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></a></p>
</dd>
<dt class="field-odd">반환</dt>
<dd class="field-odd"><p>A batch of data</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_before_batch_transfer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">batch</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">더 보기</p>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_after_batch_transfer()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">transfer_batch_to_device()</span></code></p></li>
</ul>
</div>
</dd></dl>

</section>
<section id="on-after-batch-transfer">
<h4>on_after_batch_transfer<a class="headerlink" href="#on-after-batch-transfer" title="이 표제에 대한 퍼머링크">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_after_batch_transfer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Override to alter or apply batch augmentations to your batch after it is transferred to the device.</p>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>To check the current state of execution of this hook you can use
<code class="docutils literal notranslate"><span class="pre">self.trainer.training/testing/validating/predicting</span></code> so that you can
add different logic as per your requirement.</p>
</div>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>This hook only runs on single GPU training and DDP (no data-parallel).
Data-Parallel support will come in near future.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">매개변수</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.on_after_batch_transfer.params.batch"></span><strong>batch</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.on_after_batch_transfer.params.batch">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></a>) – A batch of data that needs to be altered or augmented.</p></li>
<li><p><span class="target" id="lightning.pytorch.core.module.LightningModule.on_after_batch_transfer.params.dataloader_idx"></span><strong>dataloader_idx</strong><a class="paramlink headerlink reference internal" href="#lightning.pytorch.core.module.LightningModule.on_after_batch_transfer.params.dataloader_idx">¶</a> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(Python v3.11에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a>) – The index of the dataloader to which the batch belongs.</p></li>
</ul>
</dd>
<dt class="field-even">반환 형식</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(Python v3.11에서)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></a></p>
</dd>
<dt class="field-odd">반환</dt>
<dd class="field-odd"><p>A batch of data</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_after_batch_transfer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gpu_transforms</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">batch</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">예외 발생</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>MisconfigurationException</strong> – If using data-parallel, <code class="docutils literal notranslate"><span class="pre">Trainer(strategy='dp')</span></code>.</p></li>
<li><p><strong>MisconfigurationException</strong> – If using IPUs, <code class="docutils literal notranslate"><span class="pre">Trainer(accelerator='ipu')</span></code>.</p></li>
</ul>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">더 보기</p>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_before_batch_transfer()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">transfer_batch_to_device()</span></code></p></li>
</ul>
</div>
</dd></dl>

</section>
</section>
</section>
</section>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="trainer.html" class="btn btn-neutral float-right" title="Trainer" accesskey="n" rel="next">Next <img src="../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="../levels/expert.html" class="btn btn-neutral" title="Expert skills" accesskey="p" rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright Copyright (c) 2018-2023, Lightning AI et al...

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>
          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">LightningModule</a><ul>
<li><a class="reference internal" href="#starter-example">Starter Example</a></li>
<li><a class="reference internal" href="#training">Training</a><ul>
<li><a class="reference internal" href="#training-loop">Training Loop</a></li>
<li><a class="reference internal" href="#train-epoch-level-metrics">Train Epoch-level Metrics</a></li>
<li><a class="reference internal" href="#train-epoch-level-operations">Train Epoch-level Operations</a></li>
</ul>
</li>
<li><a class="reference internal" href="#validation">Validation</a><ul>
<li><a class="reference internal" href="#validation-loop">Validation Loop</a></li>
<li><a class="reference internal" href="#validation-epoch-level-metrics">Validation Epoch-level Metrics</a></li>
</ul>
</li>
<li><a class="reference internal" href="#testing">Testing</a><ul>
<li><a class="reference internal" href="#test-loop">Test Loop</a></li>
</ul>
</li>
<li><a class="reference internal" href="#inference">Inference</a><ul>
<li><a class="reference internal" href="#prediction-loop">Prediction Loop</a></li>
<li><a class="reference internal" href="#inference-in-research">Inference in Research</a></li>
<li><a class="reference internal" href="#inference-in-production">Inference in Production</a></li>
</ul>
</li>
<li><a class="reference internal" href="#save-hyperparameters">Save Hyperparameters</a><ul>
<li><a class="reference internal" href="#id1">save_hyperparameters</a></li>
<li><a class="reference internal" href="#excluding-hyperparameters">Excluding hyperparameters</a></li>
<li><a class="reference internal" href="#load-from-checkpoint">load_from_checkpoint</a></li>
</ul>
</li>
<li><a class="reference internal" href="#child-modules">Child Modules</a></li>
<li><a class="reference internal" href="#lightningmodule-api">LightningModule API</a><ul>
<li><a class="reference internal" href="#methods">Methods</a><ul>
<li><a class="reference internal" href="#all-gather">all_gather</a></li>
<li><a class="reference internal" href="#configure-callbacks">configure_callbacks</a></li>
<li><a class="reference internal" href="#configure-optimizers">configure_optimizers</a></li>
<li><a class="reference internal" href="#forward">forward</a></li>
<li><a class="reference internal" href="#freeze">freeze</a></li>
<li><a class="reference internal" href="#log">log</a></li>
<li><a class="reference internal" href="#log-dict">log_dict</a></li>
<li><a class="reference internal" href="#lr-schedulers">lr_schedulers</a></li>
<li><a class="reference internal" href="#manual-backward">manual_backward</a></li>
<li><a class="reference internal" href="#optimizers">optimizers</a></li>
<li><a class="reference internal" href="#print">print</a></li>
<li><a class="reference internal" href="#predict-step">predict_step</a></li>
<li><a class="reference internal" href="#id2">save_hyperparameters</a></li>
<li><a class="reference internal" href="#toggle-optimizer">toggle_optimizer</a></li>
<li><a class="reference internal" href="#test-step">test_step</a></li>
<li><a class="reference internal" href="#to-onnx">to_onnx</a></li>
<li><a class="reference internal" href="#to-torchscript">to_torchscript</a></li>
<li><a class="reference internal" href="#training-step">training_step</a></li>
<li><a class="reference internal" href="#unfreeze">unfreeze</a></li>
<li><a class="reference internal" href="#untoggle-optimizer">untoggle_optimizer</a></li>
<li><a class="reference internal" href="#validation-step">validation_step</a></li>
</ul>
</li>
<li><a class="reference internal" href="#properties">Properties</a><ul>
<li><a class="reference internal" href="#current-epoch">current_epoch</a></li>
<li><a class="reference internal" href="#device">device</a></li>
<li><a class="reference internal" href="#global-rank">global_rank</a></li>
<li><a class="reference internal" href="#global-step">global_step</a></li>
<li><a class="reference internal" href="#hparams">hparams</a></li>
<li><a class="reference internal" href="#logger">logger</a></li>
<li><a class="reference internal" href="#loggers">loggers</a></li>
<li><a class="reference internal" href="#local-rank">local_rank</a></li>
<li><a class="reference internal" href="#precision">precision</a></li>
<li><a class="reference internal" href="#trainer">trainer</a></li>
<li><a class="reference internal" href="#prepare-data-per-node">prepare_data_per_node</a></li>
<li><a class="reference internal" href="#automatic-optimization">automatic_optimization</a></li>
<li><a class="reference internal" href="#example-input-array">example_input_array</a></li>
</ul>
</li>
<li><a class="reference internal" href="#hooks">Hooks</a><ul>
<li><a class="reference internal" href="#backward">backward</a></li>
<li><a class="reference internal" href="#on-before-backward">on_before_backward</a></li>
<li><a class="reference internal" href="#on-after-backward">on_after_backward</a></li>
<li><a class="reference internal" href="#on-before-zero-grad">on_before_zero_grad</a></li>
<li><a class="reference internal" href="#on-fit-start">on_fit_start</a></li>
<li><a class="reference internal" href="#on-fit-end">on_fit_end</a></li>
<li><a class="reference internal" href="#on-load-checkpoint">on_load_checkpoint</a></li>
<li><a class="reference internal" href="#on-save-checkpoint">on_save_checkpoint</a></li>
<li><a class="reference internal" href="#id3">load_from_checkpoint</a></li>
<li><a class="reference internal" href="#on-train-start">on_train_start</a></li>
<li><a class="reference internal" href="#on-train-end">on_train_end</a></li>
<li><a class="reference internal" href="#on-validation-start">on_validation_start</a></li>
<li><a class="reference internal" href="#on-validation-end">on_validation_end</a></li>
<li><a class="reference internal" href="#on-test-batch-start">on_test_batch_start</a></li>
<li><a class="reference internal" href="#on-test-batch-end">on_test_batch_end</a></li>
<li><a class="reference internal" href="#on-test-epoch-start">on_test_epoch_start</a></li>
<li><a class="reference internal" href="#on-test-epoch-end">on_test_epoch_end</a></li>
<li><a class="reference internal" href="#on-test-start">on_test_start</a></li>
<li><a class="reference internal" href="#on-test-end">on_test_end</a></li>
<li><a class="reference internal" href="#on-predict-batch-start">on_predict_batch_start</a></li>
<li><a class="reference internal" href="#on-predict-batch-end">on_predict_batch_end</a></li>
<li><a class="reference internal" href="#on-predict-epoch-start">on_predict_epoch_start</a></li>
<li><a class="reference internal" href="#on-predict-epoch-end">on_predict_epoch_end</a></li>
<li><a class="reference internal" href="#on-predict-start">on_predict_start</a></li>
<li><a class="reference internal" href="#on-predict-end">on_predict_end</a></li>
<li><a class="reference internal" href="#on-train-batch-start">on_train_batch_start</a></li>
<li><a class="reference internal" href="#on-train-batch-end">on_train_batch_end</a></li>
<li><a class="reference internal" href="#on-train-epoch-start">on_train_epoch_start</a></li>
<li><a class="reference internal" href="#on-train-epoch-end">on_train_epoch_end</a></li>
<li><a class="reference internal" href="#on-validation-batch-start">on_validation_batch_start</a></li>
<li><a class="reference internal" href="#on-validation-batch-end">on_validation_batch_end</a></li>
<li><a class="reference internal" href="#on-validation-epoch-start">on_validation_epoch_start</a></li>
<li><a class="reference internal" href="#on-validation-epoch-end">on_validation_epoch_end</a></li>
<li><a class="reference internal" href="#configure-model">configure_model</a></li>
<li><a class="reference internal" href="#on-validation-model-eval">on_validation_model_eval</a></li>
<li><a class="reference internal" href="#on-validation-model-train">on_validation_model_train</a></li>
<li><a class="reference internal" href="#on-test-model-eval">on_test_model_eval</a></li>
<li><a class="reference internal" href="#on-test-model-train">on_test_model_train</a></li>
<li><a class="reference internal" href="#on-before-optimizer-step">on_before_optimizer_step</a></li>
<li><a class="reference internal" href="#configure-gradient-clipping">configure_gradient_clipping</a></li>
<li><a class="reference internal" href="#optimizer-step">optimizer_step</a></li>
<li><a class="reference internal" href="#optimizer-zero-grad">optimizer_zero_grad</a></li>
<li><a class="reference internal" href="#prepare-data">prepare_data</a></li>
<li><a class="reference internal" href="#setup">setup</a></li>
<li><a class="reference internal" href="#teardown">teardown</a></li>
<li><a class="reference internal" href="#train-dataloader">train_dataloader</a></li>
<li><a class="reference internal" href="#val-dataloader">val_dataloader</a></li>
<li><a class="reference internal" href="#test-dataloader">test_dataloader</a></li>
<li><a class="reference internal" href="#predict-dataloader">predict_dataloader</a></li>
<li><a class="reference internal" href="#transfer-batch-to-device">transfer_batch_to_device</a></li>
<li><a class="reference internal" href="#on-before-batch-transfer">on_before_batch_transfer</a></li>
<li><a class="reference internal" href="#on-after-batch-transfer">on_after_batch_transfer</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  

  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
         <script src="../_static/jquery.js"></script>
         <script src="../_static/underscore.js"></script>
         <script src="../_static/doctools.js"></script>
         <script src="../_static/clipboard.min.js"></script>
         <script src="../_static/copybutton.js"></script>
         <script src="../_static/copybutton.js"></script>
         <script>let toggleHintShow = 'Click to show';</script>
         <script>let toggleHintHide = 'Click to hide';</script>
         <script>let toggleOpenOnPrint = 'true';</script>
         <script src="../_static/togglebutton.js"></script>
         <script src="../_static/translations.js"></script>
         <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Best practices', 'Optional Extensions', 'Tutorials', 'API References', 'Bolts', 'Examples', 'Partner Domain Frameworks', 'Community'];
</script>



  <!-- Begin Footer -->

  <!-- <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources"> -->
    <!-- <div class="container"> -->
      <!-- <div class="row"> -->
        <!--
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://lightning.ai/docs/pytorch/latest/">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://lightning.ai/docs/pytorch/latest/#tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://lightning.ai/docs/pytorch/latest/#community-examples">View Resources</a>
        </div>
        -->
      <!-- </div> -->
    <!-- </div> -->
  <!-- </div> -->

  <!--
  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://lightning.ai/docs/pytorch/latest/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://lightning.ai/docs/pytorch/latest/">PyTorch</a></li>
            <li><a href="https://lightning.ai/docs/pytorch/latest/starter/introduction.html">Get Started</a></li>
            <li><a href="https://lightning.ai/docs/pytorch/latest/">Features</a></li>
            <li><a href="">Ecosystem</a></li>
            <li><a href="https://lightning.ai/pages/blog/">Blog</a></li>
            <li><a href="https://github.com/Lightning-AI/lightning/blob/master/.github/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://lightning.ai/docs/pytorch/latest/#community-examples">Resources</a></li>
            <li><a href="https://lightning.ai/docs/pytorch/latest/#tutorials">Tutorials</a></li>
            <li><a href="https://lightning.ai/docs/pytorch/latest/">Docs</a></li>
            <li><a href="https://www.pytorchlightning.ai/community" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/Lightning-AI/lightning/issues" target="_blank">Github Issues</a></li>
            <li><a href="" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Stay Connected</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/LightningAI" target="_blank" class="twitter"></a>
            <a href="" target="_blank" class="youtube"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>
  -->

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. Read PyTorch Lightning's <a href="https://pytorchlightning.ai/privacy-policy">Privacy Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://lightning.ai/docs/pytorch/latest/" aria-label="PyTorch Lightning"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <!-- <li>
            <a href="https://lightning.ai/docs/pytorch/latest/starter/introduction.html">Get Started</a>
          </li>

          <li>
            <a href="https://lightning.ai/pages/blog/">Blog</a>
          </li>

          <li class="resources-mobile-menu-title">
            Ecosystem
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://lightning.ai/docs/pytorch/stable/">PyTorch Lightning</a>
            </li>

            <li>
              <a href="https://lightning.ai/docs/fabric/stable/">Lightning Fabric</a>
            </li>

            <li>
              <a href="https://torchmetrics.readthedocs.io/en/stable/">TorchMetrics</a>
            </li>

            <li>
              <a href="https://lightning.ai/docs/fabric/stable/">Fabric</a>
            </li>
          </ul> -->

          <!--<li class="resources-mobile-menu-title">
            Resources
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://lightning.ai/docs/pytorch/latest/#community-examples">Developer Resources</a>
            </li>

            <li>
              <a href="https://lightning.ai/docs/pytorch/latest/">About</a>
            </li>

            <li>
              <a href="">Models (Beta)</a>
            </li>

            <li>
              <a href="https://www.pytorchlightning.ai/community">Community</a>
            </li>

            <li>
              <a href="https://lightning.ai/forums/">Forums</a>
            </li>
          </ul>-->

          <li>
            <a href="https://www.lightning.ai/">Lightning.ai</a>
          </li>

          <li>
            <a href="https://pytorch.kr/">파이토치 한국 사용자 모임</a>
          </li>

          <li>
            <a href="https://discuss.pytorch.kr/">파이토치 한국어 커뮤니티</a>
          </li>

        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>

  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-5SCNQBF5"
  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->
 </body>
</html>