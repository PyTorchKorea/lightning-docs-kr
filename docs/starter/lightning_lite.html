


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>

  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="google-site-verification" content="okUst94cAlWSsUsGZTB4xSS4UKTtRV8Nu5XZ9pdd3Aw" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>LightningLite (Stepping Stone to Lightning) &mdash; PyTorch Lightning 1.7.0dev documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/icon.svg"/>
  
  
  
    <link rel="canonical" href="https://pytorch-lightning.readthedocs.io/en/stable//starter/lightning_lite.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../_static/togglebutton.css" type="text/css" />
  <link rel="stylesheet" href="../_static/main.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sphinx_paramlinks.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
  <!-- Google Analytics -->
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-82W25RV60Q"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-82W25RV60Q');
    </script>
  
  <!-- End Google Analytics -->
  

  
  <script src="../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/UCity/UCity-Light.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/UCity/UCity-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/UCity/UCity-Semibold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/Inconsolata/Inconsolata.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <script defer src="https://use.fontawesome.com/releases/v6.1.1/js/all.js" integrity="sha384-xBXmu0dk1bEoiwd71wOonQLyH+VpgR1XcDH3rtxrLww5ajNTuMvBdL5SOiFZnNdp" crossorigin="anonymous"></script>
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch-lightning.rtfd.io/en/latest/" aria-label="PyTorch Lightning">
      <!--  <img class="call-to-action-img" src="../_static/images/logo-lightning-icon.png"/> -->
      </a>

      <div class="main-menu">
        <ul>
          <!-- <li>
            <a href="https://pytorch-lightning.readthedocs.io/en/latest/starter/introduction.html">Get Started</a>
          </li> -->

          <!-- <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch-lightning.readthedocs.io/en/stable/">
                  <span class="dropdown-title">PyTorch Lightning</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://torchmetrics.readthedocs.io/en/stable/">
                  <span class="dropdown-title">TorchMetrics</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning-flash.readthedocs.io/en/stable/">
                  <span class="dropdown-title">Lightning Flash</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning-transformers.readthedocs.io/en/stable/">
                  <span class="dropdown-title">Lightning Transformers</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning-bolts.readthedocs.io/en/stable/">
                  <span class="dropdown-title">Lightning Bolts</span>
                </a>
            </div>
          </li> -->

          <!--<li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://www.pytorchlightning.ai/community">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch-lightning.readthedocs.io/en/latest/#community-examples">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://github.com/PyTorchLightning/pytorch-lightning/discussions" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>-->

          <!-- <li>
            <a href="https://github.com/PyTorchLightning/pytorch-lightning">GitHub</a>
          </li>

          <li>
            <a href="https://www.grid.ai/">Train on the cloud</a>
          </li> -->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  1.7.0dev
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Lightning in 15 minutes</a></li>
<li class="toctree-l1"><a class="reference internal" href="converting.html">Organize existing PyTorch into Lightning</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Level Up</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../levels/core_skills.html">Basic skills</a></li>
<li class="toctree-l1"><a class="reference internal" href="../levels/intermediate.html">Intermediate skills</a></li>
<li class="toctree-l1"><a class="reference internal" href="../levels/advanced.html">Advanced skills</a></li>
<li class="toctree-l1"><a class="reference internal" href="../levels/expert.html">Expert skills</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Core API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../common/lightning_module.html">LightningModule</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/trainer.html">Trainer</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Common Workflows</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../common/evaluation.html">Avoid overfitting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model/build_model.html">Build a Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/hyperparameters.html">Configure hyperparameters from the CLI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/progress_bar.html">Customize the progress bar</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/production.html">Deploy models into production</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/training_tricks.html">Effective Training Techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cli/lightning_cli.html">Eliminate config boilerplate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tuning/profiler.html">Find bottlenecks in your code</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/transfer_learning.html">Finetune a model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../visualize/logging_intermediate.html">Manage experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../clouds/cluster.html">Run on an on-prem cluster</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/model_parallel.html">Train 1 trillion+ parameter models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../clouds/cloud_training.html">Train on the cloud</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/checkpointing.html">Save and load model progress</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/precision.html">Save memory with half-precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accelerators/gpu.html">Train on single or multiple GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accelerators/hpu.html">Train on single or multiple HPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accelerators/ipu.html">Train on single or multiple IPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accelerators/tpu.html">Train on single or multiple TPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model/own_your_loop.html">Use a pure PyTorch training loop</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Glossary</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../extensions/accelerator.html">Accelerators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../extensions/callbacks.html">Callback</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/checkpointing.html">Checkpointing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../clouds/cluster.html">Cluster</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/checkpointing_advanced.html">Cloud checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/console_logs.html">Console Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/debugging.html">Debugging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/early_stopping.html">Early stopping</a></li>
<li class="toctree-l1"><a class="reference internal" href="../visualize/experiment_managers.html">Experiment manager (Logger)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../clouds/fault_tolerant_training.html">Fault tolerant training</a></li>
<li class="toctree-l1"><a class="reference external" href="https://lightning-flash.readthedocs.io/en/stable/">Flash</a></li>
<li class="toctree-l1"><a class="reference internal" href="../clouds/cloud_training.html">Grid AI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accelerators/gpu.html">GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/precision.html">Half precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accelerators/hpu.html">HPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/production_intermediate.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accelerators/ipu.html">IPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cli/lightning_cli.html">Lightning CLI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model/build_model_expert.html">Raw PyTorch loop (expert)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model/build_model_expert.html#lightninglite-stepping-stone-to-lightning">LightningLite (Stepping Stone to Lightning)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data/datamodule.html">LightningDataModule</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/lightning_module.html">LightningModule</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch-lightning.readthedocs.io/en/stable/ecosystem/transformers.html">Lightning Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../visualize/loggers.html">Log</a></li>
<li class="toctree-l1"><a class="reference internal" href="../extensions/loops.html">Loops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accelerators/tpu.html">TPU</a></li>
<li class="toctree-l1"><a class="reference external" href="https://torchmetrics.readthedocs.io/en/stable/">Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model/build_model.html">Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/model_parallel.html">Model Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../extensions/plugins.html">Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/progress_bar.html">Progress bar</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/production_advanced.html">Production</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/production_basic.html">Predict</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tuning/profiler.html">Profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/pruning_quantization.html">Pruning and Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/remote_fs.html">Remote filesystem and FSSPEC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/strategy_registry.html">Strategy registry</a></li>
<li class="toctree-l1"><a class="reference internal" href="style_guide.html">Style guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../clouds/run_intermediate.html">Sweep</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/training_tricks.html">SWA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../clouds/cluster_advanced.html">SLURM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/transfer_learning.html">Transfer learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/trainer.html">Trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../clouds/cluster_intermediate_2.html">Torch distributed</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Hands-on Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.youtube.com/playlist?list=PLaMu-SDt_RB5NUm67hU2pdE75j6KaIOv2">PyTorch Lightning 101 class</a></li>
<li class="toctree-l1"><a class="reference external" href="https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09">From PyTorch to PyTorch Lightning [Blog]</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.youtube.com/watch?v=QHww1JH7IDU">From PyTorch to PyTorch Lightning [Video]</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>LightningLite (Stepping Stone to Lightning)</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/starter/lightning_lite.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <section id="lightninglite-stepping-stone-to-lightning">
<h1>LightningLite (Stepping Stone to Lightning)<a class="headerlink" href="#lightninglite-stepping-stone-to-lightning" title="Permalink to this headline">¶</a></h1>
<p><code class="xref py py-class docutils literal notranslate"><span class="pre">LightningLite</span></code> enables pure PyTorch users to scale their existing code
on any kind of device while retaining full control over their own loops and optimization logic.</p>
<a class="reference internal image-reference" href="https://pl-public-data.s3.amazonaws.com/docs/static/images/lite/lightning_lite.gif"><img alt="Animation showing how to convert your PyTorch code to LightningLite." class="align-center" src="https://pl-public-data.s3.amazonaws.com/docs/static/images/lite/lightning_lite.gif" style="width: 500px;" /></a>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p><code class="xref py py-class docutils literal notranslate"><span class="pre">LightningLite</span></code> is the right tool for you if you match one of the two following descriptions:</p>
<ul class="simple">
<li><p>I want to quickly scale my existing code to multiple devices with minimal code changes.</p></li>
<li><p>I would like to convert my existing code to the Lightning API, but a full path to Lightning transition might be too complex. I am looking for a stepping stone to ensure reproducibility during the transition.</p></li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><code class="xref py py-class docutils literal notranslate"><span class="pre">LightningLite</span></code> is currently a beta feature. Its API is subject to change based on your feedback.</p>
</div>
<hr class="docutils" />
<section id="learn-by-example">
<h2>Learn by example<a class="headerlink" href="#learn-by-example" title="Permalink to this headline">¶</a></h2>
<section id="my-existing-pytorch-code">
<h3>My Existing PyTorch Code<a class="headerlink" href="#my-existing-pytorch-code" title="Permalink to this headline">¶</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">run</span></code> function contains custom training loop used to train <code class="docutils literal notranslate"><span class="pre">MyModel</span></code> on <code class="docutils literal notranslate"><span class="pre">MyDataset</span></code> for <code class="docutils literal notranslate"><span class="pre">num_epochs</span></code> epochs.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">Dataset</span>


<span class="k">class</span> <span class="nc">MyModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="o">...</span>


<span class="k">class</span> <span class="nc">MyDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="o">...</span>


<span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
    <span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">(</span><span class="o">...</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="o">...</span><span class="p">)</span>

    <span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">MyDataset</span><span class="p">(</span><span class="o">...</span><span class="p">),</span> <span class="o">...</span><span class="p">)</span>

    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>


<span class="n">run</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="convert-to-lightninglite">
<h3>Convert to LightningLite<a class="headerlink" href="#convert-to-lightninglite" title="Permalink to this headline">¶</a></h3>
<p>Here are five required steps to convert to <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningLite</span></code>.</p>
<ol class="arabic simple">
<li><p>Subclass <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningLite</span></code> and override its <code class="xref py py-meth docutils literal notranslate"><span class="pre">run()</span></code> method.</p></li>
<li><p>Move the body of your existing <code class="docutils literal notranslate"><span class="pre">run</span></code> function into <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningLite</span></code> <code class="docutils literal notranslate"><span class="pre">run</span></code> method.</p></li>
<li><p>Remove all <code class="docutils literal notranslate"><span class="pre">.to(...)</span></code>, <code class="docutils literal notranslate"><span class="pre">.cuda()</span></code> etc calls since <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningLite</span></code> will take care of it.</p></li>
<li><p>Apply <code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code> over each model and optimizers pair and <code class="xref py py-meth docutils literal notranslate"><span class="pre">setup_dataloaders()</span></code> on all your dataloaders and replace <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code> by <code class="docutils literal notranslate"><span class="pre">self.backward(loss)</span></code>.</p></li>
<li><p>Instantiate your <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningLite</span></code> subclass and call its <code class="xref py py-meth docutils literal notranslate"><span class="pre">run()</span></code> method.</p></li>
</ol>
<div class="line-block">
<div class="line"><br /></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">Dataset</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.lite</span> <span class="kn">import</span> <span class="n">LightningLite</span>


<span class="k">class</span> <span class="nc">MyModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="o">...</span>


<span class="k">class</span> <span class="nc">MyDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="o">...</span>


<span class="k">class</span> <span class="nc">Lite</span><span class="p">(</span><span class="n">LightningLite</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">):</span>

        <span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="o">...</span><span class="p">)</span>
        <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>  <span class="c1"># Scale your model / optimizers</span>

        <span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">MyDataset</span><span class="p">(</span><span class="o">...</span><span class="p">),</span> <span class="o">...</span><span class="p">)</span>
        <span class="n">dataloader</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">setup_dataloaders</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)</span>  <span class="c1"># Scale your dataloaders</span>

        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">num_epochs</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
                <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>  <span class="c1"># instead of loss.backward()</span>
                <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>


<span class="n">Lite</span><span class="p">(</span><span class="o">...</span><span class="p">)</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
</pre></div>
</div>
<p>That’s all. You can now train on any kind of device and scale your training. Check out <a class="reference external" href="https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/basic_examples/mnist_examples/image_classifier_2_lite.py">this</a> full MNIST training example with LightningLite.</p>
<p><code class="xref py py-class docutils literal notranslate"><span class="pre">LightningLite</span></code> takes care of device management, so you don’t have to.
You should remove any device-specific logic within your code.</p>
<p>Here is how to train on eight GPUs with <a class="reference external" href="https://pytorch.org/docs/1.10.0/generated/torch.Tensor.bfloat16.html">torch.bfloat16</a> precision:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Lite</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;ddp&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">precision</span><span class="o">=</span><span class="s2">&quot;bf16&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
<p>Here is how to use <a class="reference external" href="https://www.deepspeed.ai/news/2021/03/07/zero3-offload.html">DeepSpeed Zero3</a> with eight GPUs and precision 16:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Lite</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;deepspeed&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">precision</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="xref py py-class docutils literal notranslate"><span class="pre">LightningLite</span></code> can also figure it out automatically for you!</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Lite</span><span class="p">(</span><span class="n">devices</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">precision</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
<p>You can also easily use distributed collectives if required.
Here is an example while running on 256 GPUs (eight GPUs times 32 nodes).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Lite</span><span class="p">(</span><span class="n">LightningLite</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

        <span class="c1"># Transfer and concatenate tensors across processes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

        <span class="c1"># Transfer an object from one process to all the others</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">src</span><span class="o">=...</span><span class="p">)</span>

        <span class="c1"># The total number of processes running across all devices and nodes.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">world_size</span>

        <span class="c1"># The global index of the current process across all devices and nodes.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span>

        <span class="c1"># The index of the current process among the processes running on the local node.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">local_rank</span>

        <span class="c1"># The index of the current node.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">node_rank</span>

        <span class="c1"># Wether this global rank is rank zero.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_global_zero</span><span class="p">:</span>
            <span class="c1"># do something on rank 0</span>
            <span class="o">...</span>

        <span class="c1"># Wait for all processes to enter this call.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>


<span class="n">Lite</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;ddp&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">num_nodes</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div>
</div>
<p>If you require custom data or model device placement, you can deactivate
<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningLite</span></code> automatic placement by doing
<code class="docutils literal notranslate"><span class="pre">self.setup_dataloaders(...,</span> <span class="pre">move_to_device=False)</span></code> for the data and
<code class="docutils literal notranslate"><span class="pre">self.setup(...,</span> <span class="pre">move_to_device=False)</span></code> for the model.
Furthermore, you can access the current device from <code class="docutils literal notranslate"><span class="pre">self.device</span></code> or
rely on <code class="xref py py-meth docutils literal notranslate"><span class="pre">to_device()</span></code>
utility to move an object to the current device.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We recommend instantiating the models within the <code class="xref py py-meth docutils literal notranslate"><span class="pre">run()</span></code> method as large models would cause an out-of-memory error otherwise.</p>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If you have hundreds or thousands of lines within your <code class="xref py py-meth docutils literal notranslate"><span class="pre">run()</span></code> function
and you are feeling unsure about them, then that is the correct feeling.
In 2019, our <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code> was getting larger
and we got the same feeling, so we started to organize our code for simplicity, interoperability and standardization.
This is definitely a good sign that you should consider refactoring your code and / or switching to
<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code> ultimately.</p>
</div>
</section>
<hr class="docutils" />
<section id="distributed-training-pitfalls">
<h3>Distributed Training Pitfalls<a class="headerlink" href="#distributed-training-pitfalls" title="Permalink to this headline">¶</a></h3>
<p>The <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningLite</span></code> provides you with the tools to scale your training,
but there are several major challenges ahead of you now:</p>
<table class="colwidths-given docutils align-default">
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p>Processes divergence</p></td>
<td><p>This happens when processes execute a different section of the code due to different if/else conditions, race conditions on existing files and so on, resulting in hanging.</p></td>
</tr>
<tr class="row-even"><td><p>Cross processes reduction</p></td>
<td><p>Miscalculated metrics or gradients due to errors in their reduction.</p></td>
</tr>
<tr class="row-odd"><td><p>Large sharded models</p></td>
<td><p>Instantiation, materialization and state management of large models.</p></td>
</tr>
<tr class="row-even"><td><p>Rank 0 only actions</p></td>
<td><p>Logging, profiling, and so on.</p></td>
</tr>
<tr class="row-odd"><td><p>Checkpointing / Early stopping / Callbacks / Logging</p></td>
<td><p>Ability to customize your training behavior easily and make it stateful.</p></td>
</tr>
<tr class="row-even"><td><p>Fault-tolerant training</p></td>
<td><p>Ability to resume from a failure as if it never happened.</p></td>
</tr>
</tbody>
</table>
<p>If you are facing one of those challenges, then you are already meeting the limit of <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningLite</span></code>.
We recommend you to convert to <a class="reference internal" href="introduction.html"><span class="doc">Lightning</span></a>, so you never have to worry about those.</p>
</section>
<hr class="docutils" />
<section id="convert-to-lightning">
<h3>Convert to Lightning<a class="headerlink" href="#convert-to-lightning" title="Permalink to this headline">¶</a></h3>
<p><code class="xref py py-class docutils literal notranslate"><span class="pre">LightningLite</span></code> is a stepping stone to transition fully to the Lightning API and benefit
from its hundreds of features.</p>
<p>You can see our <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningLite</span></code> class as a
future <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code>, and slowly refactor your code into its API.
Below, the <code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code>,
<code class="xref py py-meth docutils literal notranslate"><span class="pre">configure_optimizers()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">train_dataloader()</span></code> methods
are implemented.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Lite</span><span class="p">(</span><span class="n">LightningLite</span><span class="p">):</span>

    <span class="c1"># 1. This would become the LightningModule `__init__` function.</span>
    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">args</span> <span class="o">=</span> <span class="n">args</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>  <span class="c1"># This would be automated by the Lightning Trainer.</span>

    <span class="c1"># 2. This can be fully removed as Lightning creates its own fitting loop,</span>
    <span class="c1"># and sets up the model, optimizer, dataloader, etc for you.</span>
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># setup everything</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">configure_optimizers</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
        <span class="n">dataloader</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">setup_dataloaders</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_dataloader</span><span class="p">())</span>

        <span class="c1"># start fitting</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
                <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_step</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
                <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># 3. This stays here as it belongs to the LightningModule.</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="o">...</span><span class="p">)</span>

    <span class="c1"># 4. [Optionally] This can stay here or be extracted to the LightningDataModule to enable higher composability.</span>
    <span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">MyDataset</span><span class="p">(</span><span class="o">...</span><span class="p">),</span> <span class="o">...</span><span class="p">)</span>


<span class="n">Lite</span><span class="p">(</span><span class="o">...</span><span class="p">)</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
</pre></div>
</div>
<p>Finally, change the <code class="xref py py-meth docutils literal notranslate"><span class="pre">run()</span></code> into a
<code class="xref py py-meth docutils literal notranslate"><span class="pre">__init__()</span></code> and drop the <code class="docutils literal notranslate"><span class="pre">fit</span></code> call from inside.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_lightning</span> <span class="kn">import</span> <span class="n">LightningDataModule</span><span class="p">,</span> <span class="n">LightningModule</span><span class="p">,</span> <span class="n">Trainer</span>


<span class="k">class</span> <span class="nc">LightningModel</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;train_loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">BoringDataModule</span><span class="p">(</span><span class="n">LightningDataModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">MyDataset</span><span class="p">(</span><span class="o">...</span><span class="p">),</span> <span class="o">...</span><span class="p">)</span>


<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">max_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">LightningModel</span><span class="p">(),</span> <span class="n">datamodule</span><span class="o">=</span><span class="n">BoringDataModule</span><span class="p">())</span>
</pre></div>
</div>
<p>You have successfully converted to PyTorch Lightning, and can now benefit from its hundred of features!</p>
</section>
</section>
<hr class="docutils" />
<section id="lightning-lite-flags">
<h2>Lightning Lite Flags<a class="headerlink" href="#lightning-lite-flags" title="Permalink to this headline">¶</a></h2>
<p>Lite is specialized in accelerated distributed training and inference. It offers you convenient ways to configure
your device and communication strategy and to switch seamlessly from one to the other. The terminology and usage are
identical to Lightning, which means minimum effort for you to convert when you decide to do so.</p>
<section id="accelerator">
<h3>accelerator<a class="headerlink" href="#accelerator" title="Permalink to this headline">¶</a></h3>
<p>Choose one of <code class="docutils literal notranslate"><span class="pre">&quot;cpu&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;gpu&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;tpu&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;auto&quot;</span></code> (IPU support is coming soon).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># CPU accelerator</span>
<span class="n">lite</span> <span class="o">=</span> <span class="n">Lite</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

<span class="c1"># Running with GPU Accelerator using 2 GPUs</span>
<span class="n">lite</span> <span class="o">=</span> <span class="n">Lite</span><span class="p">(</span><span class="n">devices</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">)</span>

<span class="c1"># Running with TPU Accelerator using 8 tpu cores</span>
<span class="n">lite</span> <span class="o">=</span> <span class="n">Lite</span><span class="p">(</span><span class="n">devices</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;tpu&quot;</span><span class="p">)</span>

<span class="c1"># Running with GPU Accelerator using the DistributedDataParallel strategy</span>
<span class="n">lite</span> <span class="o">=</span> <span class="n">Lite</span><span class="p">(</span><span class="n">devices</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;ddp&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">&quot;auto&quot;</span></code> option recognizes the machine you are on and selects the available accelerator.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># If your machine has GPUs, it will use the GPU Accelerator</span>
<span class="n">lite</span> <span class="o">=</span> <span class="n">Lite</span><span class="p">(</span><span class="n">devices</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="strategy">
<h3>strategy<a class="headerlink" href="#strategy" title="Permalink to this headline">¶</a></h3>
<p>Choose a training strategy: <code class="docutils literal notranslate"><span class="pre">&quot;dp&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;ddp&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;ddp_spawn&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;tpu_spawn&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;deepspeed&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;ddp_sharded&quot;</span></code>, or <code class="docutils literal notranslate"><span class="pre">&quot;ddp_sharded_spawn&quot;</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Running with the DistributedDataParallel strategy on 4 GPUs</span>
<span class="n">lite</span> <span class="o">=</span> <span class="n">Lite</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;ddp&quot;</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="c1"># Running with the DDP Spawn strategy using 4 cpu processes</span>
<span class="n">lite</span> <span class="o">=</span> <span class="n">Lite</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;ddp_spawn&quot;</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
<p>Additionally, you can pass in your custom strategy by configuring additional parameters.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_lightning.strategies</span> <span class="kn">import</span> <span class="n">DeepSpeedStrategy</span>

<span class="n">lite</span> <span class="o">=</span> <span class="n">Lite</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="n">DeepSpeedStrategy</span><span class="p">(</span><span class="n">stage</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>Support for Horovod and Fully Sharded training strategies are coming soon.</p>
</section>
<section id="devices">
<h3>devices<a class="headerlink" href="#devices" title="Permalink to this headline">¶</a></h3>
<p>Configure the devices to run on. Can be of type:</p>
<ul class="simple">
<li><p>int: the number of devices (e.g., GPUs) to train on</p></li>
<li><p>list of int: which device index (e.g., GPU ID) to train on (0-indexed)</p></li>
<li><p>str: a string representation of one of the above</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># default used by Lite, i.e., use the CPU</span>
<span class="n">lite</span> <span class="o">=</span> <span class="n">Lite</span><span class="p">(</span><span class="n">devices</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="c1"># equivalent</span>
<span class="n">lite</span> <span class="o">=</span> <span class="n">Lite</span><span class="p">(</span><span class="n">devices</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># int: run on two GPUs</span>
<span class="n">lite</span> <span class="o">=</span> <span class="n">Lite</span><span class="p">(</span><span class="n">devices</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">)</span>

<span class="c1"># list: run on GPUs 1, 4 (by bus ordering)</span>
<span class="n">lite</span> <span class="o">=</span> <span class="n">Lite</span><span class="p">(</span><span class="n">devices</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">)</span>
<span class="n">lite</span> <span class="o">=</span> <span class="n">Lite</span><span class="p">(</span><span class="n">devices</span><span class="o">=</span><span class="s2">&quot;1, 4&quot;</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">)</span>  <span class="c1"># equivalent</span>

<span class="c1"># -1: run on all GPUs</span>
<span class="n">lite</span> <span class="o">=</span> <span class="n">Lite</span><span class="p">(</span><span class="n">devices</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">)</span>
<span class="n">lite</span> <span class="o">=</span> <span class="n">Lite</span><span class="p">(</span><span class="n">devices</span><span class="o">=</span><span class="s2">&quot;-1&quot;</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">)</span>  <span class="c1"># equivalent</span>
</pre></div>
</div>
</section>
<section id="gpus">
<h3>gpus<a class="headerlink" href="#gpus" title="Permalink to this headline">¶</a></h3>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><code class="docutils literal notranslate"><span class="pre">gpus=x</span></code> has been deprecated in v1.7 and will be removed in v2.0.
Please use <code class="docutils literal notranslate"><span class="pre">accelerator='gpu'</span></code> and <code class="docutils literal notranslate"><span class="pre">devices=x</span></code> instead.</p>
</div>
<p>Shorthand for setting <code class="docutils literal notranslate"><span class="pre">devices=X</span></code> and <code class="docutils literal notranslate"><span class="pre">accelerator=&quot;gpu&quot;</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run on two GPUs</span>
<span class="n">lite</span> <span class="o">=</span> <span class="n">Lite</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Equivalent</span>
<span class="n">lite</span> <span class="o">=</span> <span class="n">Lite</span><span class="p">(</span><span class="n">devices</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="tpu-cores">
<h3>tpu_cores<a class="headerlink" href="#tpu-cores" title="Permalink to this headline">¶</a></h3>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><code class="docutils literal notranslate"><span class="pre">tpu_cores=x</span></code> has been deprecated in v1.7 and will be removed in v2.0.
Please use <code class="docutils literal notranslate"><span class="pre">accelerator='tpu'</span></code> and <code class="docutils literal notranslate"><span class="pre">devices=x</span></code> instead.</p>
</div>
<p>Shorthand for <code class="docutils literal notranslate"><span class="pre">devices=X</span></code> and <code class="docutils literal notranslate"><span class="pre">accelerator=&quot;tpu&quot;</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run on eight TPUs</span>
<span class="n">lite</span> <span class="o">=</span> <span class="n">Lite</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;tpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>

<span class="c1"># Equivalent</span>
<span class="n">lite</span> <span class="o">=</span> <span class="n">Lite</span><span class="p">(</span><span class="n">devices</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;tpu&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="num-nodes">
<h3>num_nodes<a class="headerlink" href="#num-nodes" title="Permalink to this headline">¶</a></h3>
<p>Number of cluster nodes for distributed operation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Default used by Lite</span>
<span class="n">lite</span> <span class="o">=</span> <span class="n">Lite</span><span class="p">(</span><span class="n">num_nodes</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Run on 8 nodes</span>
<span class="n">lite</span> <span class="o">=</span> <span class="n">Lite</span><span class="p">(</span><span class="n">num_nodes</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
</pre></div>
</div>
<p>Learn more about distributed multi-node training on clusters <a class="reference internal" href="../clouds/cluster.html"><span class="doc">here</span></a>.</p>
</section>
<section id="precision">
<h3>precision<a class="headerlink" href="#precision" title="Permalink to this headline">¶</a></h3>
<p>Lightning Lite supports double precision (64), full precision (32), or half precision (16) operation (including <a class="reference external" href="https://pytorch.org/docs/1.10.0/generated/torch.Tensor.bfloat16.html">bfloat16</a>).
Half precision, or mixed precision, is the combined use of 32 and 16-bit floating points to reduce the memory footprint during model training.
This can result in improved performance, achieving significant speedups on modern GPUs.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Default used by the Lite</span>
<span class="n">lite</span> <span class="o">=</span> <span class="n">Lite</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># 16-bit (mixed) precision</span>
<span class="n">lite</span> <span class="o">=</span> <span class="n">Lite</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># 16-bit bfloat precision</span>
<span class="n">lite</span> <span class="o">=</span> <span class="n">Lite</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="s2">&quot;bf16&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># 64-bit (double) precision</span>
<span class="n">lite</span> <span class="o">=</span> <span class="n">Lite</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="plugins">
<h3>plugins<a class="headerlink" href="#plugins" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="../extensions/plugins.html#plugins"><span class="std std-ref">Plugins</span></a> allow you to connect arbitrary backends, precision libraries, clusters etc. For example:
To define your own behavior, subclass the relevant class and pass it in. Here’s an example linking up your own
<code class="xref py py-class docutils literal notranslate"><span class="pre">ClusterEnvironment</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_lightning.plugins.environments</span> <span class="kn">import</span> <span class="n">ClusterEnvironment</span>


<span class="k">class</span> <span class="nc">MyCluster</span><span class="p">(</span><span class="n">ClusterEnvironment</span><span class="p">):</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">main_address</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">your_main_address</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">main_port</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">your_main_port</span>

    <span class="k">def</span> <span class="nf">world_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">the_world_size</span>


<span class="n">lite</span> <span class="o">=</span> <span class="n">Lite</span><span class="p">(</span><span class="n">plugins</span><span class="o">=</span><span class="p">[</span><span class="n">MyCluster</span><span class="p">()],</span> <span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="lightning-lite-methods">
<h2>Lightning Lite Methods<a class="headerlink" href="#lightning-lite-methods" title="Permalink to this headline">¶</a></h2>
<section id="run">
<h3>run<a class="headerlink" href="#run" title="Permalink to this headline">¶</a></h3>
<p>The run method serves two purposes:</p>
<ol class="arabic simple">
<li><p>Override this method from the <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningLite</span></code> class and put your
training (or inference) code inside.</p></li>
<li><p>Launch the training procedure by calling the run method. Lite will take care of setting up the distributed backend.</p></li>
</ol>
<p>You can optionally pass arguments to the run method. For example, the hyperparameters or a backbone for the model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_lightning.lite</span> <span class="kn">import</span> <span class="n">LightningLite</span>


<span class="k">class</span> <span class="nc">Lite</span><span class="p">(</span><span class="n">LightningLite</span><span class="p">):</span>

    <span class="c1"># Input arguments are optional; put whatever you need</span>
    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Here goes your training loop&quot;&quot;&quot;</span>


<span class="n">lite</span> <span class="o">=</span> <span class="n">Lite</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">lite</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="setup">
<h3>setup<a class="headerlink" href="#setup" title="Permalink to this headline">¶</a></h3>
<p>Set up a model and corresponding optimizer(s). If you need to set up multiple models, call <code class="docutils literal notranslate"><span class="pre">setup()</span></code> on each of them.
Moves the model and optimizer to the correct device automatically.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="c1"># Set up model and optimizer for accelerated training</span>
<span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>

<span class="c1"># If you don&#39;t want Lite to set the device</span>
<span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">move_to_device</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>The setup method also prepares the model for the selected precision choice so that operations during <code class="docutils literal notranslate"><span class="pre">forward()</span></code> get
cast automatically.</p>
</section>
<section id="setup-dataloaders">
<h3>setup_dataloaders<a class="headerlink" href="#setup-dataloaders" title="Permalink to this headline">¶</a></h3>
<p>Set up one or multiple dataloaders for accelerated operation. If you are running a distributed strategy (e.g., DDP), Lite
replaces the sampler automatically for you. In addition, the dataloader will be configured to move the returned
data tensors to the correct device automatically.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">train_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>

<span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">setup_dataloaders</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span><span class="p">)</span>

<span class="c1"># If you don&#39;t want Lite to move the data to the device</span>
<span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">setup_dataloaders</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span><span class="p">,</span> <span class="n">move_to_device</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># If you don&#39;t want Lite to replace the sampler in the context of distributed training</span>
<span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">setup_dataloaders</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span><span class="p">,</span> <span class="n">replace_sampler</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="backward">
<h3>backward<a class="headerlink" href="#backward" title="Permalink to this headline">¶</a></h3>
<p>This replaces any occurrences of <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code> and makes your code accelerator and precision agnostic.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

<span class="c1"># loss.backward()</span>
<span class="bp">self</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="to-device">
<h3>to_device<a class="headerlink" href="#to-device" title="Permalink to this headline">¶</a></h3>
<p>Use <code class="xref py py-meth docutils literal notranslate"><span class="pre">to_device()</span></code> to move models, tensors or collections of tensors to
the current device. By default <code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code> and
<code class="xref py py-meth docutils literal notranslate"><span class="pre">setup_dataloaders()</span></code> already move the model and data to the correct
device, so calling this method is only necessary for manual operation when needed.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;dataset.pt&quot;</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_device</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="seed-everything">
<h3>seed_everything<a class="headerlink" href="#seed-everything" title="Permalink to this headline">¶</a></h3>
<p>Make your code reproducible by calling this method at the beginning of your run.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Instead of `torch.manual_seed(...)`, call:</span>
<span class="bp">self</span><span class="o">.</span><span class="n">seed_everything</span><span class="p">(</span><span class="mi">1234</span><span class="p">)</span>
</pre></div>
</div>
<p>This covers PyTorch, NumPy and Python random number generators. In addition, Lite takes care of properly initializing
the seed of dataloader worker processes (can be turned off by passing <code class="docutils literal notranslate"><span class="pre">workers=False</span></code>).</p>
</section>
<section id="autocast">
<h3>autocast<a class="headerlink" href="#autocast" title="Permalink to this headline">¶</a></h3>
<p>Let the precision backend autocast the block of code under this context manager. This is optional and already done by
Lite for the model’s forward method (once the model was <code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code>).
You need this only if you wish to autocast more operations outside the ones in model forward:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>

<span class="c1"># Lite handles precision automatically for the model</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

<span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">autocast</span><span class="p">():</span>  <span class="c1"># optional</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

<span class="bp">self</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
<span class="o">...</span>
</pre></div>
</div>
</section>
<section id="print">
<h3>print<a class="headerlink" href="#print" title="Permalink to this headline">¶</a></h3>
<p>Print to the console via the built-in print function, but only on the main process.
This avoids excessive printing and logs when running on multiple devices/nodes.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Print only on the main process</span>
<span class="bp">self</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">num_epochs</span><span class="si">}</span><span class="s2">| Train Epoch Loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="save">
<h3>save<a class="headerlink" href="#save" title="Permalink to this headline">¶</a></h3>
<p>Save contents to a checkpoint. Replaces all occurrences of <code class="docutils literal notranslate"><span class="pre">torch.save(...)</span></code> in your code. Lite will take care of
handling the saving part correctly, no matter if you are running a single device, multi-devices or multi-nodes.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Instead of `torch.save(...)`, call:</span>
<span class="bp">self</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s2">&quot;path/to/checkpoint.ckpt&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="load">
<h3>load<a class="headerlink" href="#load" title="Permalink to this headline">¶</a></h3>
<p>Load checkpoint contents from a file. Replaces all occurrences of <code class="docutils literal notranslate"><span class="pre">torch.load(...)</span></code> in your code. Lite will take care of
handling the loading part correctly, no matter if you are running a single device, multi-device, or multi-node.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Instead of `torch.load(...)`, call:</span>
<span class="bp">self</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;path/to/checkpoint.ckpt&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="barrier">
<h3>barrier<a class="headerlink" href="#barrier" title="Permalink to this headline">¶</a></h3>
<p>Call this if you want all processes to wait and synchronize. Once all processes have entered this call,
execution continues. Useful for example when you want to download data on one process and make all others wait until
the data is written to disk.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Download data only on one process</span>
<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">download_data</span><span class="p">(</span><span class="s2">&quot;http://...&quot;</span><span class="p">)</span>

<span class="c1"># Wait until all processes meet up here</span>
<span class="bp">self</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>

<span class="c1"># All processes are allowed to read the data now</span>
</pre></div>
</div>
</section>
</section>
</section>


             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright Copyright (c) 2018-2022, William Falcon et al...

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">LightningLite (Stepping Stone to Lightning)</a><ul>
<li><a class="reference internal" href="#learn-by-example">Learn by example</a><ul>
<li><a class="reference internal" href="#my-existing-pytorch-code">My Existing PyTorch Code</a></li>
<li><a class="reference internal" href="#convert-to-lightninglite">Convert to LightningLite</a></li>
<li><a class="reference internal" href="#distributed-training-pitfalls">Distributed Training Pitfalls</a></li>
<li><a class="reference internal" href="#convert-to-lightning">Convert to Lightning</a></li>
</ul>
</li>
<li><a class="reference internal" href="#lightning-lite-flags">Lightning Lite Flags</a><ul>
<li><a class="reference internal" href="#accelerator">accelerator</a></li>
<li><a class="reference internal" href="#strategy">strategy</a></li>
<li><a class="reference internal" href="#devices">devices</a></li>
<li><a class="reference internal" href="#gpus">gpus</a></li>
<li><a class="reference internal" href="#tpu-cores">tpu_cores</a></li>
<li><a class="reference internal" href="#num-nodes">num_nodes</a></li>
<li><a class="reference internal" href="#precision">precision</a></li>
<li><a class="reference internal" href="#plugins">plugins</a></li>
</ul>
</li>
<li><a class="reference internal" href="#lightning-lite-methods">Lightning Lite Methods</a><ul>
<li><a class="reference internal" href="#run">run</a></li>
<li><a class="reference internal" href="#setup">setup</a></li>
<li><a class="reference internal" href="#setup-dataloaders">setup_dataloaders</a></li>
<li><a class="reference internal" href="#backward">backward</a></li>
<li><a class="reference internal" href="#to-device">to_device</a></li>
<li><a class="reference internal" href="#seed-everything">seed_everything</a></li>
<li><a class="reference internal" href="#autocast">autocast</a></li>
<li><a class="reference internal" href="#print">print</a></li>
<li><a class="reference internal" href="#save">save</a></li>
<li><a class="reference internal" href="#load">load</a></li>
<li><a class="reference internal" href="#barrier">barrier</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
         <script src="../_static/jquery.js"></script>
         <script src="../_static/underscore.js"></script>
         <script src="../_static/doctools.js"></script>
         <script src="../_static/clipboard.min.js"></script>
         <script src="../_static/copybutton.js"></script>
         <script src="../_static/copybutton.js"></script>
         <script>let toggleHintShow = 'Click to show';</script>
         <script>let toggleHintHide = 'Click to hide';</script>
         <script>let toggleOpenOnPrint = 'true';</script>
         <script src="../_static/togglebutton.js"></script>
         <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Best practices', 'Optional Extensions', 'Tutorials', 'API References', 'Bolts', 'Examples', 'Partner Domain Frameworks', 'Community'];
</script>



  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <!--
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch-lightning.rtfd.io/en/latest">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch-lightning.readthedocs.io/en/latest/#tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch-lightning.readthedocs.io/en/latest/#community-examples">View Resources</a>
        </div>
        -->
      </div>
    </div>
  </div>

  <!--
  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch-lightning.rtfd.io/en/latest/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch-lightning.rtfd.io/en/latest/">PyTorch</a></li>
            <li><a href="https://pytorch-lightning.readthedocs.io/en/latest/starter/introduction.html">Get Started</a></li>
            <li><a href="https://pytorch-lightning.rtfd.io/en/latest/">Features</a></li>
            <li><a href="">Ecosystem</a></li>
            <li><a href="https://www.pytorchlightning.ai/blog">Blog</a></li>
            <li><a href="https://github.com/PyTorchLightning/pytorch-lightning/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch-lightning.readthedocs.io/en/latest/#community-examples">Resources</a></li>
            <li><a href="https://pytorch-lightning.readthedocs.io/en/latest/#tutorials">Tutorials</a></li>
            <li><a href="https://pytorch-lightning.rtfd.io/en/latest">Docs</a></li>
            <li><a href="https://www.pytorchlightning.ai/community" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/PyTorchLightning/pytorch-lightning/issues" target="_blank">Github Issues</a></li>
            <li><a href="" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Stay Connected</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/PyTorchLightnin" target="_blank" class="twitter"></a>
            <a href="" target="_blank" class="youtube"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>
  -->

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. Read PyTorch Lightning's <a href="https://pytorchlightning.ai/privacy-policy">Privacy Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch-lightning.rtfd.io/en/latest/" aria-label="PyTorch Lightning"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <!-- <li>
            <a href="https://pytorch-lightning.readthedocs.io/en/latest/starter/introduction.html">Get Started</a>
          </li>

          <li>
            <a href="https://www.pytorchlightning.ai/blog">Blog</a>
          </li>

          <li class="resources-mobile-menu-title">
            Ecosystem
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch-lightning.readthedocs.io/en/stable/">PyTorch Lightning</a>
            </li>

            <li>
              <a href="https://torchmetrics.readthedocs.io/en/stable/">TorchMetrics</a>
            </li>

            <li>
              <a href="https://lightning-flash.readthedocs.io/en/stable/">Lightning Flash</a>
            </li>

            <li>
              <a href="https://lightning-transformers.readthedocs.io/en/stable/">Lightning Transformers</a>
            </li>

            <li>
              <a href="https://lightning-bolts.readthedocs.io/en/stable/">Lightning Bolts</a>
            </li>
          </ul> -->

          <!--<li class="resources-mobile-menu-title">
            Resources
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch-lightning.readthedocs.io/en/latest/#community-examples">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch-lightning.rtfd.io/en/latest/">About</a>
            </li>

            <li>
              <a href="">Models (Beta)</a>
            </li>

            <li>
              <a href="https://www.pytorchlightning.ai/community">Community</a>
            </li>

            <li>
              <a href="https://github.com/PyTorchLightning/pytorch-lightning/discussions">Forums</a>
            </li>
          </ul>-->

          <!-- <li>
            <a href="https://github.com/PyTorchLightning/pytorch-lightning">Github</a>
          </li> -->

          <!-- <li>
            <a href="https://www.grid.ai/">Grid.ai</a>
          </li> -->
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>

 </body>
</html>