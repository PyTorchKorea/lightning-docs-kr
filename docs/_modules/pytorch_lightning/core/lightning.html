


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-PQBQ3CV');
  </script>
  <!-- End Google Tag Manager -->

  <meta charset="utf-8">
  
  <meta name="google-site-verification" content="okUst94cAlWSsUsGZTB4xSS4UKTtRV8Nu5XZ9pdd3Aw" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>pytorch_lightning.core.lightning &mdash; PyTorch Lightning 1.7.0dev documentation</title>
  

  
  
    <link rel="shortcut icon" href="../../../_static/icon.svg"/>
  
  
  
    <link rel="canonical" href="https://pytorch-lightning.readthedocs.io/en/stable//_modules/pytorch_lightning/core/lightning.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/togglebutton.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/main.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sphinx_paramlinks.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
  <!-- Google Analytics -->
  
  <!-- End Google Analytics -->
  

  
  <script src="../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/UCity/UCity-Light.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/UCity/UCity-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/UCity/UCity-Semibold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/Inconsolata/Inconsolata.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <script defer src="https://use.fontawesome.com/releases/v6.1.1/js/all.js" integrity="sha384-xBXmu0dk1bEoiwd71wOonQLyH+VpgR1XcDH3rtxrLww5ajNTuMvBdL5SOiFZnNdp" crossorigin="anonymous"></script>
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch-lightning.rtfd.io/en/latest/" aria-label="PyTorch Lightning">
      <!--  <img class="call-to-action-img" src="../../../_static/images/logo-lightning-icon.png"/> -->
      </a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch-lightning.readthedocs.io/en/latest/starter/introduction.html">Get Started</a>
          </li>

          <li>
            <a href="https://www.pytorchlightning.ai/blog">Blog</a>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch-lightning.readthedocs.io/en/stable/">
                  <span class="dropdown-title">PyTorch Lightning</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://torchmetrics.readthedocs.io/en/stable/">
                  <span class="dropdown-title">TorchMetrics</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning-flash.readthedocs.io/en/stable/">
                  <span class="dropdown-title">Lightning Flash</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning-transformers.readthedocs.io/en/stable/">
                  <span class="dropdown-title">Lightning Transformers</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning-bolts.readthedocs.io/en/stable/">
                  <span class="dropdown-title">Lightning Bolts</span>
                </a>
            </div>
          </li>

          <!--<li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://www.pytorchlightning.ai/community">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch-lightning.readthedocs.io/en/latest/#community-examples">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://github.com/PyTorchLightning/pytorch-lightning/discussions" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>-->

          <li>
            <a href="https://github.com/PyTorchLightning/pytorch-lightning">GitHub</a>
          </li>

          <li>
            <a href="https://www.grid.ai/">Train on the cloud</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  1.7.0dev
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../starter/introduction.html">Lightning in 15 minutes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../starter/converting.html">Organize existing PyTorch into Lightning</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Level Up</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../levels/core_skills.html">Basic skills</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../levels/intermediate.html">Intermediate skills</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../levels/advanced.html">Advanced skills</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../levels/expert.html">Expert skills</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Core API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../common/lightning_module.html">LightningModule</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../common/trainer.html">Trainer</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Common Workflows</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../common/evaluation.html">Avoid overfitting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../model/build_model.html">Build a Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../common/hyperparameters.html">Configure hyperparameters from the CLI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../common/progress_bar.html">Customize the progress bar</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../deploy/production.html">Deploy models into production</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../advanced/training_tricks.html">Effective Training Techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cli/lightning_cli.html">Eliminate config boilerplate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tuning/profiler.html">Find bottlenecks in your code</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../advanced/transfer_learning.html">Finetune a model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../visualize/logging_intermediate.html">Manage experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../clouds/cluster.html">Run on an on-prem cluster</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../advanced/model_parallel.html">Train 1 trillion+ parameter models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../clouds/cloud_training.html">Train on the cloud</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../common/checkpointing.html">Save and load model progress</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../common/precision.html">Save memory with half-precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../accelerators/gpu.html">Train on single or multiple GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../accelerators/hpu.html">Train on single or multiple HPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../accelerators/ipu.html">Train on single or multiple IPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../accelerators/tpu.html">Train on single or multiple TPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../model/own_your_loop.html">Use a pure PyTorch training loop</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Glossary</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../extensions/accelerator.html">Accelerators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../extensions/callbacks.html">Callback</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../common/checkpointing.html">Checkpointing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../clouds/cluster.html">Cluster</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../common/checkpointing_advanced.html">Cloud checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../common/console_logs.html">Console Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../debug/debugging.html">Debugging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../common/early_stopping.html">Early stopping</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../visualize/experiment_managers.html">Experiment manager (Logger)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../clouds/fault_tolerant_training.html">Fault tolerant training</a></li>
<li class="toctree-l1"><a class="reference external" href="https://lightning-flash.readthedocs.io/en/stable/">Flash</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../clouds/cloud_training.html">Grid AI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../accelerators/gpu.html">GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../common/precision.html">Half precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../accelerators/hpu.html">HPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../deploy/production_intermediate.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../accelerators/ipu.html">IPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cli/lightning_cli.html">Lightning CLI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../model/build_model_expert.html">Raw PyTorch loop (expert)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../model/build_model_expert.html#lightninglite-stepping-stone-to-lightning">LightningLite (Stepping Stone to Lightning)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../data/datamodule.html">LightningDataModule</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../common/lightning_module.html">LightningModule</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch-lightning.readthedocs.io/en/stable/ecosystem/transformers.html">Lightning Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../visualize/loggers.html">Log</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../extensions/loops.html">Loops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../accelerators/tpu.html">TPU</a></li>
<li class="toctree-l1"><a class="reference external" href="https://torchmetrics.readthedocs.io/en/stable/">Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../model/build_model.html">Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../advanced/model_parallel.html">Model Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../extensions/plugins.html">Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../common/progress_bar.html">Progress bar</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../deploy/production_advanced.html">Production</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../deploy/production_basic.html">Predict</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tuning/profiler.html">Profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../advanced/pruning_quantization.html">Pruning and Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../common/remote_fs.html">Remote filesystem and FSSPEC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../advanced/strategy_registry.html">Strategy registry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../starter/style_guide.html">Style guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../clouds/run_intermediate.html">Sweep</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../advanced/training_tricks.html">SWA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../clouds/cluster_advanced.html">SLURM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../advanced/transfer_learning.html">Transfer learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../common/trainer.html">Trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../clouds/cluster_intermediate_2.html">Torch distributed</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Hands-on Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.youtube.com/playlist?list=PLaMu-SDt_RB5NUm67hU2pdE75j6KaIOv2">PyTorch Lightning 101 class</a></li>
<li class="toctree-l1"><a class="reference external" href="https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09">From PyTorch to PyTorch Lightning [Blog]</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.youtube.com/watch?v=QHww1JH7IDU">From PyTorch to PyTorch Lightning [Video]</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
      <li>pytorch_lightning.core.lightning</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for pytorch_lightning.core.lightning</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright The PyTorch Lightning team.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="sd">&quot;&quot;&quot;The LightningModule - an nn.Module with many additional features.&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">inspect</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">numbers</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">tempfile</span>
<span class="kn">from</span> <span class="nn">contextlib</span> <span class="kn">import</span> <span class="n">contextmanager</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Mapping</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">overload</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">ScriptModule</span><span class="p">,</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">Module</span>
<span class="kn">from</span> <span class="nn">torch.optim.optimizer</span> <span class="kn">import</span> <span class="n">Optimizer</span>
<span class="kn">from</span> <span class="nn">torchmetrics</span> <span class="kn">import</span> <span class="n">Metric</span>
<span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">Literal</span>

<span class="kn">import</span> <span class="nn">pytorch_lightning</span> <span class="k">as</span> <span class="nn">pl</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.callbacks.base</span> <span class="kn">import</span> <span class="n">Callback</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.callbacks.progress</span> <span class="kn">import</span> <span class="n">base</span> <span class="k">as</span> <span class="n">progress_base</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.core.hooks</span> <span class="kn">import</span> <span class="n">CheckpointHooks</span><span class="p">,</span> <span class="n">DataHooks</span><span class="p">,</span> <span class="n">ModelHooks</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.core.mixins</span> <span class="kn">import</span> <span class="n">DeviceDtypeModuleMixin</span><span class="p">,</span> <span class="n">HyperparametersMixin</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.core.optimizer</span> <span class="kn">import</span> <span class="n">LightningOptimizer</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.core.saving</span> <span class="kn">import</span> <span class="n">ModelIO</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.loggers</span> <span class="kn">import</span> <span class="n">LightningLoggerBase</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.trainer.connectors.data_connector</span> <span class="kn">import</span> <span class="n">_DataHookSelector</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.trainer.connectors.logger_connector.fx_validator</span> <span class="kn">import</span> <span class="n">_FxValidator</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.utilities</span> <span class="kn">import</span> <span class="n">_IS_WINDOWS</span><span class="p">,</span> <span class="n">_TORCH_GREATER_EQUAL_1_10</span><span class="p">,</span> <span class="n">GradClipAlgorithmType</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.utilities.apply_func</span> <span class="kn">import</span> <span class="n">apply_to_collection</span><span class="p">,</span> <span class="n">convert_to_tensors</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.utilities.cloud_io</span> <span class="kn">import</span> <span class="n">get_filesystem</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.utilities.distributed</span> <span class="kn">import</span> <span class="n">distributed_available</span><span class="p">,</span> <span class="n">sync_ddp</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.utilities.exceptions</span> <span class="kn">import</span> <span class="n">MisconfigurationException</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.utilities.memory</span> <span class="kn">import</span> <span class="n">get_model_size_mb</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.utilities.model_summary</span> <span class="kn">import</span> <span class="n">ModelSummary</span><span class="p">,</span> <span class="n">summarize</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.utilities.parsing</span> <span class="kn">import</span> <span class="n">collect_init_args</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.utilities.rank_zero</span> <span class="kn">import</span> <span class="n">rank_zero_debug</span><span class="p">,</span> <span class="n">rank_zero_deprecation</span><span class="p">,</span> <span class="n">rank_zero_warn</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.utilities.signature_utils</span> <span class="kn">import</span> <span class="n">is_param_in_hook_signature</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.utilities.types</span> <span class="kn">import</span> <span class="n">_METRIC_COLLECTION</span><span class="p">,</span> <span class="n">EPOCH_OUTPUT</span><span class="p">,</span> <span class="n">LRSchedulerTypeUnion</span><span class="p">,</span> <span class="n">STEP_OUTPUT</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.utilities.warnings</span> <span class="kn">import</span> <span class="n">WarningCache</span>

<span class="n">warning_cache</span> <span class="o">=</span> <span class="n">WarningCache</span><span class="p">()</span>
<span class="n">log</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">LightningModule</span><span class="p">(</span>
    <span class="n">DeviceDtypeModuleMixin</span><span class="p">,</span>
    <span class="n">HyperparametersMixin</span><span class="p">,</span>
    <span class="n">ModelIO</span><span class="p">,</span>
    <span class="n">ModelHooks</span><span class="p">,</span>
    <span class="n">DataHooks</span><span class="p">,</span>
    <span class="n">CheckpointHooks</span><span class="p">,</span>
    <span class="n">Module</span><span class="p">,</span>
<span class="p">):</span>
    <span class="c1"># Below is for property support of JIT</span>
    <span class="c1"># since none of these are important when using JIT, we are going to ignore them.</span>
    <span class="n">__jit_unused_properties__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="p">[</span>
            <span class="s2">&quot;example_input_array&quot;</span><span class="p">,</span>
            <span class="s2">&quot;on_gpu&quot;</span><span class="p">,</span>
            <span class="s2">&quot;current_epoch&quot;</span><span class="p">,</span>
            <span class="s2">&quot;global_step&quot;</span><span class="p">,</span>
            <span class="s2">&quot;global_rank&quot;</span><span class="p">,</span>
            <span class="s2">&quot;local_rank&quot;</span><span class="p">,</span>
            <span class="s2">&quot;logger&quot;</span><span class="p">,</span>
            <span class="s2">&quot;loggers&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model_size&quot;</span><span class="p">,</span>
            <span class="s2">&quot;automatic_optimization&quot;</span><span class="p">,</span>
            <span class="s2">&quot;truncated_bptt_steps&quot;</span><span class="p">,</span>
            <span class="s2">&quot;use_amp&quot;</span><span class="p">,</span>
        <span class="p">]</span>
        <span class="o">+</span> <span class="n">DeviceDtypeModuleMixin</span><span class="o">.</span><span class="n">__jit_unused_properties__</span>
        <span class="o">+</span> <span class="n">HyperparametersMixin</span><span class="o">.</span><span class="n">__jit_unused_properties__</span>
    <span class="p">)</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># see (https://github.com/pytorch/pytorch/blob/3e6bb5233f9ca2c5aa55d9cda22a7ee85439aa6e/</span>
        <span class="c1"># torch/nn/modules/module.py#L227)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;lightning.module.</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># pointer to the trainer object</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;pl.Trainer&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_use_amp</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># the precision used</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">precision</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span>

        <span class="c1"># optionally can be set by user</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_example_input_array</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_current_fx_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_automatic_optimization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_truncated_bptt_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_param_requires_grad_state</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_metric_attributes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_should_prevent_trainer_and_dataloaders_deepcopy</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="c1"># TODO: remove in 1.8</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_running_torchscript</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_register_sharded_tensor_state_dict_hooks_if_available</span><span class="p">()</span>

    <span class="nd">@overload</span>
    <span class="k">def</span> <span class="nf">optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_pl_optimizer</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="kc">True</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">LightningOptimizer</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">LightningOptimizer</span><span class="p">]]:</span>
        <span class="o">...</span>

    <span class="nd">@overload</span>
    <span class="k">def</span> <span class="nf">optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_pl_optimizer</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="kc">False</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Optimizer</span><span class="p">]]:</span>
        <span class="o">...</span>

    <span class="nd">@overload</span>
    <span class="k">def</span> <span class="nf">optimizers</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">use_pl_optimizer</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">LightningOptimizer</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Optimizer</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">LightningOptimizer</span><span class="p">]]:</span>
        <span class="o">...</span>

<div class="viewcode-block" id="LightningModule.optimizers"><a class="viewcode-back" href="../../../common/lightning_module.html#pytorch_lightning.core.lightning.LightningModule.optimizers">[docs]</a>    <span class="k">def</span> <span class="nf">optimizers</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">use_pl_optimizer</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">LightningOptimizer</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Optimizer</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">LightningOptimizer</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;Returns the optimizer(s) that are being used during training. Useful for manual optimization.</span>

<span class="sd">        Args:</span>
<span class="sd">            use_pl_optimizer: If ``True``, will wrap the optimizer(s) in a</span>
<span class="sd">                :class:`~pytorch_lightning.core.optimizer.LightningOptimizer` for automatic handling of precision and</span>
<span class="sd">                profiling.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A single optimizer, or a list of optimizers in case multiple ones are present.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">use_pl_optimizer</span><span class="p">:</span>
            <span class="n">opts</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">strategy</span><span class="o">.</span><span class="n">_lightning_optimizers</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">opts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">optimizers</span>

        <span class="c1"># single optimizer</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">opts</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">opts</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">opts</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">LightningOptimizer</span><span class="p">)):</span>
            <span class="k">return</span> <span class="n">opts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c1"># multiple opts</span>
        <span class="k">return</span> <span class="n">opts</span></div>

<div class="viewcode-block" id="LightningModule.lr_schedulers"><a class="viewcode-back" href="../../../common/lightning_module.html#pytorch_lightning.core.lightning.LightningModule.lr_schedulers">[docs]</a>    <span class="k">def</span> <span class="nf">lr_schedulers</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">LRSchedulerTypeUnion</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">LRSchedulerTypeUnion</span><span class="p">]]]:</span>
        <span class="sd">&quot;&quot;&quot;Returns the learning rate scheduler(s) that are being used during training. Useful for manual</span>
<span class="sd">        optimization.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A single scheduler, or a list of schedulers in case multiple ones are present, or ``None`` if no</span>
<span class="sd">            schedulers were returned in :meth:`configure_optimizers`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">lr_scheduler_configs</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>

        <span class="c1"># ignore other keys &quot;interval&quot;, &quot;frequency&quot;, etc.</span>
        <span class="n">lr_schedulers</span> <span class="o">=</span> <span class="p">[</span><span class="n">config</span><span class="o">.</span><span class="n">scheduler</span> <span class="k">for</span> <span class="n">config</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">lr_scheduler_configs</span><span class="p">]</span>

        <span class="c1"># single scheduler</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">lr_schedulers</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">lr_schedulers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># multiple schedulers</span>
        <span class="k">return</span> <span class="n">lr_schedulers</span></div>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">example_input_array</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;The example input array is a specification of what the module can consume in the :meth:`forward` method.</span>
<span class="sd">        The return type is interpreted as follows:</span>

<span class="sd">        -   Single tensor: It is assumed the model takes a single argument, i.e.,</span>
<span class="sd">            ``model.forward(model.example_input_array)``</span>
<span class="sd">        -   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,</span>
<span class="sd">            ``model.forward(*model.example_input_array)``</span>
<span class="sd">        -   Dict: The input array represents named keyword arguments, i.e.,</span>
<span class="sd">            ``model.forward(**model.example_input_array)``</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_example_input_array</span>

    <span class="nd">@example_input_array</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">example_input_array</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">example</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_example_input_array</span> <span class="o">=</span> <span class="n">example</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">current_epoch</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;The current epoch in the ``Trainer``, or 0 if not attached.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">current_epoch</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span> <span class="k">else</span> <span class="mi">0</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">global_step</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Total training batches seen across all epochs.</span>

<span class="sd">        If no Trainer is attached, this propery is 0.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span> <span class="k">else</span> <span class="mi">0</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">global_rank</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;The index of the current process across all nodes and devices.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_rank</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span> <span class="k">else</span> <span class="mi">0</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">local_rank</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;The index of the current process within a single node.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">local_rank</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span> <span class="k">else</span> <span class="mi">0</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">on_gpu</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Returns ``True`` if this model is currently located on a GPU.</span>

<span class="sd">        Useful to set flags around the LightningModule for different CPU vs GPU behavior.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">automatic_optimization</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_automatic_optimization</span>

    <span class="nd">@automatic_optimization</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">automatic_optimization</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">automatic_optimization</span><span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_automatic_optimization</span> <span class="o">=</span> <span class="n">automatic_optimization</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">truncated_bptt_steps</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Enables `Truncated Backpropagation Through Time` in the Trainer when set to a positive integer.</span>

<span class="sd">        It represents</span>
<span class="sd">        the number of times :meth:`training_step` gets called before backpropagation. If this is &gt; 0, the</span>
<span class="sd">        :meth:`training_step` receives an additional argument ``hiddens`` and is expected to return a hidden state.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_truncated_bptt_steps</span>

    <span class="nd">@truncated_bptt_steps</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">truncated_bptt_steps</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">truncated_bptt_steps</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_truncated_bptt_steps</span> <span class="o">=</span> <span class="n">truncated_bptt_steps</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">logger</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LightningLoggerBase</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Reference to the logger object in the Trainer.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">logger</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">loggers</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">LightningLoggerBase</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Reference to the list of loggers in the Trainer.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">loggers</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span> <span class="k">else</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">_apply_batch_transfer_handler</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">device</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
        <span class="n">datahook_selector</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">_DataHookSelector</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">_data_connector</span><span class="o">.</span><span class="n">_datahook_selector</span>
        <span class="p">)</span>

        <span class="n">hook</span> <span class="o">=</span> <span class="n">datahook_selector</span><span class="o">.</span><span class="n">get_hook</span><span class="p">(</span><span class="s2">&quot;on_before_batch_transfer&quot;</span><span class="p">)</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">hook</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">)</span>
        <span class="n">hook</span> <span class="o">=</span> <span class="n">datahook_selector</span><span class="o">.</span><span class="n">get_hook</span><span class="p">(</span><span class="s2">&quot;transfer_batch_to_device&quot;</span><span class="p">)</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">hook</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">)</span>
        <span class="n">hook</span> <span class="o">=</span> <span class="n">datahook_selector</span><span class="o">.</span><span class="n">get_hook</span><span class="p">(</span><span class="s2">&quot;on_after_batch_transfer&quot;</span><span class="p">)</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">hook</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">batch</span>

<div class="viewcode-block" id="LightningModule.print"><a class="viewcode-back" href="../../../common/lightning_module.html#pytorch_lightning.core.lightning.LightningModule.print">[docs]</a>    <span class="k">def</span> <span class="nf">print</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Prints only from process 0. Use this in any distributed mode to log only once.</span>

<span class="sd">        Args:</span>
<span class="sd">            *args: The thing to print. The same as for Python&#39;s built-in print function.</span>
<span class="sd">            **kwargs: The same as for Python&#39;s built-in print function.</span>

<span class="sd">        Example::</span>

<span class="sd">            def forward(self, x):</span>
<span class="sd">                self.print(x, &#39;in forward&#39;)</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">is_global_zero</span><span class="p">:</span>
            <span class="n">progress_bar</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">progress_bar_callback</span>
            <span class="k">if</span> <span class="n">progress_bar</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">progress_bar</span><span class="o">.</span><span class="n">is_enabled</span><span class="p">:</span>
                <span class="n">progress_bar</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="LightningModule.log"><a class="viewcode-back" href="../../../common/lightning_module.html#pytorch_lightning.core.lightning.LightningModule.log">[docs]</a>    <span class="k">def</span> <span class="nf">log</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">value</span><span class="p">:</span> <span class="n">_METRIC_COLLECTION</span><span class="p">,</span>
        <span class="n">prog_bar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">logger</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">on_step</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">on_epoch</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">reduce_fx</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;mean&quot;</span><span class="p">,</span>
        <span class="n">enable_graph</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">sync_dist</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">sync_dist_group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">add_dataloader_idx</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">metric_attribute</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">rank_zero_only</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Log a key, value pair.</span>

<span class="sd">        Example::</span>

<span class="sd">            self.log(&#39;train_loss&#39;, loss)</span>

<span class="sd">        The default behavior per hook is documented here: :ref:`extensions/logging:Automatic Logging`.</span>

<span class="sd">        Args:</span>
<span class="sd">            name: key to log.</span>
<span class="sd">            value: value to log. Can be a ``float``, ``Tensor``, ``Metric``, or a dictionary of the former.</span>
<span class="sd">            prog_bar: if ``True`` logs to the progress bar.</span>
<span class="sd">            logger: if ``True`` logs to the logger.</span>
<span class="sd">            on_step: if ``True`` logs at this step. The default value is determined by the hook.</span>
<span class="sd">                See :ref:`extensions/logging:Automatic Logging` for details.</span>
<span class="sd">            on_epoch: if ``True`` logs epoch accumulated metrics. The default value is determined by the hook.</span>
<span class="sd">                See :ref:`extensions/logging:Automatic Logging` for details.</span>
<span class="sd">            reduce_fx: reduction function over step values for end of epoch. :meth:`torch.mean` by default.</span>
<span class="sd">            enable_graph: if ``True``, will not auto detach the graph.</span>
<span class="sd">            sync_dist: if ``True``, reduces the metric across devices. Use with care as this may lead to a significant</span>
<span class="sd">                communication overhead.</span>
<span class="sd">            sync_dist_group: the DDP group to sync across.</span>
<span class="sd">            add_dataloader_idx: if ``True``, appends the index of the current dataloader to</span>
<span class="sd">                the name (when using multiple dataloaders). If False, user needs to give unique names for</span>
<span class="sd">                each dataloader to not mix the values.</span>
<span class="sd">            batch_size: Current batch_size. This will be directly inferred from the loaded batch,</span>
<span class="sd">                but for some data structures you might need to explicitly provide it.</span>
<span class="sd">            metric_attribute: To restore the metric state, Lightning requires the reference of the</span>
<span class="sd">                :class:`torchmetrics.Metric` in your model. This is found automatically if it is a model attribute.</span>
<span class="sd">            rank_zero_only: Whether the value will be logged only on rank 0. This will prevent synchronization which</span>
<span class="sd">                would produce a deadlock as not all processes would perform this log call.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># check for invalid values</span>
        <span class="n">apply_to_collection</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">dict</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">__check_not_nested</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
        <span class="n">apply_to_collection</span><span class="p">(</span>
            <span class="n">value</span><span class="p">,</span> <span class="nb">object</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">__check_allowed</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">wrong_dtype</span><span class="o">=</span><span class="p">(</span><span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">,</span> <span class="n">Metric</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># not an error to support testing the `*_step` methods without a `Trainer` reference</span>
            <span class="n">rank_zero_warn</span><span class="p">(</span>
                <span class="s2">&quot;You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet.&quot;</span>
                <span class="s2">&quot; This is most likely because the model hasn&#39;t been passed to the `Trainer`&quot;</span>
            <span class="p">)</span>
            <span class="k">return</span>
        <span class="n">results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">_results</span>
        <span class="k">if</span> <span class="n">results</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">MisconfigurationException</span><span class="p">(</span>
                <span class="s2">&quot;You are trying to `self.log()` but the loop&#39;s result collection is not registered&quot;</span>
                <span class="s2">&quot; yet. This is most likely because you are trying to log in a `predict` hook,&quot;</span>
                <span class="s2">&quot; but it doesn&#39;t support logging&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_current_fx_name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">MisconfigurationException</span><span class="p">(</span>
                <span class="s2">&quot;You are trying to `self.log()` but it is not managed by the `Trainer` control flow&quot;</span>
            <span class="p">)</span>

        <span class="n">on_step</span><span class="p">,</span> <span class="n">on_epoch</span> <span class="o">=</span> <span class="n">_FxValidator</span><span class="o">.</span><span class="n">check_logging_and_get_default_levels</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_current_fx_name</span><span class="p">,</span> <span class="n">on_step</span><span class="o">=</span><span class="n">on_step</span><span class="p">,</span> <span class="n">on_epoch</span><span class="o">=</span><span class="n">on_epoch</span>
        <span class="p">)</span>

        <span class="c1"># make sure user doesn&#39;t introduce logic for multi-dataloaders</span>
        <span class="k">if</span> <span class="s2">&quot;/dataloader_idx_&quot;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">MisconfigurationException</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;You called `self.log` with the key `</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">`&quot;</span>
                <span class="s2">&quot; but it should not contain information about `dataloader_idx`&quot;</span>
            <span class="p">)</span>

        <span class="n">value</span> <span class="o">=</span> <span class="n">apply_to_collection</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">__to_tensor</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">_logger_connector</span><span class="o">.</span><span class="n">should_reset_tensors</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_fx_name</span><span class="p">):</span>
            <span class="c1"># if we started a new epoch (running its first batch) the hook name has changed</span>
            <span class="c1"># reset any tensors for the new hook name</span>
            <span class="n">results</span><span class="o">.</span><span class="n">reset</span><span class="p">(</span><span class="n">metrics</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">fx</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_current_fx_name</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">metric_attribute</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">Metric</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_metric_attributes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># compute once</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_metric_attributes</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="nb">id</span><span class="p">(</span><span class="n">module</span><span class="p">):</span> <span class="n">name</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_modules</span><span class="p">()</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">Metric</span><span class="p">)</span>
                <span class="p">}</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_metric_attributes</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="n">MisconfigurationException</span><span class="p">(</span>
                        <span class="s2">&quot;Could not find the `LightningModule` attribute for the `torchmetrics.Metric` logged.&quot;</span>
                        <span class="s2">&quot; You can fix this by setting an attribute for the metric in your `LightningModule`.&quot;</span>
                    <span class="p">)</span>
            <span class="c1"># try to find the passed metric in the LightningModule</span>
            <span class="n">metric_attribute</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_metric_attributes</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="n">value</span><span class="p">),</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">metric_attribute</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="n">MisconfigurationException</span><span class="p">(</span>
                    <span class="s2">&quot;Could not find the `LightningModule` attribute for the `torchmetrics.Metric` logged.&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot; You can fix this by calling `self.log(</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">, ..., metric_attribute=name)` where `name` is one&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot; of </span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_metric_attributes</span><span class="o">.</span><span class="n">values</span><span class="p">())</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">training</span>
            <span class="ow">and</span> <span class="n">is_param_in_hook_signature</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_step</span><span class="p">,</span> <span class="s2">&quot;dataloader_iter&quot;</span><span class="p">,</span> <span class="n">explicit</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="ow">and</span> <span class="n">batch_size</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="n">MisconfigurationException</span><span class="p">(</span>
                <span class="s2">&quot;With `def training_step(self, dataloader_iter)`, `self.log(..., batch_size=...)` should be provided.&quot;</span>
            <span class="p">)</span>

        <span class="n">results</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_current_fx_name</span><span class="p">,</span>
            <span class="n">name</span><span class="p">,</span>
            <span class="n">value</span><span class="p">,</span>
            <span class="n">prog_bar</span><span class="o">=</span><span class="n">prog_bar</span><span class="p">,</span>
            <span class="n">logger</span><span class="o">=</span><span class="n">logger</span><span class="p">,</span>
            <span class="n">on_step</span><span class="o">=</span><span class="n">on_step</span><span class="p">,</span>
            <span class="n">on_epoch</span><span class="o">=</span><span class="n">on_epoch</span><span class="p">,</span>
            <span class="n">reduce_fx</span><span class="o">=</span><span class="n">reduce_fx</span><span class="p">,</span>
            <span class="n">enable_graph</span><span class="o">=</span><span class="n">enable_graph</span><span class="p">,</span>
            <span class="n">add_dataloader_idx</span><span class="o">=</span><span class="n">add_dataloader_idx</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">sync_dist</span><span class="o">=</span><span class="n">sync_dist</span> <span class="ow">and</span> <span class="n">distributed_available</span><span class="p">(),</span>
            <span class="n">sync_dist_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">strategy</span><span class="o">.</span><span class="n">reduce</span> <span class="ow">or</span> <span class="n">sync_ddp</span><span class="p">,</span>
            <span class="n">sync_dist_group</span><span class="o">=</span><span class="n">sync_dist_group</span><span class="p">,</span>
            <span class="n">metric_attribute</span><span class="o">=</span><span class="n">metric_attribute</span><span class="p">,</span>
            <span class="n">rank_zero_only</span><span class="o">=</span><span class="n">rank_zero_only</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">_logger_connector</span><span class="o">.</span><span class="n">_current_fx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_current_fx_name</span></div>

<div class="viewcode-block" id="LightningModule.log_dict"><a class="viewcode-back" href="../../../common/lightning_module.html#pytorch_lightning.core.lightning.LightningModule.log_dict">[docs]</a>    <span class="k">def</span> <span class="nf">log_dict</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">dictionary</span><span class="p">:</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">_METRIC_COLLECTION</span><span class="p">],</span>
        <span class="n">prog_bar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">logger</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">on_step</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">on_epoch</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">reduce_fx</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;mean&quot;</span><span class="p">,</span>
        <span class="n">enable_graph</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">sync_dist</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">sync_dist_group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">add_dataloader_idx</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">rank_zero_only</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Log a dictionary of values at once.</span>

<span class="sd">        Example::</span>

<span class="sd">            values = {&#39;loss&#39;: loss, &#39;acc&#39;: acc, ..., &#39;metric_n&#39;: metric_n}</span>
<span class="sd">            self.log_dict(values)</span>

<span class="sd">        Args:</span>
<span class="sd">            dictionary: key value pairs.</span>
<span class="sd">                The values can be a ``float``, ``Tensor``, ``Metric``, or a dictionary of the former.</span>
<span class="sd">            prog_bar: if ``True`` logs to the progress base.</span>
<span class="sd">            logger: if ``True`` logs to the logger.</span>
<span class="sd">            on_step: if ``True`` logs at this step.</span>
<span class="sd">                ``None`` auto-logs for training_step but not validation/test_step.</span>
<span class="sd">                The default value is determined by the hook.</span>
<span class="sd">                See :ref:`extensions/logging:Automatic Logging` for details.</span>
<span class="sd">            on_epoch: if ``True`` logs epoch accumulated metrics.</span>
<span class="sd">                ``None`` auto-logs for val/test step but not ``training_step``.</span>
<span class="sd">                The default value is determined by the hook.</span>
<span class="sd">                See :ref:`extensions/logging:Automatic Logging` for details.</span>
<span class="sd">            reduce_fx: reduction function over step values for end of epoch. :meth:`torch.mean` by default.</span>
<span class="sd">            enable_graph: if ``True``, will not auto-detach the graph</span>
<span class="sd">            sync_dist: if ``True``, reduces the metric across GPUs/TPUs. Use with care as this may lead to a significant</span>
<span class="sd">                communication overhead.</span>
<span class="sd">            sync_dist_group: the ddp group to sync across.</span>
<span class="sd">            add_dataloader_idx: if ``True``, appends the index of the current dataloader to</span>
<span class="sd">                the name (when using multiple). If ``False``, user needs to give unique names for</span>
<span class="sd">                each dataloader to not mix values.</span>
<span class="sd">            batch_size: Current batch size. This will be directly inferred from the loaded batch,</span>
<span class="sd">                but some data structures might need to explicitly provide it.</span>
<span class="sd">            rank_zero_only: Whether the value will be logged only on rank 0. This will prevent synchronization which</span>
<span class="sd">                would produce a deadlock as not all processes would perform this log call.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">dictionary</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
                <span class="n">name</span><span class="o">=</span><span class="n">k</span><span class="p">,</span>
                <span class="n">value</span><span class="o">=</span><span class="n">v</span><span class="p">,</span>
                <span class="n">prog_bar</span><span class="o">=</span><span class="n">prog_bar</span><span class="p">,</span>
                <span class="n">logger</span><span class="o">=</span><span class="n">logger</span><span class="p">,</span>
                <span class="n">on_step</span><span class="o">=</span><span class="n">on_step</span><span class="p">,</span>
                <span class="n">on_epoch</span><span class="o">=</span><span class="n">on_epoch</span><span class="p">,</span>
                <span class="n">reduce_fx</span><span class="o">=</span><span class="n">reduce_fx</span><span class="p">,</span>
                <span class="n">enable_graph</span><span class="o">=</span><span class="n">enable_graph</span><span class="p">,</span>
                <span class="n">sync_dist</span><span class="o">=</span><span class="n">sync_dist</span><span class="p">,</span>
                <span class="n">sync_dist_group</span><span class="o">=</span><span class="n">sync_dist_group</span><span class="p">,</span>
                <span class="n">add_dataloader_idx</span><span class="o">=</span><span class="n">add_dataloader_idx</span><span class="p">,</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                <span class="n">rank_zero_only</span><span class="o">=</span><span class="n">rank_zero_only</span><span class="p">,</span>
            <span class="p">)</span></div>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">__check_not_nested</span><span class="p">(</span><span class="n">value</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="c1"># self-imposed restriction. for simplicity</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">value</span><span class="o">.</span><span class="n">values</span><span class="p">()):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`self.log(</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2">)` was called, but nested dictionaries cannot be logged&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">value</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">__check_allowed</span><span class="p">(</span><span class="n">v</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`self.log(</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2">)` was called, but `</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">v</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">` values cannot be logged&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__to_tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">log_grad_norm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_norm_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Override this method to change the default behaviour of ``log_grad_norm``.</span>

<span class="sd">        If clipping gradients, the gradients will not have been clipped yet.</span>

<span class="sd">        Args:</span>
<span class="sd">            grad_norm_dict: Dictionary containing current grad norm metrics</span>

<span class="sd">        Example::</span>

<span class="sd">            # DEFAULT</span>
<span class="sd">            def log_grad_norm(self, grad_norm_dict):</span>
<span class="sd">                self.log_dict(grad_norm_dict, on_step=True, on_epoch=True, prog_bar=False, logger=True)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">grad_norm_dict</span><span class="p">,</span> <span class="n">on_step</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">on_epoch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">prog_bar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">logger</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<div class="viewcode-block" id="LightningModule.all_gather"><a class="viewcode-back" href="../../../common/lightning_module.html#pytorch_lightning.core.lightning.LightningModule.all_gather">[docs]</a>    <span class="k">def</span> <span class="nf">all_gather</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">],</span> <span class="n">group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">sync_grads</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Allows users to call ``self.all_gather()`` from the LightningModule, thus making the ``all_gather`` operation</span>
<span class="sd">        accelerator agnostic. ``all_gather`` is a function provided by accelerators to gather a tensor from several</span>
<span class="sd">        distributed processes.</span>

<span class="sd">        Args:</span>
<span class="sd">            data: int, float, tensor of shape (batch, ...), or a (possibly nested) collection thereof.</span>
<span class="sd">            group: the process group to gather results from. Defaults to all processes (world)</span>
<span class="sd">            sync_grads: flag that allows users to synchronize gradients for the all_gather operation</span>

<span class="sd">        Return:</span>
<span class="sd">            A tensor of shape (world_size, batch, ...), or if the input was a collection</span>
<span class="sd">            the output will also be a collection with tensors of this shape.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">group</span> <span class="o">=</span> <span class="n">group</span> <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">group</span><span class="o">.</span><span class="n">WORLD</span>
        <span class="n">all_gather</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">strategy</span><span class="o">.</span><span class="n">all_gather</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">convert_to_tensors</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">apply_to_collection</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">all_gather</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">,</span> <span class="n">sync_grads</span><span class="o">=</span><span class="n">sync_grads</span><span class="p">)</span></div>

<div class="viewcode-block" id="LightningModule.forward"><a class="viewcode-back" href="../../../common/lightning_module.html#pytorch_lightning.core.lightning.LightningModule.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Same as :meth:`torch.nn.Module.forward()`.</span>

<span class="sd">        Args:</span>
<span class="sd">            *args: Whatever you decide to pass into the forward method.</span>
<span class="sd">            **kwargs: Keyword arguments are also possible.</span>

<span class="sd">        Return:</span>
<span class="sd">            Your model&#39;s output</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="LightningModule.training_step"><a class="viewcode-back" href="../../../common/lightning_module.html#pytorch_lightning.core.lightning.LightningModule.training_step">[docs]</a>    <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">STEP_OUTPUT</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Here you compute and return the training loss and some additional metrics for e.g.</span>
<span class="sd">        the progress bar or logger.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch (:class:`~torch.Tensor` | (:class:`~torch.Tensor`, ...) | [:class:`~torch.Tensor`, ...]):</span>
<span class="sd">                The output of your :class:`~torch.utils.data.DataLoader`. A tensor, tuple or list.</span>
<span class="sd">            batch_idx (``int``): Integer displaying index of this batch</span>
<span class="sd">            optimizer_idx (``int``): When using multiple optimizers, this argument will also be present.</span>
<span class="sd">            hiddens (``Any``): Passed in if</span>
<span class="sd">                :paramref:`~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps` &gt; 0.</span>

<span class="sd">        Return:</span>
<span class="sd">            Any of.</span>

<span class="sd">            - :class:`~torch.Tensor` - The loss tensor</span>
<span class="sd">            - ``dict`` - A dictionary. Can include any keys, but must include the key ``&#39;loss&#39;``</span>
<span class="sd">            - ``None`` - Training will skip to the next batch. This is only for automatic optimization.</span>
<span class="sd">                This is not supported for multi-GPU, TPU, IPU, or DeepSpeed.</span>

<span class="sd">        In this step you&#39;d normally do the forward pass and calculate the loss for a batch.</span>
<span class="sd">        You can also do fancier things like multiple forward passes or something model specific.</span>

<span class="sd">        Example::</span>

<span class="sd">            def training_step(self, batch, batch_idx):</span>
<span class="sd">                x, y, z = batch</span>
<span class="sd">                out = self.encoder(x)</span>
<span class="sd">                loss = self.loss(out, x)</span>
<span class="sd">                return loss</span>

<span class="sd">        If you define multiple optimizers, this step will be called with an additional</span>
<span class="sd">        ``optimizer_idx`` parameter.</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            # Multiple optimizers (e.g.: GANs)</span>
<span class="sd">            def training_step(self, batch, batch_idx, optimizer_idx):</span>
<span class="sd">                if optimizer_idx == 0:</span>
<span class="sd">                    # do training_step with encoder</span>
<span class="sd">                    ...</span>
<span class="sd">                if optimizer_idx == 1:</span>
<span class="sd">                    # do training_step with decoder</span>
<span class="sd">                    ...</span>


<span class="sd">        If you add truncated back propagation through time you will also get an additional</span>
<span class="sd">        argument with the hidden states of the previous step.</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            # Truncated back-propagation through time</span>
<span class="sd">            def training_step(self, batch, batch_idx, hiddens):</span>
<span class="sd">                # hiddens are the hidden states from the previous truncated backprop step</span>
<span class="sd">                out, hiddens = self.lstm(data, hiddens)</span>
<span class="sd">                loss = ...</span>
<span class="sd">                return {&quot;loss&quot;: loss, &quot;hiddens&quot;: hiddens}</span>

<span class="sd">        Note:</span>
<span class="sd">            The loss value shown in the progress bar is smoothed (averaged) over the last values,</span>
<span class="sd">            so it differs from the actual loss returned in train/validation step.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">rank_zero_warn</span><span class="p">(</span><span class="s2">&quot;`training_step` must be implemented to be used with the Lightning Trainer&quot;</span><span class="p">)</span></div>

<div class="viewcode-block" id="LightningModule.training_step_end"><a class="viewcode-back" href="../../../common/lightning_module.html#pytorch_lightning.core.lightning.LightningModule.training_step_end">[docs]</a>    <span class="k">def</span> <span class="nf">training_step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">step_output</span><span class="p">:</span> <span class="n">STEP_OUTPUT</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">STEP_OUTPUT</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Use this when training with dp or ddp2 because :meth:`training_step` will operate on only part of the</span>
<span class="sd">        batch. However, this is still optional and only needed for things like softmax or NCE loss.</span>

<span class="sd">        Note:</span>
<span class="sd">            If you later switch to ddp or some other mode, this will still be called</span>
<span class="sd">            so that you don&#39;t have to change your code</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            # pseudocode</span>
<span class="sd">            sub_batches = split_batches_for_dp(batch)</span>
<span class="sd">            step_output = [training_step(sub_batch) for sub_batch in sub_batches]</span>
<span class="sd">            training_step_end(step_output)</span>

<span class="sd">        Args:</span>
<span class="sd">            step_output: What you return in `training_step` for each batch part.</span>

<span class="sd">        Return:</span>
<span class="sd">            Anything</span>

<span class="sd">        When using dp/ddp2 distributed backends, only a portion of the batch is inside the training_step:</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            def training_step(self, batch, batch_idx):</span>
<span class="sd">                # batch is 1/num_gpus big</span>
<span class="sd">                x, y = batch</span>

<span class="sd">                out = self(x)</span>

<span class="sd">                # softmax uses only a portion of the batch in the denominator</span>
<span class="sd">                loss = self.softmax(out)</span>
<span class="sd">                loss = nce_loss(loss)</span>
<span class="sd">                return loss</span>

<span class="sd">        If you wish to do something with all the parts of the batch, then use this method to do it:</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            def training_step(self, batch, batch_idx):</span>
<span class="sd">                # batch is 1/num_gpus big</span>
<span class="sd">                x, y = batch</span>

<span class="sd">                out = self.encoder(x)</span>
<span class="sd">                return {&quot;pred&quot;: out}</span>


<span class="sd">            def training_step_end(self, training_step_outputs):</span>
<span class="sd">                gpu_0_pred = training_step_outputs[0][&quot;pred&quot;]</span>
<span class="sd">                gpu_1_pred = training_step_outputs[1][&quot;pred&quot;]</span>
<span class="sd">                gpu_n_pred = training_step_outputs[n][&quot;pred&quot;]</span>

<span class="sd">                # this softmax now uses the full batch</span>
<span class="sd">                loss = nce_loss([gpu_0_pred, gpu_1_pred, gpu_n_pred])</span>
<span class="sd">                return loss</span>

<span class="sd">        See Also:</span>
<span class="sd">            See the :ref:`accelerators/gpu:Multi GPU Training` guide for more details.</span>
<span class="sd">        &quot;&quot;&quot;</span></div>

<div class="viewcode-block" id="LightningModule.training_epoch_end"><a class="viewcode-back" href="../../../common/lightning_module.html#pytorch_lightning.core.lightning.LightningModule.training_epoch_end">[docs]</a>    <span class="k">def</span> <span class="nf">training_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">:</span> <span class="n">EPOCH_OUTPUT</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Called at the end of the training epoch with the outputs of all training steps. Use this in case you</span>
<span class="sd">        need to do something with all the outputs returned by :meth:`training_step`.</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            # the pseudocode for these calls</span>
<span class="sd">            train_outs = []</span>
<span class="sd">            for train_batch in train_data:</span>
<span class="sd">                out = training_step(train_batch)</span>
<span class="sd">                train_outs.append(out)</span>
<span class="sd">            training_epoch_end(train_outs)</span>

<span class="sd">        Args:</span>
<span class="sd">            outputs: List of outputs you defined in :meth:`training_step`. If there are multiple optimizers or when</span>
<span class="sd">                using ``truncated_bptt_steps &gt; 0``, the lists have the dimensions</span>
<span class="sd">                (n_batches, tbptt_steps, n_optimizers). Dimensions of length 1 are squeezed.</span>

<span class="sd">        Return:</span>
<span class="sd">            None</span>

<span class="sd">        Note:</span>
<span class="sd">            If this method is not overridden, this won&#39;t be called.</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            def training_epoch_end(self, training_step_outputs):</span>
<span class="sd">                # do something with all training_step outputs</span>
<span class="sd">                for out in training_step_outputs:</span>
<span class="sd">                    ...</span>
<span class="sd">        &quot;&quot;&quot;</span></div>

<div class="viewcode-block" id="LightningModule.validation_step"><a class="viewcode-back" href="../../../common/lightning_module.html#pytorch_lightning.core.lightning.LightningModule.validation_step">[docs]</a>    <span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">STEP_OUTPUT</span><span class="p">]:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Operates on a single batch of data from the validation set.</span>
<span class="sd">        In this step you&#39;d might generate examples or calculate anything of interest like accuracy.</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            # the pseudocode for these calls</span>
<span class="sd">            val_outs = []</span>
<span class="sd">            for val_batch in val_data:</span>
<span class="sd">                out = validation_step(val_batch)</span>
<span class="sd">                val_outs.append(out)</span>
<span class="sd">            validation_epoch_end(val_outs)</span>

<span class="sd">        Args:</span>
<span class="sd">            batch: The output of your :class:`~torch.utils.data.DataLoader`.</span>
<span class="sd">            batch_idx: The index of this batch.</span>
<span class="sd">            dataloader_idx: The index of the dataloader that produced this batch.</span>
<span class="sd">                (only if multiple val dataloaders used)</span>

<span class="sd">        Return:</span>
<span class="sd">            - Any object or value</span>
<span class="sd">            - ``None`` - Validation will skip to the next batch</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            # pseudocode of order</span>
<span class="sd">            val_outs = []</span>
<span class="sd">            for val_batch in val_data:</span>
<span class="sd">                out = validation_step(val_batch)</span>
<span class="sd">                if defined(&quot;validation_step_end&quot;):</span>
<span class="sd">                    out = validation_step_end(out)</span>
<span class="sd">                val_outs.append(out)</span>
<span class="sd">            val_outs = validation_epoch_end(val_outs)</span>


<span class="sd">        .. code-block:: python</span>

<span class="sd">            # if you have one val dataloader:</span>
<span class="sd">            def validation_step(self, batch, batch_idx):</span>
<span class="sd">                ...</span>


<span class="sd">            # if you have multiple val dataloaders:</span>
<span class="sd">            def validation_step(self, batch, batch_idx, dataloader_idx=0):</span>
<span class="sd">                ...</span>

<span class="sd">        Examples::</span>

<span class="sd">            # CASE 1: A single validation dataset</span>
<span class="sd">            def validation_step(self, batch, batch_idx):</span>
<span class="sd">                x, y = batch</span>

<span class="sd">                # implement your own</span>
<span class="sd">                out = self(x)</span>
<span class="sd">                loss = self.loss(out, y)</span>

<span class="sd">                # log 6 example images</span>
<span class="sd">                # or generated text... or whatever</span>
<span class="sd">                sample_imgs = x[:6]</span>
<span class="sd">                grid = torchvision.utils.make_grid(sample_imgs)</span>
<span class="sd">                self.logger.experiment.add_image(&#39;example_images&#39;, grid, 0)</span>

<span class="sd">                # calculate acc</span>
<span class="sd">                labels_hat = torch.argmax(out, dim=1)</span>
<span class="sd">                val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)</span>

<span class="sd">                # log the outputs!</span>
<span class="sd">                self.log_dict({&#39;val_loss&#39;: loss, &#39;val_acc&#39;: val_acc})</span>

<span class="sd">        If you pass in multiple val dataloaders, :meth:`validation_step` will have an additional argument. We recommend</span>
<span class="sd">        setting the default value of 0 so that you can quickly switch between single and multiple dataloaders.</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            # CASE 2: multiple validation dataloaders</span>
<span class="sd">            def validation_step(self, batch, batch_idx, dataloader_idx=0):</span>
<span class="sd">                # dataloader_idx tells you which dataset this is.</span>
<span class="sd">                ...</span>

<span class="sd">        Note:</span>
<span class="sd">            If you don&#39;t need to validate you don&#39;t need to implement this method.</span>

<span class="sd">        Note:</span>
<span class="sd">            When the :meth:`validation_step` is called, the model has been put in eval mode</span>
<span class="sd">            and PyTorch gradients have been disabled. At the end of validation,</span>
<span class="sd">            the model goes back to training mode and gradients are enabled.</span>
<span class="sd">        &quot;&quot;&quot;</span></div>

<div class="viewcode-block" id="LightningModule.validation_step_end"><a class="viewcode-back" href="../../../common/lightning_module.html#pytorch_lightning.core.lightning.LightningModule.validation_step_end">[docs]</a>    <span class="k">def</span> <span class="nf">validation_step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">STEP_OUTPUT</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Use this when validating with dp or ddp2 because :meth:`validation_step` will operate on only part of</span>
<span class="sd">        the batch. However, this is still optional and only needed for things like softmax or NCE loss.</span>

<span class="sd">        Note:</span>
<span class="sd">            If you later switch to ddp or some other mode, this will still be called</span>
<span class="sd">            so that you don&#39;t have to change your code.</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            # pseudocode</span>
<span class="sd">            sub_batches = split_batches_for_dp(batch)</span>
<span class="sd">            step_output = [validation_step(sub_batch) for sub_batch in sub_batches]</span>
<span class="sd">            validation_step_end(step_output)</span>

<span class="sd">        Args:</span>
<span class="sd">            step_output: What you return in :meth:`validation_step` for each batch part.</span>

<span class="sd">        Return:</span>
<span class="sd">            None or anything</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            # WITHOUT validation_step_end</span>
<span class="sd">            # if used in DP or DDP2, this batch is 1/num_gpus large</span>
<span class="sd">            def validation_step(self, batch, batch_idx):</span>
<span class="sd">                # batch is 1/num_gpus big</span>
<span class="sd">                x, y = batch</span>

<span class="sd">                out = self.encoder(x)</span>
<span class="sd">                loss = self.softmax(out)</span>
<span class="sd">                loss = nce_loss(loss)</span>
<span class="sd">                self.log(&quot;val_loss&quot;, loss)</span>


<span class="sd">            # --------------</span>
<span class="sd">            # with validation_step_end to do softmax over the full batch</span>
<span class="sd">            def validation_step(self, batch, batch_idx):</span>
<span class="sd">                # batch is 1/num_gpus big</span>
<span class="sd">                x, y = batch</span>

<span class="sd">                out = self(x)</span>
<span class="sd">                return out</span>


<span class="sd">            def validation_step_end(self, val_step_outputs):</span>
<span class="sd">                for out in val_step_outputs:</span>
<span class="sd">                    ...</span>

<span class="sd">        See Also:</span>
<span class="sd">            See the :ref:`accelerators/gpu:Multi GPU Training` guide for more details.</span>
<span class="sd">        &quot;&quot;&quot;</span></div>

<div class="viewcode-block" id="LightningModule.validation_epoch_end"><a class="viewcode-back" href="../../../common/lightning_module.html#pytorch_lightning.core.lightning.LightningModule.validation_epoch_end">[docs]</a>    <span class="k">def</span> <span class="nf">validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">EPOCH_OUTPUT</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">EPOCH_OUTPUT</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Called at the end of the validation epoch with the outputs of all validation steps.</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            # the pseudocode for these calls</span>
<span class="sd">            val_outs = []</span>
<span class="sd">            for val_batch in val_data:</span>
<span class="sd">                out = validation_step(val_batch)</span>
<span class="sd">                val_outs.append(out)</span>
<span class="sd">            validation_epoch_end(val_outs)</span>

<span class="sd">        Args:</span>
<span class="sd">            outputs: List of outputs you defined in :meth:`validation_step`, or if there</span>
<span class="sd">                are multiple dataloaders, a list containing a list of outputs for each dataloader.</span>

<span class="sd">        Return:</span>
<span class="sd">            None</span>

<span class="sd">        Note:</span>
<span class="sd">            If you didn&#39;t define a :meth:`validation_step`, this won&#39;t be called.</span>

<span class="sd">        Examples:</span>
<span class="sd">            With a single dataloader:</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                def validation_epoch_end(self, val_step_outputs):</span>
<span class="sd">                    for out in val_step_outputs:</span>
<span class="sd">                        ...</span>

<span class="sd">            With multiple dataloaders, `outputs` will be a list of lists. The outer list contains</span>
<span class="sd">            one entry per dataloader, while the inner list contains the individual outputs of</span>
<span class="sd">            each validation step for that dataloader.</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                def validation_epoch_end(self, outputs):</span>
<span class="sd">                    for dataloader_output_result in outputs:</span>
<span class="sd">                        dataloader_outs = dataloader_output_result.dataloader_i_outputs</span>

<span class="sd">                    self.log(&quot;final_metric&quot;, final_value)</span>
<span class="sd">        &quot;&quot;&quot;</span></div>

<div class="viewcode-block" id="LightningModule.test_step"><a class="viewcode-back" href="../../../common/lightning_module.html#pytorch_lightning.core.lightning.LightningModule.test_step">[docs]</a>    <span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">STEP_OUTPUT</span><span class="p">]:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Operates on a single batch of data from the test set.</span>
<span class="sd">        In this step you&#39;d normally generate examples or calculate anything of interest</span>
<span class="sd">        such as accuracy.</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            # the pseudocode for these calls</span>
<span class="sd">            test_outs = []</span>
<span class="sd">            for test_batch in test_data:</span>
<span class="sd">                out = test_step(test_batch)</span>
<span class="sd">                test_outs.append(out)</span>
<span class="sd">            test_epoch_end(test_outs)</span>

<span class="sd">        Args:</span>
<span class="sd">            batch: The output of your :class:`~torch.utils.data.DataLoader`.</span>
<span class="sd">            batch_idx: The index of this batch.</span>
<span class="sd">            dataloader_id: The index of the dataloader that produced this batch.</span>
<span class="sd">                (only if multiple test dataloaders used).</span>

<span class="sd">        Return:</span>
<span class="sd">           Any of.</span>

<span class="sd">            - Any object or value</span>
<span class="sd">            - ``None`` - Testing will skip to the next batch</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            # if you have one test dataloader:</span>
<span class="sd">            def test_step(self, batch, batch_idx):</span>
<span class="sd">                ...</span>


<span class="sd">            # if you have multiple test dataloaders:</span>
<span class="sd">            def test_step(self, batch, batch_idx, dataloader_idx=0):</span>
<span class="sd">                ...</span>

<span class="sd">        Examples::</span>

<span class="sd">            # CASE 1: A single test dataset</span>
<span class="sd">            def test_step(self, batch, batch_idx):</span>
<span class="sd">                x, y = batch</span>

<span class="sd">                # implement your own</span>
<span class="sd">                out = self(x)</span>
<span class="sd">                loss = self.loss(out, y)</span>

<span class="sd">                # log 6 example images</span>
<span class="sd">                # or generated text... or whatever</span>
<span class="sd">                sample_imgs = x[:6]</span>
<span class="sd">                grid = torchvision.utils.make_grid(sample_imgs)</span>
<span class="sd">                self.logger.experiment.add_image(&#39;example_images&#39;, grid, 0)</span>

<span class="sd">                # calculate acc</span>
<span class="sd">                labels_hat = torch.argmax(out, dim=1)</span>
<span class="sd">                test_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)</span>

<span class="sd">                # log the outputs!</span>
<span class="sd">                self.log_dict({&#39;test_loss&#39;: loss, &#39;test_acc&#39;: test_acc})</span>

<span class="sd">        If you pass in multiple test dataloaders, :meth:`test_step` will have an additional argument. We recommend</span>
<span class="sd">        setting the default value of 0 so that you can quickly switch between single and multiple dataloaders.</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            # CASE 2: multiple test dataloaders</span>
<span class="sd">            def test_step(self, batch, batch_idx, dataloader_idx=0):</span>
<span class="sd">                # dataloader_idx tells you which dataset this is.</span>
<span class="sd">                ...</span>

<span class="sd">        Note:</span>
<span class="sd">            If you don&#39;t need to test you don&#39;t need to implement this method.</span>

<span class="sd">        Note:</span>
<span class="sd">            When the :meth:`test_step` is called, the model has been put in eval mode and</span>
<span class="sd">            PyTorch gradients have been disabled. At the end of the test epoch, the model goes back</span>
<span class="sd">            to training mode and gradients are enabled.</span>
<span class="sd">        &quot;&quot;&quot;</span></div>

<div class="viewcode-block" id="LightningModule.test_step_end"><a class="viewcode-back" href="../../../common/lightning_module.html#pytorch_lightning.core.lightning.LightningModule.test_step_end">[docs]</a>    <span class="k">def</span> <span class="nf">test_step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">STEP_OUTPUT</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Use this when testing with dp or ddp2 because :meth:`test_step` will operate on only part of the batch.</span>
<span class="sd">        However, this is still optional and only needed for things like softmax or NCE loss.</span>

<span class="sd">        Note:</span>
<span class="sd">            If you later switch to ddp or some other mode, this will still be called</span>
<span class="sd">            so that you don&#39;t have to change your code.</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            # pseudocode</span>
<span class="sd">            sub_batches = split_batches_for_dp(batch)</span>
<span class="sd">            step_output = [test_step(sub_batch) for sub_batch in sub_batches]</span>
<span class="sd">            test_step_end(step_output)</span>

<span class="sd">        Args:</span>
<span class="sd">            step_output: What you return in :meth:`test_step` for each batch part.</span>

<span class="sd">        Return:</span>
<span class="sd">            None or anything</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            # WITHOUT test_step_end</span>
<span class="sd">            # if used in DP or DDP2, this batch is 1/num_gpus large</span>
<span class="sd">            def test_step(self, batch, batch_idx):</span>
<span class="sd">                # batch is 1/num_gpus big</span>
<span class="sd">                x, y = batch</span>

<span class="sd">                out = self(x)</span>
<span class="sd">                loss = self.softmax(out)</span>
<span class="sd">                self.log(&quot;test_loss&quot;, loss)</span>


<span class="sd">            # --------------</span>
<span class="sd">            # with test_step_end to do softmax over the full batch</span>
<span class="sd">            def test_step(self, batch, batch_idx):</span>
<span class="sd">                # batch is 1/num_gpus big</span>
<span class="sd">                x, y = batch</span>

<span class="sd">                out = self.encoder(x)</span>
<span class="sd">                return out</span>


<span class="sd">            def test_step_end(self, output_results):</span>
<span class="sd">                # this out is now the full size of the batch</span>
<span class="sd">                all_test_step_outs = output_results.out</span>
<span class="sd">                loss = nce_loss(all_test_step_outs)</span>
<span class="sd">                self.log(&quot;test_loss&quot;, loss)</span>

<span class="sd">        See Also:</span>
<span class="sd">            See the :ref:`accelerators/gpu:Multi GPU Training` guide for more details.</span>
<span class="sd">        &quot;&quot;&quot;</span></div>

<div class="viewcode-block" id="LightningModule.test_epoch_end"><a class="viewcode-back" href="../../../common/lightning_module.html#pytorch_lightning.core.lightning.LightningModule.test_epoch_end">[docs]</a>    <span class="k">def</span> <span class="nf">test_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">EPOCH_OUTPUT</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">EPOCH_OUTPUT</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Called at the end of a test epoch with the output of all test steps.</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            # the pseudocode for these calls</span>
<span class="sd">            test_outs = []</span>
<span class="sd">            for test_batch in test_data:</span>
<span class="sd">                out = test_step(test_batch)</span>
<span class="sd">                test_outs.append(out)</span>
<span class="sd">            test_epoch_end(test_outs)</span>

<span class="sd">        Args:</span>
<span class="sd">            outputs: List of outputs you defined in :meth:`test_step_end`, or if there</span>
<span class="sd">                are multiple dataloaders, a list containing a list of outputs for each dataloader</span>

<span class="sd">        Return:</span>
<span class="sd">            None</span>

<span class="sd">        Note:</span>
<span class="sd">            If you didn&#39;t define a :meth:`test_step`, this won&#39;t be called.</span>

<span class="sd">        Examples:</span>
<span class="sd">            With a single dataloader:</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                def test_epoch_end(self, outputs):</span>
<span class="sd">                    # do something with the outputs of all test batches</span>
<span class="sd">                    all_test_preds = test_step_outputs.predictions</span>

<span class="sd">                    some_result = calc_all_results(all_test_preds)</span>
<span class="sd">                    self.log(some_result)</span>

<span class="sd">            With multiple dataloaders, `outputs` will be a list of lists. The outer list contains</span>
<span class="sd">            one entry per dataloader, while the inner list contains the individual outputs of</span>
<span class="sd">            each test step for that dataloader.</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                def test_epoch_end(self, outputs):</span>
<span class="sd">                    final_value = 0</span>
<span class="sd">                    for dataloader_outputs in outputs:</span>
<span class="sd">                        for test_step_out in dataloader_outputs:</span>
<span class="sd">                            # do something</span>
<span class="sd">                            final_value += test_step_out</span>

<span class="sd">                    self.log(&quot;final_metric&quot;, final_value)</span>
<span class="sd">        &quot;&quot;&quot;</span></div>

<div class="viewcode-block" id="LightningModule.predict_step"><a class="viewcode-back" href="../../../common/lightning_module.html#pytorch_lightning.core.lightning.LightningModule.predict_step">[docs]</a>    <span class="k">def</span> <span class="nf">predict_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Step function called during :meth:`~pytorch_lightning.trainer.trainer.Trainer.predict`. By default, it</span>
<span class="sd">        calls :meth:`~pytorch_lightning.core.lightning.LightningModule.forward`. Override to add any processing</span>
<span class="sd">        logic.</span>

<span class="sd">        The :meth:`~pytorch_lightning.core.lightning.LightningModule.predict_step` is used</span>
<span class="sd">        to scale inference on multi-devices.</span>

<span class="sd">        To prevent an OOM error, it is possible to use :class:`~pytorch_lightning.callbacks.BasePredictionWriter`</span>
<span class="sd">        callback to write the predictions to disk or database after each batch or on epoch end.</span>

<span class="sd">        The :class:`~pytorch_lightning.callbacks.BasePredictionWriter` should be used while using a spawn</span>
<span class="sd">        based accelerator. This happens for ``Trainer(strategy=&quot;ddp_spawn&quot;)``</span>
<span class="sd">        or training on 8 TPU cores with ``Trainer(accelerator=&quot;tpu&quot;, devices=8)`` as predictions won&#39;t be returned.</span>

<span class="sd">        Example ::</span>

<span class="sd">            class MyModel(LightningModule):</span>

<span class="sd">                def predicts_step(self, batch, batch_idx, dataloader_idx=0):</span>
<span class="sd">                    return self(batch)</span>

<span class="sd">            dm = ...</span>
<span class="sd">            model = MyModel()</span>
<span class="sd">            trainer = Trainer(accelerator=&quot;gpu&quot;, devices=2)</span>
<span class="sd">            predictions = trainer.predict(model, dm)</span>


<span class="sd">        Args:</span>
<span class="sd">            batch: Current batch.</span>
<span class="sd">            batch_idx: Index of current batch.</span>
<span class="sd">            dataloader_idx: Index of the current dataloader.</span>

<span class="sd">        Return:</span>
<span class="sd">            Predicted output</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span></div>

<div class="viewcode-block" id="LightningModule.configure_callbacks"><a class="viewcode-back" href="../../../common/lightning_module.html#pytorch_lightning.core.lightning.LightningModule.configure_callbacks">[docs]</a>    <span class="k">def</span> <span class="nf">configure_callbacks</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">Callback</span><span class="p">],</span> <span class="n">Callback</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Configure model-specific callbacks. When the model gets attached, e.g., when ``.fit()`` or ``.test()``</span>
<span class="sd">        gets called, the list or a callback returned here will be merged with the list of callbacks passed to the</span>
<span class="sd">        Trainer&#39;s ``callbacks`` argument. If a callback returned here has the same type as one or several callbacks</span>
<span class="sd">        already present in the Trainer&#39;s callbacks list, it will take priority and replace them. In addition,</span>
<span class="sd">        Lightning will make sure :class:`~pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint` callbacks</span>
<span class="sd">        run last.</span>

<span class="sd">        Return:</span>
<span class="sd">            A callback or a list of callbacks which will extend the list of callbacks in the Trainer.</span>

<span class="sd">        Example::</span>

<span class="sd">            def configure_callbacks(self):</span>
<span class="sd">                early_stop = EarlyStopping(monitor=&quot;val_acc&quot;, mode=&quot;max&quot;)</span>
<span class="sd">                checkpoint = ModelCheckpoint(monitor=&quot;val_loss&quot;)</span>
<span class="sd">                return [early_stop, checkpoint]</span>

<span class="sd">        Note:</span>
<span class="sd">            Certain callback methods like :meth:`~pytorch_lightning.callbacks.base.Callback.on_init_start`</span>
<span class="sd">            will never be invoked on the new callbacks returned here.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">[]</span></div>

<div class="viewcode-block" id="LightningModule.configure_optimizers"><a class="viewcode-back" href="../../../common/lightning_module.html#pytorch_lightning.core.lightning.LightningModule.configure_optimizers">[docs]</a>    <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Choose what optimizers and learning-rate schedulers to use in your optimization.</span>
<span class="sd">        Normally you&#39;d need one. But in the case of GANs or similar you might have multiple.</span>

<span class="sd">        Return:</span>
<span class="sd">            Any of these 6 options.</span>

<span class="sd">            - **Single optimizer**.</span>
<span class="sd">            - **List or Tuple** of optimizers.</span>
<span class="sd">            - **Two lists** - The first list has multiple optimizers, and the second has multiple LR schedulers</span>
<span class="sd">              (or multiple ``lr_scheduler_config``).</span>
<span class="sd">            - **Dictionary**, with an ``&quot;optimizer&quot;`` key, and (optionally) a ``&quot;lr_scheduler&quot;``</span>
<span class="sd">              key whose value is a single LR scheduler or ``lr_scheduler_config``.</span>
<span class="sd">            - **Tuple of dictionaries** as described above, with an optional ``&quot;frequency&quot;`` key.</span>
<span class="sd">            - **None** - Fit will run without any optimizer.</span>

<span class="sd">        The ``lr_scheduler_config`` is a dictionary which contains the scheduler and its associated configuration.</span>
<span class="sd">        The default configuration is shown below.</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            lr_scheduler_config = {</span>
<span class="sd">                # REQUIRED: The scheduler instance</span>
<span class="sd">                &quot;scheduler&quot;: lr_scheduler,</span>
<span class="sd">                # The unit of the scheduler&#39;s step size, could also be &#39;step&#39;.</span>
<span class="sd">                # &#39;epoch&#39; updates the scheduler on epoch end whereas &#39;step&#39;</span>
<span class="sd">                # updates it after a optimizer update.</span>
<span class="sd">                &quot;interval&quot;: &quot;epoch&quot;,</span>
<span class="sd">                # How many epochs/steps should pass between calls to</span>
<span class="sd">                # `scheduler.step()`. 1 corresponds to updating the learning</span>
<span class="sd">                # rate after every epoch/step.</span>
<span class="sd">                &quot;frequency&quot;: 1,</span>
<span class="sd">                # Metric to to monitor for schedulers like `ReduceLROnPlateau`</span>
<span class="sd">                &quot;monitor&quot;: &quot;val_loss&quot;,</span>
<span class="sd">                # If set to `True`, will enforce that the value specified &#39;monitor&#39;</span>
<span class="sd">                # is available when the scheduler is updated, thus stopping</span>
<span class="sd">                # training if not found. If set to `False`, it will only produce a warning</span>
<span class="sd">                &quot;strict&quot;: True,</span>
<span class="sd">                # If using the `LearningRateMonitor` callback to monitor the</span>
<span class="sd">                # learning rate progress, this keyword can be used to specify</span>
<span class="sd">                # a custom logged name</span>
<span class="sd">                &quot;name&quot;: None,</span>
<span class="sd">            }</span>

<span class="sd">        When there are schedulers in which the ``.step()`` method is conditioned on a value, such as the</span>
<span class="sd">        :class:`torch.optim.lr_scheduler.ReduceLROnPlateau` scheduler, Lightning requires that the</span>
<span class="sd">        ``lr_scheduler_config`` contains the keyword ``&quot;monitor&quot;`` set to the metric name that the scheduler</span>
<span class="sd">        should be conditioned on.</span>

<span class="sd">        .. testcode::</span>

<span class="sd">            # The ReduceLROnPlateau scheduler requires a monitor</span>
<span class="sd">            def configure_optimizers(self):</span>
<span class="sd">                optimizer = Adam(...)</span>
<span class="sd">                return {</span>
<span class="sd">                    &quot;optimizer&quot;: optimizer,</span>
<span class="sd">                    &quot;lr_scheduler&quot;: {</span>
<span class="sd">                        &quot;scheduler&quot;: ReduceLROnPlateau(optimizer, ...),</span>
<span class="sd">                        &quot;monitor&quot;: &quot;metric_to_track&quot;,</span>
<span class="sd">                        &quot;frequency&quot;: &quot;indicates how often the metric is updated&quot;</span>
<span class="sd">                        # If &quot;monitor&quot; references validation metrics, then &quot;frequency&quot; should be set to a</span>
<span class="sd">                        # multiple of &quot;trainer.check_val_every_n_epoch&quot;.</span>
<span class="sd">                    },</span>
<span class="sd">                }</span>


<span class="sd">            # In the case of two optimizers, only one using the ReduceLROnPlateau scheduler</span>
<span class="sd">            def configure_optimizers(self):</span>
<span class="sd">                optimizer1 = Adam(...)</span>
<span class="sd">                optimizer2 = SGD(...)</span>
<span class="sd">                scheduler1 = ReduceLROnPlateau(optimizer1, ...)</span>
<span class="sd">                scheduler2 = LambdaLR(optimizer2, ...)</span>
<span class="sd">                return (</span>
<span class="sd">                    {</span>
<span class="sd">                        &quot;optimizer&quot;: optimizer1,</span>
<span class="sd">                        &quot;lr_scheduler&quot;: {</span>
<span class="sd">                            &quot;scheduler&quot;: scheduler1,</span>
<span class="sd">                            &quot;monitor&quot;: &quot;metric_to_track&quot;,</span>
<span class="sd">                        },</span>
<span class="sd">                    },</span>
<span class="sd">                    {&quot;optimizer&quot;: optimizer2, &quot;lr_scheduler&quot;: scheduler2},</span>
<span class="sd">                )</span>

<span class="sd">        Metrics can be made available to monitor by simply logging it using</span>
<span class="sd">        ``self.log(&#39;metric_to_track&#39;, metric_val)`` in your :class:`~pytorch_lightning.core.lightning.LightningModule`.</span>

<span class="sd">        Note:</span>
<span class="sd">            The ``frequency`` value specified in a dict along with the ``optimizer`` key is an int corresponding</span>
<span class="sd">            to the number of sequential batches optimized with the specific optimizer.</span>
<span class="sd">            It should be given to none or to all of the optimizers.</span>
<span class="sd">            There is a difference between passing multiple optimizers in a list,</span>
<span class="sd">            and passing multiple optimizers in dictionaries with a frequency of 1:</span>

<span class="sd">                - In the former case, all optimizers will operate on the given batch in each optimization step.</span>
<span class="sd">                - In the latter, only one optimizer will operate on the given batch at every step.</span>

<span class="sd">            This is different from the ``frequency`` value specified in the ``lr_scheduler_config`` mentioned above.</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                def configure_optimizers(self):</span>
<span class="sd">                    optimizer_one = torch.optim.SGD(self.model.parameters(), lr=0.01)</span>
<span class="sd">                    optimizer_two = torch.optim.SGD(self.model.parameters(), lr=0.01)</span>
<span class="sd">                    return [</span>
<span class="sd">                        {&quot;optimizer&quot;: optimizer_one, &quot;frequency&quot;: 5},</span>
<span class="sd">                        {&quot;optimizer&quot;: optimizer_two, &quot;frequency&quot;: 10},</span>
<span class="sd">                    ]</span>

<span class="sd">            In this example, the first optimizer will be used for the first 5 steps,</span>
<span class="sd">            the second optimizer for the next 10 steps and that cycle will continue.</span>
<span class="sd">            If an LR scheduler is specified for an optimizer using the ``lr_scheduler`` key in the above dict,</span>
<span class="sd">            the scheduler will only be updated when its optimizer is being used.</span>

<span class="sd">        Examples::</span>

<span class="sd">            # most cases. no learning rate scheduler</span>
<span class="sd">            def configure_optimizers(self):</span>
<span class="sd">                return Adam(self.parameters(), lr=1e-3)</span>

<span class="sd">            # multiple optimizer case (e.g.: GAN)</span>
<span class="sd">            def configure_optimizers(self):</span>
<span class="sd">                gen_opt = Adam(self.model_gen.parameters(), lr=0.01)</span>
<span class="sd">                dis_opt = Adam(self.model_dis.parameters(), lr=0.02)</span>
<span class="sd">                return gen_opt, dis_opt</span>

<span class="sd">            # example with learning rate schedulers</span>
<span class="sd">            def configure_optimizers(self):</span>
<span class="sd">                gen_opt = Adam(self.model_gen.parameters(), lr=0.01)</span>
<span class="sd">                dis_opt = Adam(self.model_dis.parameters(), lr=0.02)</span>
<span class="sd">                dis_sch = CosineAnnealing(dis_opt, T_max=10)</span>
<span class="sd">                return [gen_opt, dis_opt], [dis_sch]</span>

<span class="sd">            # example with step-based learning rate schedulers</span>
<span class="sd">            # each optimizer has its own scheduler</span>
<span class="sd">            def configure_optimizers(self):</span>
<span class="sd">                gen_opt = Adam(self.model_gen.parameters(), lr=0.01)</span>
<span class="sd">                dis_opt = Adam(self.model_dis.parameters(), lr=0.02)</span>
<span class="sd">                gen_sch = {</span>
<span class="sd">                    &#39;scheduler&#39;: ExponentialLR(gen_opt, 0.99),</span>
<span class="sd">                    &#39;interval&#39;: &#39;step&#39;  # called after each training step</span>
<span class="sd">                }</span>
<span class="sd">                dis_sch = CosineAnnealing(dis_opt, T_max=10) # called every epoch</span>
<span class="sd">                return [gen_opt, dis_opt], [gen_sch, dis_sch]</span>

<span class="sd">            # example with optimizer frequencies</span>
<span class="sd">            # see training procedure in `Improved Training of Wasserstein GANs`, Algorithm 1</span>
<span class="sd">            # https://arxiv.org/abs/1704.00028</span>
<span class="sd">            def configure_optimizers(self):</span>
<span class="sd">                gen_opt = Adam(self.model_gen.parameters(), lr=0.01)</span>
<span class="sd">                dis_opt = Adam(self.model_dis.parameters(), lr=0.02)</span>
<span class="sd">                n_critic = 5</span>
<span class="sd">                return (</span>
<span class="sd">                    {&#39;optimizer&#39;: dis_opt, &#39;frequency&#39;: n_critic},</span>
<span class="sd">                    {&#39;optimizer&#39;: gen_opt, &#39;frequency&#39;: 1}</span>
<span class="sd">                )</span>

<span class="sd">        Note:</span>
<span class="sd">            Some things to know:</span>

<span class="sd">            - Lightning calls ``.backward()`` and ``.step()`` on each optimizer and learning rate scheduler as needed.</span>
<span class="sd">            - If you use 16-bit precision (``precision=16``), Lightning will automatically handle the optimizers.</span>
<span class="sd">            - If you use multiple optimizers, :meth:`training_step` will have an additional ``optimizer_idx`` parameter.</span>
<span class="sd">            - If you use :class:`torch.optim.LBFGS`, Lightning handles the closure function automatically for you.</span>
<span class="sd">            - If you use multiple optimizers, gradients will be calculated only for the parameters of current optimizer</span>
<span class="sd">              at each training step.</span>
<span class="sd">            - If you need to control how often those optimizers step or override the default ``.step()`` schedule,</span>
<span class="sd">              override the :meth:`optimizer_step` hook.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">rank_zero_warn</span><span class="p">(</span><span class="s2">&quot;`configure_optimizers` must be implemented to be used with the Lightning Trainer&quot;</span><span class="p">)</span></div>

<div class="viewcode-block" id="LightningModule.manual_backward"><a class="viewcode-back" href="../../../common/lightning_module.html#pytorch_lightning.core.lightning.LightningModule.manual_backward">[docs]</a>    <span class="k">def</span> <span class="nf">manual_backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Call this directly from your :meth:`training_step` when doing optimizations manually. By using this,</span>
<span class="sd">        Lightning can ensure that all the proper scaling gets applied when using mixed precision.</span>

<span class="sd">        See :ref:`manual optimization&lt;common/optimization:Manual optimization&gt;` for more examples.</span>

<span class="sd">        Example::</span>

<span class="sd">            def training_step(...):</span>
<span class="sd">                opt = self.optimizers()</span>
<span class="sd">                loss = ...</span>
<span class="sd">                opt.zero_grad()</span>
<span class="sd">                # automatically applies scaling, etc...</span>
<span class="sd">                self.manual_backward(loss)</span>
<span class="sd">                opt.step()</span>

<span class="sd">        Args:</span>
<span class="sd">            loss: The tensor on which to compute gradients. Must have a graph attached.</span>
<span class="sd">            *args: Additional positional arguments to be forwarded to :meth:`~torch.Tensor.backward`</span>
<span class="sd">            **kwargs: Additional keyword arguments to be forwarded to :meth:`~torch.Tensor.backward`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_verify_is_manual_optimization</span><span class="p">(</span><span class="s2">&quot;manual_backward&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">strategy</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="LightningModule.backward"><a class="viewcode-back" href="../../../common/lightning_module.html#pytorch_lightning.core.lightning.LightningModule.backward">[docs]</a>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Optimizer</span><span class="p">],</span> <span class="n">optimizer_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Called to perform backward on the loss returned in :meth:`training_step`. Override this hook with your</span>
<span class="sd">        own implementation if you need to.</span>

<span class="sd">        Args:</span>
<span class="sd">            loss: The loss tensor returned by :meth:`training_step`. If gradient accumulation is used, the loss here</span>
<span class="sd">                holds the normalized value (scaled by 1 / accumulation steps).</span>
<span class="sd">            optimizer: Current optimizer being used. ``None`` if using manual optimization.</span>
<span class="sd">            optimizer_idx: Index of the current optimizer being used. ``None`` if using manual optimization.</span>

<span class="sd">        Example::</span>

<span class="sd">            def backward(self, loss, optimizer, optimizer_idx):</span>
<span class="sd">                loss.backward()</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="LightningModule.toggle_optimizer"><a class="viewcode-back" href="../../../common/lightning_module.html#pytorch_lightning.core.lightning.LightningModule.toggle_optimizer">[docs]</a>    <span class="k">def</span> <span class="nf">toggle_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">LightningOptimizer</span><span class="p">],</span> <span class="n">optimizer_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Makes sure only the gradients of the current optimizer&#39;s parameters are calculated in the training step</span>
<span class="sd">        to prevent dangling gradients in multiple-optimizer setup.</span>

<span class="sd">        This is only called automatically when automatic optimization is enabled and multiple optimizers are used.</span>
<span class="sd">        It works with :meth:`untoggle_optimizer` to make sure ``param_requires_grad_state`` is properly reset.</span>

<span class="sd">        Args:</span>
<span class="sd">            optimizer: The optimizer to toggle.</span>
<span class="sd">            optimizer_idx: The index of the optimizer to toggle.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Iterate over all optimizer parameters to preserve their `requires_grad` information</span>
        <span class="c1"># in case these are pre-defined during `configure_optimizers`</span>
        <span class="n">param_requires_grad_state</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">opt</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">optimizers</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">opt</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]:</span>
                    <span class="c1"># If a param already appear in param_requires_grad_state, continue</span>
                    <span class="k">if</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">param_requires_grad_state</span><span class="p">:</span>
                        <span class="k">continue</span>
                    <span class="n">param_requires_grad_state</span><span class="p">[</span><span class="n">param</span><span class="p">]</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span>
                    <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># Then iterate over the current optimizer&#39;s parameters and set its `requires_grad`</span>
        <span class="c1"># properties accordingly</span>
        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]:</span>
                <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="n">param_requires_grad_state</span><span class="p">[</span><span class="n">param</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_param_requires_grad_state</span> <span class="o">=</span> <span class="n">param_requires_grad_state</span></div>

<div class="viewcode-block" id="LightningModule.untoggle_optimizer"><a class="viewcode-back" href="../../../common/lightning_module.html#pytorch_lightning.core.lightning.LightningModule.untoggle_optimizer">[docs]</a>    <span class="k">def</span> <span class="nf">untoggle_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Resets the state of required gradients that were toggled with :meth:`toggle_optimizer`.</span>

<span class="sd">        This is only called automatically when automatic optimization is enabled and multiple optimizers are used.</span>

<span class="sd">        Args:</span>
<span class="sd">            optimizer_idx: The index of the optimizer to untoggle.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">opt_idx</span><span class="p">,</span> <span class="n">opt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">optimizers</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">!=</span> <span class="n">opt_idx</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">opt</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]:</span>
                        <span class="k">if</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_param_requires_grad_state</span><span class="p">:</span>
                            <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_param_requires_grad_state</span><span class="p">[</span><span class="n">param</span><span class="p">]</span>
        <span class="c1"># save memory</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_param_requires_grad_state</span> <span class="o">=</span> <span class="p">{}</span></div>

    <span class="k">def</span> <span class="nf">clip_gradients</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span>
        <span class="n">gradient_clip_val</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">gradient_clip_algorithm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Handles gradient clipping internally.</span>

<span class="sd">        Note:</span>
<span class="sd">            Do not override this method. If you want to customize gradient clipping, consider</span>
<span class="sd">            using :meth:`configure_gradient_clipping` method.</span>

<span class="sd">        Args:</span>
<span class="sd">            optimizer: Current optimizer being used.</span>
<span class="sd">            gradient_clip_val: The value at which to clip gradients.</span>
<span class="sd">            gradient_clip_algorithm: The gradient clipping algorithm to use. Pass ``gradient_clip_algorithm=&quot;value&quot;``</span>
<span class="sd">                to clip by value, and ``gradient_clip_algorithm=&quot;norm&quot;`` to clip by norm.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">gradient_clip_val</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">gradient_clip_val</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">gradient_clip_val</span> <span class="ow">or</span> <span class="mf">0.0</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">gradient_clip_val</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">gradient_clip_val</span> <span class="o">!=</span> <span class="n">gradient_clip_val</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">MisconfigurationException</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;You have set `Trainer(gradient_clip_val=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">gradient_clip_val</span><span class="si">!r}</span><span class="s2">)`&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; and have passed `clip_gradients(gradient_clip_val=</span><span class="si">{</span><span class="n">gradient_clip_val</span><span class="si">!r}</span><span class="s2">)`.&quot;</span>
                <span class="s2">&quot; Please use only one of them.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">gradient_clip_algorithm</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">gradient_clip_algorithm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">gradient_clip_algorithm</span> <span class="ow">or</span> <span class="s2">&quot;norm&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">gradient_clip_algorithm</span> <span class="o">=</span> <span class="n">gradient_clip_algorithm</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">gradient_clip_algorithm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">gradient_clip_algorithm</span> <span class="o">!=</span> <span class="n">gradient_clip_algorithm</span>
            <span class="p">):</span>
                <span class="k">raise</span> <span class="n">MisconfigurationException</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;You have set `Trainer(gradient_clip_algorithm=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">gradient_clip_algorithm</span><span class="o">.</span><span class="n">value</span><span class="si">!r}</span><span class="s2">)`&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot; and have passed `clip_gradients(gradient_clip_algorithm=</span><span class="si">{</span><span class="n">gradient_clip_algorithm</span><span class="si">!r}</span><span class="s2">)&quot;</span>
                    <span class="s2">&quot; Please use only one of them.&quot;</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">gradient_clip_val</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`gradient_clip_val` should be an int or a float. Got </span><span class="si">{</span><span class="n">gradient_clip_val</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">GradClipAlgorithmType</span><span class="o">.</span><span class="n">supported_type</span><span class="p">(</span><span class="n">gradient_clip_algorithm</span><span class="o">.</span><span class="n">lower</span><span class="p">()):</span>
            <span class="k">raise</span> <span class="n">MisconfigurationException</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;`gradient_clip_algorithm` </span><span class="si">{</span><span class="n">gradient_clip_algorithm</span><span class="si">}</span><span class="s2"> is invalid.&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; Allowed algorithms: </span><span class="si">{</span><span class="n">GradClipAlgorithmType</span><span class="o">.</span><span class="n">supported_types</span><span class="p">()</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

        <span class="n">gradient_clip_algorithm</span> <span class="o">=</span> <span class="n">GradClipAlgorithmType</span><span class="p">(</span><span class="n">gradient_clip_algorithm</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">precision_plugin</span><span class="o">.</span><span class="n">clip_gradients</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">gradient_clip_val</span><span class="p">,</span> <span class="n">gradient_clip_algorithm</span><span class="p">)</span>

<div class="viewcode-block" id="LightningModule.configure_gradient_clipping"><a class="viewcode-back" href="../../../common/lightning_module.html#pytorch_lightning.core.lightning.LightningModule.configure_gradient_clipping">[docs]</a>    <span class="k">def</span> <span class="nf">configure_gradient_clipping</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span>
        <span class="n">optimizer_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">gradient_clip_val</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">gradient_clip_algorithm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Perform gradient clipping for the optimizer parameters. Called before :meth:`optimizer_step`.</span>

<span class="sd">        Args:</span>
<span class="sd">            optimizer: Current optimizer being used.</span>
<span class="sd">            optimizer_idx: Index of the current optimizer being used.</span>
<span class="sd">            gradient_clip_val: The value at which to clip gradients. By default value passed in Trainer</span>
<span class="sd">                will be available here.</span>
<span class="sd">            gradient_clip_algorithm: The gradient clipping algorithm to use. By default value</span>
<span class="sd">                passed in Trainer will be available here.</span>

<span class="sd">        Example::</span>

<span class="sd">            # Perform gradient clipping on gradients associated with discriminator (optimizer_idx=1) in GAN</span>
<span class="sd">            def configure_gradient_clipping(self, optimizer, optimizer_idx, gradient_clip_val, gradient_clip_algorithm):</span>
<span class="sd">                if optimizer_idx == 1:</span>
<span class="sd">                    # Lightning will handle the gradient clipping</span>
<span class="sd">                    self.clip_gradients(</span>
<span class="sd">                        optimizer,</span>
<span class="sd">                        gradient_clip_val=gradient_clip_val,</span>
<span class="sd">                        gradient_clip_algorithm=gradient_clip_algorithm</span>
<span class="sd">                    )</span>
<span class="sd">                else:</span>
<span class="sd">                    # implement your own custom logic to clip gradients for generator (optimizer_idx=0)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">clip_gradients</span><span class="p">(</span>
            <span class="n">optimizer</span><span class="p">,</span> <span class="n">gradient_clip_val</span><span class="o">=</span><span class="n">gradient_clip_val</span><span class="p">,</span> <span class="n">gradient_clip_algorithm</span><span class="o">=</span><span class="n">gradient_clip_algorithm</span>
        <span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">lr_scheduler_step</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">scheduler</span><span class="p">:</span> <span class="n">LRSchedulerTypeUnion</span><span class="p">,</span>
        <span class="n">optimizer_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">metric</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Override this method to adjust the default way the</span>
<span class="sd">        :class:`~pytorch_lightning.trainer.trainer.Trainer` calls each scheduler.</span>
<span class="sd">        By default, Lightning calls ``step()`` and as shown in the example</span>
<span class="sd">        for each scheduler based on its ``interval``.</span>

<span class="sd">        Args:</span>
<span class="sd">            scheduler: Learning rate scheduler.</span>
<span class="sd">            optimizer_idx: Index of the optimizer associated with this scheduler.</span>
<span class="sd">            metric: Value of the monitor used for schedulers like ``ReduceLROnPlateau``.</span>

<span class="sd">        Examples::</span>

<span class="sd">            # DEFAULT</span>
<span class="sd">            def lr_scheduler_step(self, scheduler, optimizer_idx, metric):</span>
<span class="sd">                if metric is None:</span>
<span class="sd">                    scheduler.step()</span>
<span class="sd">                else:</span>
<span class="sd">                    scheduler.step(metric)</span>

<span class="sd">            # Alternative way to update schedulers if it requires an epoch value</span>
<span class="sd">            def lr_scheduler_step(self, scheduler, optimizer_idx, metric):</span>
<span class="sd">                scheduler.step(epoch=self.current_epoch)</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">metric</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">metric</span><span class="p">)</span>

<div class="viewcode-block" id="LightningModule.optimizer_step"><a class="viewcode-back" href="../../../common/lightning_module.html#pytorch_lightning.core.lightning.LightningModule.optimizer_step">[docs]</a>    <span class="k">def</span> <span class="nf">optimizer_step</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">epoch</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">LightningOptimizer</span><span class="p">],</span>
        <span class="n">optimizer_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">optimizer_closure</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[],</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">on_tpu</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">using_native_amp</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">using_lbfgs</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls</span>
<span class="sd">        each optimizer.</span>

<span class="sd">        By default, Lightning calls ``step()`` and ``zero_grad()`` as shown in the example once per optimizer.</span>
<span class="sd">        This method (and ``zero_grad()``) won&#39;t be called during the accumulation phase when</span>
<span class="sd">        ``Trainer(accumulate_grad_batches != 1)``. Overriding this hook has no benefit with manual optimization.</span>

<span class="sd">        Args:</span>
<span class="sd">            epoch: Current epoch</span>
<span class="sd">            batch_idx: Index of current batch</span>
<span class="sd">            optimizer: A PyTorch optimizer</span>
<span class="sd">            optimizer_idx: If you used multiple optimizers, this indexes into that list.</span>
<span class="sd">            optimizer_closure: The optimizer closure. This closure must be executed as it includes the</span>
<span class="sd">                calls to ``training_step()``, ``optimizer.zero_grad()``, and ``backward()``.</span>
<span class="sd">            on_tpu: ``True`` if TPU backward is required</span>
<span class="sd">            using_native_amp: ``True`` if using native amp</span>
<span class="sd">            using_lbfgs: True if the matching optimizer is :class:`torch.optim.LBFGS`</span>

<span class="sd">        Examples::</span>

<span class="sd">            # DEFAULT</span>
<span class="sd">            def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx,</span>
<span class="sd">                               optimizer_closure, on_tpu, using_native_amp, using_lbfgs):</span>
<span class="sd">                optimizer.step(closure=optimizer_closure)</span>

<span class="sd">            # Alternating schedule for optimizer steps (i.e.: GANs)</span>
<span class="sd">            def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx,</span>
<span class="sd">                               optimizer_closure, on_tpu, using_native_amp, using_lbfgs):</span>
<span class="sd">                # update generator opt every step</span>
<span class="sd">                if optimizer_idx == 0:</span>
<span class="sd">                    optimizer.step(closure=optimizer_closure)</span>

<span class="sd">                # update discriminator opt every 2 steps</span>
<span class="sd">                if optimizer_idx == 1:</span>
<span class="sd">                    if (batch_idx + 1) % 2 == 0 :</span>
<span class="sd">                        optimizer.step(closure=optimizer_closure)</span>
<span class="sd">                    else:</span>
<span class="sd">                        # call the closure by itself to run `training_step` + `backward` without an optimizer step</span>
<span class="sd">                        optimizer_closure()</span>

<span class="sd">                # ...</span>
<span class="sd">                # add as many optimizers as you want</span>

<span class="sd">        Here&#39;s another example showing how to use this for more advanced things such as</span>
<span class="sd">        learning rate warm-up:</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            # learning rate warm-up</span>
<span class="sd">            def optimizer_step(</span>
<span class="sd">                self,</span>
<span class="sd">                epoch,</span>
<span class="sd">                batch_idx,</span>
<span class="sd">                optimizer,</span>
<span class="sd">                optimizer_idx,</span>
<span class="sd">                optimizer_closure,</span>
<span class="sd">                on_tpu,</span>
<span class="sd">                using_native_amp,</span>
<span class="sd">                using_lbfgs,</span>
<span class="sd">            ):</span>
<span class="sd">                # update params</span>
<span class="sd">                optimizer.step(closure=optimizer_closure)</span>

<span class="sd">                # manually warm up lr without a scheduler</span>
<span class="sd">                if self.trainer.global_step &lt; 500:</span>
<span class="sd">                    lr_scale = min(1.0, float(self.trainer.global_step + 1) / 500.0)</span>
<span class="sd">                    for pg in optimizer.param_groups:</span>
<span class="sd">                        pg[&quot;lr&quot;] = lr_scale * self.learning_rate</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="o">=</span><span class="n">optimizer_closure</span><span class="p">)</span></div>

<div class="viewcode-block" id="LightningModule.optimizer_zero_grad"><a class="viewcode-back" href="../../../common/lightning_module.html#pytorch_lightning.core.lightning.LightningModule.optimizer_zero_grad">[docs]</a>    <span class="k">def</span> <span class="nf">optimizer_zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Override this method to change the default behaviour of ``optimizer.zero_grad()``.</span>

<span class="sd">        Args:</span>
<span class="sd">            epoch: Current epoch</span>
<span class="sd">            batch_idx: Index of current batch</span>
<span class="sd">            optimizer: A PyTorch optimizer</span>
<span class="sd">            optimizer_idx: If you used multiple optimizers this indexes into that list.</span>

<span class="sd">        Examples::</span>

<span class="sd">            # DEFAULT</span>
<span class="sd">            def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx):</span>
<span class="sd">                optimizer.zero_grad()</span>

<span class="sd">            # Set gradients to `None` instead of zero to improve performance.</span>
<span class="sd">            def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx):</span>
<span class="sd">                optimizer.zero_grad(set_to_none=True)</span>

<span class="sd">        See :meth:`torch.optim.Optimizer.zero_grad` for the explanation of the above example.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span></div>

<div class="viewcode-block" id="LightningModule.tbptt_split_batch"><a class="viewcode-back" href="../../../common/lightning_module.html#pytorch_lightning.core.lightning.LightningModule.tbptt_split_batch">[docs]</a>    <span class="k">def</span> <span class="nf">tbptt_split_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">split_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        When using truncated backpropagation through time, each batch must be split along the</span>
<span class="sd">        time dimension. Lightning handles this by default, but for custom behavior override</span>
<span class="sd">        this function.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch: Current batch</span>
<span class="sd">            split_size: The size of the split</span>

<span class="sd">        Return:</span>
<span class="sd">            List of batch splits. Each split will be passed to :meth:`training_step` to enable truncated</span>
<span class="sd">            back propagation through time. The default implementation splits root level Tensors and</span>
<span class="sd">            Sequences at dim=1 (i.e. time dim). It assumes that each time dim is the same length.</span>

<span class="sd">        Examples::</span>

<span class="sd">            def tbptt_split_batch(self, batch, split_size):</span>
<span class="sd">                splits = []</span>
<span class="sd">                for t in range(0, time_dims[0], split_size):</span>
<span class="sd">                    batch_split = []</span>
<span class="sd">                    for i, x in enumerate(batch):</span>
<span class="sd">                        if isinstance(x, torch.Tensor):</span>
<span class="sd">                            split_x = x[:, t:t + split_size]</span>
<span class="sd">                        elif isinstance(x, collections.Sequence):</span>
<span class="sd">                            split_x = [None] * len(x)</span>
<span class="sd">                            for batch_idx in range(len(x)):</span>
<span class="sd">                              split_x[batch_idx] = x[batch_idx][t:t + split_size]</span>
<span class="sd">                        batch_split.append(split_x)</span>
<span class="sd">                    splits.append(batch_split)</span>
<span class="sd">                return splits</span>

<span class="sd">        Note:</span>
<span class="sd">            Called in the training loop after</span>
<span class="sd">            :meth:`~pytorch_lightning.callbacks.base.Callback.on_train_batch_start`</span>
<span class="sd">            if :paramref:`~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps` &gt; 0.</span>
<span class="sd">            Each returned batch split is passed separately to :meth:`training_step`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">time_dims</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">batch</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">Sequence</span><span class="p">))]</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">time_dims</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;Unable to determine batch time dimension&quot;</span>
        <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">x</span> <span class="o">==</span> <span class="n">time_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">time_dims</span><span class="p">),</span> <span class="s2">&quot;Batch time dimension length is ambiguous&quot;</span>

        <span class="n">splits</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">time_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">split_size</span><span class="p">):</span>
            <span class="n">batch_split</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                    <span class="n">split_x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="n">t</span> <span class="p">:</span> <span class="n">t</span> <span class="o">+</span> <span class="n">split_size</span><span class="p">]</span>
                <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">Sequence</span><span class="p">):</span>
                    <span class="n">split_x</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">batch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
                        <span class="n">split_x</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">][</span><span class="n">t</span> <span class="p">:</span> <span class="n">t</span> <span class="o">+</span> <span class="n">split_size</span><span class="p">]</span>

                <span class="n">batch_split</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">split_x</span><span class="p">)</span>

            <span class="n">splits</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch_split</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">splits</span></div>

    <span class="k">def</span> <span class="nf">summarize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_depth</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ModelSummary</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Summarize this LightningModule.</span>

<span class="sd">        .. deprecated:: v1.5</span>
<span class="sd">            This method was deprecated in v1.5 in favor of `pytorch_lightning.utilities.model_summary.summarize`</span>
<span class="sd">            and will be removed in v1.7.</span>

<span class="sd">        Args:</span>
<span class="sd">            max_depth: The maximum depth of layer nesting that the summary will include. A value of 0 turns the</span>
<span class="sd">                layer summary off. Default: 1.</span>

<span class="sd">        Return:</span>
<span class="sd">            The model summary object</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">rank_zero_deprecation</span><span class="p">(</span>
            <span class="s2">&quot;The `LightningModule.summarize` method is deprecated in v1.5 and will be removed in v1.7. &quot;</span>
            <span class="s2">&quot;Use `pytorch_lightning.utilities.model_summary.summarize` instead.&quot;</span><span class="p">,</span>
            <span class="n">stacklevel</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">summarize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_depth</span><span class="p">)</span>

<div class="viewcode-block" id="LightningModule.freeze"><a class="viewcode-back" href="../../../common/lightning_module.html#pytorch_lightning.core.lightning.LightningModule.freeze">[docs]</a>    <span class="k">def</span> <span class="nf">freeze</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Freeze all params for inference.</span>

<span class="sd">        Example::</span>

<span class="sd">            model = MyLightningModule(...)</span>
<span class="sd">            model.freeze()</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span></div>

<div class="viewcode-block" id="LightningModule.unfreeze"><a class="viewcode-back" href="../../../common/lightning_module.html#pytorch_lightning.core.lightning.LightningModule.unfreeze">[docs]</a>    <span class="k">def</span> <span class="nf">unfreeze</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Unfreeze all parameters for training.</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            model = MyLightningModule(...)</span>
<span class="sd">            model.unfreeze()</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">()</span></div>

    <span class="k">def</span> <span class="nf">get_progress_bar_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        .. deprecated:: v1.5</span>
<span class="sd">            This method was deprecated in v1.5 in favor of</span>
<span class="sd">            `pytorch_lightning.callbacks.progress.base.get_metrics` and will be removed in v1.7.</span>

<span class="sd">        Implement this to override the default items displayed in the progress bar.</span>
<span class="sd">        By default it includes the average loss value, split index of BPTT (if used)</span>
<span class="sd">        and the version of the experiment when using a logger.</span>

<span class="sd">        .. code-block::</span>

<span class="sd">            Epoch 1:   4%|▎         | 40/1095 [00:03&lt;01:37, 10.84it/s, loss=4.501, v_num=10]</span>

<span class="sd">        Here is an example how to override the defaults:</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            def get_progress_bar_dict(self):</span>
<span class="sd">                # don&#39;t show the version number</span>
<span class="sd">                items = super().get_progress_bar_dict()</span>
<span class="sd">                items.pop(&quot;v_num&quot;, None)</span>
<span class="sd">                return items</span>

<span class="sd">        Return:</span>
<span class="sd">            Dictionary with the items to be displayed in the progress bar.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">progress_base</span><span class="o">.</span><span class="n">get_standard_metrics</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_verify_is_manual_optimization</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fn_name</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">automatic_optimization</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">MisconfigurationException</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;to use </span><span class="si">{</span><span class="n">fn_name</span><span class="si">}</span><span class="s2">, please disable automatic optimization:&quot;</span>
                <span class="s2">&quot; set model property `automatic_optimization` as False&quot;</span>
            <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_auto_collect_arguments</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">frame</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Dict</span><span class="p">,</span> <span class="n">Dict</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Collect all module arguments in the current constructor and all child constructors. The child</span>
<span class="sd">        constructors are all the ``__init__`` methods that reach the current class through (chained)</span>
<span class="sd">        ``super().__init__()`` calls.</span>

<span class="sd">        Args:</span>
<span class="sd">            frame: instance frame</span>

<span class="sd">        Returns:</span>
<span class="sd">            self_arguments: arguments dictionary of the first instance</span>
<span class="sd">            parents_arguments: arguments dictionary of the parent&#39;s instances</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">frame</span><span class="p">:</span>
            <span class="n">frame</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">currentframe</span><span class="p">()</span>

        <span class="n">frame_args</span> <span class="o">=</span> <span class="n">collect_init_args</span><span class="p">(</span><span class="n">frame</span><span class="o">.</span><span class="n">f_back</span><span class="p">,</span> <span class="p">[])</span>
        <span class="n">self_arguments</span> <span class="o">=</span> <span class="n">frame_args</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># set hyper_parameters in child</span>
        <span class="n">self_arguments</span> <span class="o">=</span> <span class="n">self_arguments</span>
        <span class="n">parents_arguments</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># add all arguments from parents</span>
        <span class="k">for</span> <span class="n">args</span> <span class="ow">in</span> <span class="n">frame_args</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">parents_arguments</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self_arguments</span><span class="p">,</span> <span class="n">parents_arguments</span>

<div class="viewcode-block" id="LightningModule.to_onnx"><a class="viewcode-back" href="../../../common/lightning_module.html#pytorch_lightning.core.lightning.LightningModule.to_onnx">[docs]</a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">to_onnx</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">file_path</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Path</span><span class="p">],</span> <span class="n">input_sample</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Saves the model in ONNX format.</span>

<span class="sd">        Args:</span>
<span class="sd">            file_path: The path of the file the onnx model should be saved to.</span>
<span class="sd">            input_sample: An input for tracing. Default: None (Use self.example_input_array)</span>
<span class="sd">            **kwargs: Will be passed to torch.onnx.export function.</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; class SimpleModel(LightningModule):</span>
<span class="sd">            ...     def __init__(self):</span>
<span class="sd">            ...         super().__init__()</span>
<span class="sd">            ...         self.l1 = torch.nn.Linear(in_features=64, out_features=4)</span>
<span class="sd">            ...</span>
<span class="sd">            ...     def forward(self, x):</span>
<span class="sd">            ...         return torch.relu(self.l1(x.view(x.size(0), -1)))</span>

<span class="sd">            &gt;&gt;&gt; with tempfile.NamedTemporaryFile(suffix=&#39;.onnx&#39;, delete=False) as tmpfile:</span>
<span class="sd">            ...     model = SimpleModel()</span>
<span class="sd">            ...     input_sample = torch.randn((1, 64))</span>
<span class="sd">            ...     model.to_onnx(tmpfile.name, input_sample, export_params=True)</span>
<span class="sd">            ...     os.path.isfile(tmpfile.name)</span>
<span class="sd">            True</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">mode</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span>

        <span class="k">if</span> <span class="n">input_sample</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">example_input_array</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Could not export to ONNX since neither `input_sample` nor&quot;</span>
                    <span class="s2">&quot; `model.example_input_array` attribute is set.&quot;</span>
                <span class="p">)</span>
            <span class="n">input_sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">example_input_array</span>

        <span class="n">input_sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_batch_transfer_handler</span><span class="p">(</span><span class="n">input_sample</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">_TORCH_GREATER_EQUAL_1_10</span> <span class="ow">and</span> <span class="s2">&quot;example_outputs&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_sample</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">):</span>
                <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;example_outputs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="o">*</span><span class="n">input_sample</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;example_outputs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">input_sample</span><span class="p">)</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_sample</span><span class="p">,</span> <span class="n">file_path</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span></div>

<div class="viewcode-block" id="LightningModule.to_torchscript"><a class="viewcode-back" href="../../../common/lightning_module.html#pytorch_lightning.core.lightning.LightningModule.to_torchscript">[docs]</a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">to_torchscript</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">file_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Path</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;script&quot;</span><span class="p">,</span>
        <span class="n">example_inputs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">ScriptModule</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ScriptModule</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;By default compiles the whole model to a :class:`~torch.jit.ScriptModule`. If you want to use tracing,</span>
<span class="sd">        please provided the argument ``method=&#39;trace&#39;`` and make sure that either the `example_inputs` argument is</span>
<span class="sd">        provided, or the model has :attr:`example_input_array` set. If you would like to customize the modules that</span>
<span class="sd">        are scripted you should override this method. In case you want to return multiple modules, we recommend</span>
<span class="sd">        using a dictionary.</span>

<span class="sd">        Args:</span>
<span class="sd">            file_path: Path where to save the torchscript. Default: None (no file saved).</span>
<span class="sd">            method: Whether to use TorchScript&#39;s script or trace method. Default: &#39;script&#39;</span>
<span class="sd">            example_inputs: An input to be used to do tracing when method is set to &#39;trace&#39;.</span>
<span class="sd">              Default: None (uses :attr:`example_input_array`)</span>
<span class="sd">            **kwargs: Additional arguments that will be passed to the :func:`torch.jit.script` or</span>
<span class="sd">              :func:`torch.jit.trace` function.</span>

<span class="sd">        Note:</span>
<span class="sd">            - Requires the implementation of the</span>
<span class="sd">              :meth:`~pytorch_lightning.core.lightning.LightningModule.forward` method.</span>
<span class="sd">            - The exported script will be set to evaluation mode.</span>
<span class="sd">            - It is recommended that you install the latest supported version of PyTorch</span>
<span class="sd">              to use this feature without limitations. See also the :mod:`torch.jit`</span>
<span class="sd">              documentation for supported features.</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; class SimpleModel(LightningModule):</span>
<span class="sd">            ...     def __init__(self):</span>
<span class="sd">            ...         super().__init__()</span>
<span class="sd">            ...         self.l1 = torch.nn.Linear(in_features=64, out_features=4)</span>
<span class="sd">            ...</span>
<span class="sd">            ...     def forward(self, x):</span>
<span class="sd">            ...         return torch.relu(self.l1(x.view(x.size(0), -1)))</span>
<span class="sd">            ...</span>
<span class="sd">            &gt;&gt;&gt; model = SimpleModel()</span>
<span class="sd">            &gt;&gt;&gt; model.to_torchscript(file_path=&quot;model.pt&quot;)  # doctest: +SKIP</span>
<span class="sd">            &gt;&gt;&gt; os.path.isfile(&quot;model.pt&quot;)  # doctest: +SKIP</span>
<span class="sd">            &gt;&gt;&gt; torch.jit.save(model.to_torchscript(file_path=&quot;model_trace.pt&quot;, method=&#39;trace&#39;, # doctest: +SKIP</span>
<span class="sd">            ...                                     example_inputs=torch.randn(1, 64)))  # doctest: +SKIP</span>
<span class="sd">            &gt;&gt;&gt; os.path.isfile(&quot;model_trace.pt&quot;)  # doctest: +SKIP</span>
<span class="sd">            True</span>

<span class="sd">        Return:</span>
<span class="sd">            This LightningModule as a torchscript, regardless of whether `file_path` is</span>
<span class="sd">            defined or not.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">mode</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_running_torchscript</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;script&quot;</span><span class="p">:</span>
            <span class="n">torchscript_module</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">eval</span><span class="p">(),</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;trace&quot;</span><span class="p">:</span>
            <span class="c1"># if no example inputs are provided, try to see if model has example_input_array set</span>
            <span class="k">if</span> <span class="n">example_inputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">example_input_array</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;Choosing method=`trace` requires either `example_inputs`&quot;</span>
                        <span class="s2">&quot; or `model.example_input_array` to be defined.&quot;</span>
                    <span class="p">)</span>
                <span class="n">example_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">example_input_array</span>

            <span class="c1"># automatically send example inputs to the right device and use trace</span>
            <span class="n">example_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_batch_transfer_handler</span><span class="p">(</span><span class="n">example_inputs</span><span class="p">)</span>
            <span class="n">torchscript_module</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">eval</span><span class="p">(),</span> <span class="n">example_inputs</span><span class="o">=</span><span class="n">example_inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The &#39;method&#39; parameter only supports &#39;script&#39; or &#39;trace&#39;, but value given was: </span><span class="si">{</span><span class="n">method</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">file_path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">fs</span> <span class="o">=</span> <span class="n">get_filesystem</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>
            <span class="k">with</span> <span class="n">fs</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">torchscript_module</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_running_torchscript</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">return</span> <span class="n">torchscript_module</span></div>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">model_size</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Returns the model size in MegaBytes (MB)</span>

<span class="sd">        Note:</span>
<span class="sd">            This property will not return correct value for Deepspeed (stage 3) and fully-sharded training.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_running_torchscript</span><span class="p">:</span>  <span class="c1"># remove with the deprecation removal</span>
            <span class="n">rank_zero_deprecation</span><span class="p">(</span>
                <span class="s2">&quot;The `LightningModule.model_size` property was deprecated in v1.5 and will be removed in v1.7.&quot;</span>
                <span class="s2">&quot; Please use the `pytorch_lightning.utilities.memory.get_model_size_mb`.&quot;</span><span class="p">,</span>
                <span class="n">stacklevel</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">get_model_size_mb</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">use_amp</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        .. deprecated:: v1.6.</span>

<span class="sd">            This property was deprecated in v1.6 and will be removed in v1.8.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_running_torchscript</span><span class="p">:</span>  <span class="c1"># remove with the deprecation removal</span>
            <span class="n">rank_zero_deprecation</span><span class="p">(</span>
                <span class="s2">&quot;`LightningModule.use_amp` was deprecated in v1.6 and will be removed in v1.8.&quot;</span>
                <span class="s2">&quot; Please use `Trainer.amp_backend`.&quot;</span><span class="p">,</span>
                <span class="n">stacklevel</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_amp</span>

    <span class="nd">@use_amp</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">use_amp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_amp</span><span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        .. deprecated:: v1.6.</span>

<span class="sd">            This property was deprecated in v1.6 and will be removed in v1.8.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_running_torchscript</span><span class="p">:</span>  <span class="c1"># remove with the deprecation removal</span>
            <span class="n">rank_zero_deprecation</span><span class="p">(</span>
                <span class="s2">&quot;`LightningModule.use_amp` was deprecated in v1.6 and will be removed in v1.8.&quot;</span>
                <span class="s2">&quot; Please use `Trainer.amp_backend`.&quot;</span><span class="p">,</span>
                <span class="n">stacklevel</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_use_amp</span> <span class="o">=</span> <span class="n">use_amp</span>

<div class="viewcode-block" id="LightningModule.add_to_queue"><a class="viewcode-back" href="../../../common/lightning_module.html#pytorch_lightning.core.lightning.LightningModule.add_to_queue">[docs]</a>    <span class="k">def</span> <span class="nf">add_to_queue</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">queue</span><span class="p">:</span> <span class="n">pl</span><span class="o">.</span><span class="n">strategies</span><span class="o">.</span><span class="n">launchers</span><span class="o">.</span><span class="n">spawn</span><span class="o">.</span><span class="n">_FakeQueue</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Appends the :attr:`trainer.callback_metrics` dictionary to the given queue. To avoid issues with memory</span>
<span class="sd">        sharing, we cast the data to numpy.</span>

<span class="sd">        Args:</span>
<span class="sd">            queue: the instance of the queue to append the data.</span>

<span class="sd">        .. deprecated:: v1.5</span>
<span class="sd">            This method was deprecated in v1.5 and will be removed in v1.7.</span>
<span class="sd">        &quot;&quot;&quot;</span></div>

<div class="viewcode-block" id="LightningModule.get_from_queue"><a class="viewcode-back" href="../../../common/lightning_module.html#pytorch_lightning.core.lightning.LightningModule.get_from_queue">[docs]</a>    <span class="k">def</span> <span class="nf">get_from_queue</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">queue</span><span class="p">:</span> <span class="n">pl</span><span class="o">.</span><span class="n">strategies</span><span class="o">.</span><span class="n">launchers</span><span class="o">.</span><span class="n">spawn</span><span class="o">.</span><span class="n">_FakeQueue</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Retrieve the :attr:`trainer.callback_metrics` dictionary from the given queue. To preserve consistency,</span>
<span class="sd">        we cast back the data to ``torch.Tensor``.</span>

<span class="sd">        Args:</span>
<span class="sd">            queue: the instance of the queue from where to get the data.</span>

<span class="sd">        .. deprecated:: v1.5</span>
<span class="sd">            This method was deprecated in v1.5 and will be removed in v1.7.</span>
<span class="sd">        &quot;&quot;&quot;</span></div>

    <span class="nd">@contextmanager</span>
    <span class="k">def</span> <span class="nf">_prevent_trainer_and_dataloaders_deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_should_prevent_trainer_and_dataloaders_deepcopy</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">yield</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_should_prevent_trainer_and_dataloaders_deepcopy</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="n">state</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_should_prevent_trainer_and_dataloaders_deepcopy</span><span class="p">:</span>
            <span class="n">state</span><span class="p">[</span><span class="s2">&quot;trainer&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">state</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;train_dataloader&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">state</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;val_dataloader&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">state</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;test_dataloader&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">state</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;predict_dataloader&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">state</span>

    <span class="k">def</span> <span class="nf">_register_sharded_tensor_state_dict_hooks_if_available</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Adds ShardedTensor state dict hooks if ShardedTensors are supported.</span>

<span class="sd">        These hooks ensure that ShardedTensors are included when saving, and are loaded the LightningModule correctly.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">_TORCH_GREATER_EQUAL_1_10</span> <span class="ow">or</span> <span class="n">_IS_WINDOWS</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="n">rank_zero_debug</span><span class="p">(</span><span class="s2">&quot;Could not register sharded tensor state dict hooks&quot;</span><span class="p">)</span>
            <span class="k">return</span>

        <span class="kn">from</span> <span class="nn">torch.distributed._sharded_tensor</span> <span class="kn">import</span> <span class="n">pre_load_state_dict_hook</span><span class="p">,</span> <span class="n">state_dict_hook</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_register_state_dict_hook</span><span class="p">(</span><span class="n">state_dict_hook</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_register_load_state_dict_pre_hook</span><span class="p">(</span><span class="n">pre_load_state_dict_hook</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright Copyright (c) 2018-2022, William Falcon et al...

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
         <script src="../../../_static/jquery.js"></script>
         <script src="../../../_static/underscore.js"></script>
         <script src="../../../_static/doctools.js"></script>
         <script src="../../../_static/clipboard.min.js"></script>
         <script src="../../../_static/copybutton.js"></script>
         <script src="../../../_static/copybutton.js"></script>
         <script>let toggleHintShow = 'Click to show';</script>
         <script>let toggleHintHide = 'Click to hide';</script>
         <script>let toggleOpenOnPrint = 'true';</script>
         <script src="../../../_static/togglebutton.js"></script>
         <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Best practices', 'Optional Extensions', 'Tutorials', 'API References', 'Bolts', 'Examples', 'Partner Domain Frameworks', 'Community'];
</script>



  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <!--
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch-lightning.rtfd.io/en/latest">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch-lightning.readthedocs.io/en/latest/#tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch-lightning.readthedocs.io/en/latest/#community-examples">View Resources</a>
        </div>
        -->
      </div>
    </div>
  </div>

  <!--
  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch-lightning.rtfd.io/en/latest/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch-lightning.rtfd.io/en/latest/">PyTorch</a></li>
            <li><a href="https://pytorch-lightning.readthedocs.io/en/latest/starter/introduction.html">Get Started</a></li>
            <li><a href="https://pytorch-lightning.rtfd.io/en/latest/">Features</a></li>
            <li><a href="">Ecosystem</a></li>
            <li><a href="https://www.pytorchlightning.ai/blog">Blog</a></li>
            <li><a href="https://github.com/PyTorchLightning/pytorch-lightning/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch-lightning.readthedocs.io/en/latest/#community-examples">Resources</a></li>
            <li><a href="https://pytorch-lightning.readthedocs.io/en/latest/#tutorials">Tutorials</a></li>
            <li><a href="https://pytorch-lightning.rtfd.io/en/latest">Docs</a></li>
            <li><a href="https://www.pytorchlightning.ai/community" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/PyTorchLightning/pytorch-lightning/issues" target="_blank">Github Issues</a></li>
            <li><a href="" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Stay Connected</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/PyTorchLightnin" target="_blank" class="twitter"></a>
            <a href="" target="_blank" class="youtube"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>
  -->

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. Read PyTorch Lightning's <a href="https://pytorchlightning.ai/privacy-policy">Privacy Policy</a>.</p>
    <img class="close-button" src="../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch-lightning.rtfd.io/en/latest/" aria-label="PyTorch Lightning"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch-lightning.readthedocs.io/en/latest/starter/introduction.html">Get Started</a>
          </li>

          <li>
            <a href="https://www.pytorchlightning.ai/blog">Blog</a>
          </li>

          <li class="resources-mobile-menu-title">
            Ecosystem
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch-lightning.readthedocs.io/en/stable/">PyTorch Lightning</a>
            </li>

            <li>
              <a href="https://torchmetrics.readthedocs.io/en/stable/">TorchMetrics</a>
            </li>

            <li>
              <a href="https://lightning-flash.readthedocs.io/en/stable/">Lightning Flash</a>
            </li>

            <li>
              <a href="https://lightning-transformers.readthedocs.io/en/stable/">Lightning Transformers</a>
            </li>

            <li>
              <a href="https://lightning-bolts.readthedocs.io/en/stable/">Lightning Bolts</a>
            </li>
          </ul>

          <!--<li class="resources-mobile-menu-title">
            Resources
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch-lightning.readthedocs.io/en/latest/#community-examples">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch-lightning.rtfd.io/en/latest/">About</a>
            </li>

            <li>
              <a href="">Models (Beta)</a>
            </li>

            <li>
              <a href="https://www.pytorchlightning.ai/community">Community</a>
            </li>

            <li>
              <a href="https://github.com/PyTorchLightning/pytorch-lightning/discussions">Forums</a>
            </li>
          </ul>-->

          <li>
            <a href="https://github.com/PyTorchLightning/pytorch-lightning">Github</a>
          </li>

          <li>
            <a href="https://www.grid.ai/">Grid.ai</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })                                                                                         
  </script>

  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PQBQ3CV"
  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->
 </body>
</html>