


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>

  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="google-site-verification" content="okUst94cAlWSsUsGZTB4xSS4UKTtRV8Nu5XZ9pdd3Aw" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Conversational AI &mdash; PyTorch Lightning 1.7.0dev documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/icon.svg"/>
  
  
  
    <link rel="canonical" href="https://pytorch-lightning.readthedocs.io/en/stable//ecosystem/asr_nlp_tts.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../_static/togglebutton.css" type="text/css" />
  <link rel="stylesheet" href="../_static/main.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sphinx_paramlinks.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
  <!-- Google Analytics -->
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-82W25RV60Q"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-82W25RV60Q');
    </script>
  
  <!-- End Google Analytics -->
  

  
  <script src="../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/UCity/UCity-Light.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/UCity/UCity-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/UCity/UCity-Semibold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/Inconsolata/Inconsolata.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <script defer src="https://use.fontawesome.com/releases/v6.1.1/js/all.js" integrity="sha384-xBXmu0dk1bEoiwd71wOonQLyH+VpgR1XcDH3rtxrLww5ajNTuMvBdL5SOiFZnNdp" crossorigin="anonymous"></script>
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch-lightning.rtfd.io/en/latest/" aria-label="PyTorch Lightning">
      <!--  <img class="call-to-action-img" src="../_static/images/logo-lightning-icon.png"/> -->
      </a>

      <div class="main-menu">
        <ul>
          <!-- <li>
            <a href="https://pytorch-lightning.readthedocs.io/en/latest/starter/introduction.html">Get Started</a>
          </li> -->

          <!-- <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch-lightning.readthedocs.io/en/stable/">
                  <span class="dropdown-title">PyTorch Lightning</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://torchmetrics.readthedocs.io/en/stable/">
                  <span class="dropdown-title">TorchMetrics</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning-flash.readthedocs.io/en/stable/">
                  <span class="dropdown-title">Lightning Flash</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning-transformers.readthedocs.io/en/stable/">
                  <span class="dropdown-title">Lightning Transformers</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning-bolts.readthedocs.io/en/stable/">
                  <span class="dropdown-title">Lightning Bolts</span>
                </a>
            </div>
          </li> -->

          <!--<li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://www.pytorchlightning.ai/community">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch-lightning.readthedocs.io/en/latest/#community-examples">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://github.com/PyTorchLightning/pytorch-lightning/discussions" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>-->

          <!-- <li>
            <a href="https://github.com/PyTorchLightning/pytorch-lightning">GitHub</a>
          </li>

          <li>
            <a href="https://www.grid.ai/">Train on the cloud</a>
          </li> -->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  1.7.0dev
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../starter/introduction.html">Lightning in 15 minutes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../starter/converting.html">Organize existing PyTorch into Lightning</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Level Up</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../levels/core_skills.html">Basic skills</a></li>
<li class="toctree-l1"><a class="reference internal" href="../levels/intermediate.html">Intermediate skills</a></li>
<li class="toctree-l1"><a class="reference internal" href="../levels/advanced.html">Advanced skills</a></li>
<li class="toctree-l1"><a class="reference internal" href="../levels/expert.html">Expert skills</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Core API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../common/lightning_module.html">LightningModule</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/trainer.html">Trainer</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Common Workflows</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../common/evaluation.html">Avoid overfitting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model/build_model.html">Build a Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/hyperparameters.html">Configure hyperparameters from the CLI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/progress_bar.html">Customize the progress bar</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/production.html">Deploy models into production</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/training_tricks.html">Effective Training Techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cli/lightning_cli.html">Eliminate config boilerplate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tuning/profiler.html">Find bottlenecks in your code</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/transfer_learning.html">Finetune a model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../visualize/logging_intermediate.html">Manage experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../clouds/cluster.html">Run on an on-prem cluster</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/model_parallel.html">Train 1 trillion+ parameter models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../clouds/cloud_training.html">Train on the cloud</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/checkpointing.html">Save and load model progress</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/precision.html">Save memory with half-precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accelerators/gpu.html">Train on single or multiple GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accelerators/hpu.html">Train on single or multiple HPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accelerators/ipu.html">Train on single or multiple IPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accelerators/tpu.html">Train on single or multiple TPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model/own_your_loop.html">Use a pure PyTorch training loop</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Glossary</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../extensions/accelerator.html">Accelerators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../extensions/callbacks.html">Callback</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/checkpointing.html">Checkpointing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../clouds/cluster.html">Cluster</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/checkpointing_advanced.html">Cloud checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/console_logs.html">Console Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/debugging.html">Debugging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/early_stopping.html">Early stopping</a></li>
<li class="toctree-l1"><a class="reference internal" href="../visualize/experiment_managers.html">Experiment manager (Logger)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../clouds/fault_tolerant_training.html">Fault tolerant training</a></li>
<li class="toctree-l1"><a class="reference external" href="https://lightning-flash.readthedocs.io/en/stable/">Flash</a></li>
<li class="toctree-l1"><a class="reference internal" href="../clouds/cloud_training.html">Grid AI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accelerators/gpu.html">GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/precision.html">Half precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accelerators/hpu.html">HPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/production_intermediate.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accelerators/ipu.html">IPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cli/lightning_cli.html">Lightning CLI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model/build_model_expert.html">Raw PyTorch loop (expert)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model/build_model_expert.html#lightninglite-stepping-stone-to-lightning">LightningLite (Stepping Stone to Lightning)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data/datamodule.html">LightningDataModule</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/lightning_module.html">LightningModule</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch-lightning.readthedocs.io/en/stable/ecosystem/transformers.html">Lightning Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../visualize/loggers.html">Log</a></li>
<li class="toctree-l1"><a class="reference internal" href="../extensions/loops.html">Loops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accelerators/tpu.html">TPU</a></li>
<li class="toctree-l1"><a class="reference external" href="https://torchmetrics.readthedocs.io/en/stable/">Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model/build_model.html">Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/model_parallel.html">Model Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../extensions/plugins.html">Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/progress_bar.html">Progress bar</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/production_advanced.html">Production</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/production_basic.html">Predict</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tuning/profiler.html">Profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/pruning_quantization.html">Pruning and Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/remote_fs.html">Remote filesystem and FSSPEC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/strategy_registry.html">Strategy registry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../starter/style_guide.html">Style guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../clouds/run_intermediate.html">Sweep</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/training_tricks.html">SWA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../clouds/cluster_advanced.html">SLURM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/transfer_learning.html">Transfer learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/trainer.html">Trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../clouds/cluster_intermediate_2.html">Torch distributed</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Hands-on Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.youtube.com/playlist?list=PLaMu-SDt_RB5NUm67hU2pdE75j6KaIOv2">PyTorch Lightning 101 class</a></li>
<li class="toctree-l1"><a class="reference external" href="https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09">From PyTorch to PyTorch Lightning [Blog]</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.youtube.com/watch?v=QHww1JH7IDU">From PyTorch to PyTorch Lightning [Video]</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>Conversational AI</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/ecosystem/asr_nlp_tts.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <section id="conversational-ai">
<h1>Conversational AI<a class="headerlink" href="#conversational-ai" title="Permalink to this headline">¶</a></h1>
<p>These are amazing ecosystems to help with Automatic Speech Recognition (ASR), Natural Language Processing (NLP), and Text to speech (TTS).</p>
<hr class="docutils" />
<section id="nemo">
<h2>NeMo<a class="headerlink" href="#nemo" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://github.com/NVIDIA/NeMo">NVIDIA NeMo</a> is a toolkit for building new State-of-the-Art
Conversational AI models. NeMo has separate collections for Automatic Speech Recognition (ASR),
Natural Language Processing (NLP), and Text-to-Speech (TTS) models. Each collection consists of
prebuilt modules that include everything needed to train on your data.
Every module can easily be customized, extended, and composed to create new Conversational AI
model architectures.</p>
<p>Conversational AI architectures are typically very large and require a lot of data  and compute
for training. NeMo uses PyTorch Lightning for easy and performant multi-GPU/multi-node
mixed-precision training.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Every NeMo model is a LightningModule that comes equipped with all supporting infrastructure for training and reproducibility.</p>
</div>
<hr class="docutils" />
<section id="nemo-models">
<h3>NeMo Models<a class="headerlink" href="#nemo-models" title="Permalink to this headline">¶</a></h3>
<p>NeMo Models contain everything needed to train and reproduce state of the art Conversational AI
research and applications, including:</p>
<ul class="simple">
<li><p>neural network architectures</p></li>
<li><p>datasets/data loaders</p></li>
<li><p>data preprocessing/postprocessing</p></li>
<li><p>data augmentors</p></li>
<li><p>optimizers and schedulers</p></li>
<li><p>tokenizers</p></li>
<li><p>language models</p></li>
</ul>
<p>NeMo uses <a class="reference external" href="https://hydra.cc/">Hydra</a> for configuring both NeMo models and the PyTorch Lightning Trainer.
Depending on the domain and application, many different AI libraries will have to be configured
to build the application. Hydra makes it easy to bring all of these libraries together
so that each can be configured from .yaml or the Hydra CLI.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Every NeMo model has an example configuration file and a corresponding script that contains all configurations needed for training.</p>
</div>
<p>The end result of using NeMo, Pytorch Lightning, and Hydra is that
NeMo models all have the same look and feel. This makes it easy to do Conversational AI research
across multiple domains. NeMo models are also fully compatible with the PyTorch ecosystem.</p>
<section id="installing-nemo">
<h4>Installing NeMo<a class="headerlink" href="#installing-nemo" title="Permalink to this headline">¶</a></h4>
<p>Before installing NeMo, please install Cython first.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install Cython
</pre></div>
</div>
<p>For ASR and TTS models, also install these linux utilities.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>apt-get update <span class="o">&amp;&amp;</span> apt-get install -y libsndfile1 ffmpeg
</pre></div>
</div>
<p>Then installing the latest NeMo release is a simple pip install.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install nemo_toolkit<span class="o">[</span>all<span class="o">]==</span><span class="m">1</span>.0.0b1
</pre></div>
</div>
<p>To install the main branch from GitHub:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python -m pip install git+https://github.com/NVIDIA/NeMo.git@main#egg<span class="o">=</span>nemo_toolkit<span class="o">[</span>all<span class="o">]</span>
</pre></div>
</div>
<p>To install from a local clone of NeMo:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./reinstall.sh <span class="c1"># from cloned NeMo&#39;s git root</span>
</pre></div>
</div>
<p>For Docker users, the NeMo container is available on
<a class="reference external" href="https://ngc.nvidia.com/catalog/containers/nvidia:nemo">NGC</a>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker pull nvcr.io/nvidia/nemo:v1.0.0b1
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker run --runtime<span class="o">=</span>nvidia -it --rm -v --shm-size<span class="o">=</span>8g -p <span class="m">8888</span>:8888 -p <span class="m">6006</span>:6006 --ulimit <span class="nv">memlock</span><span class="o">=</span>-1 --ulimit <span class="nv">stack</span><span class="o">=</span><span class="m">67108864</span> nvcr.io/nvidia/nemo:v1.0.0b1
</pre></div>
</div>
</section>
<section id="experiment-manager">
<h4>Experiment Manager<a class="headerlink" href="#experiment-manager" title="Permalink to this headline">¶</a></h4>
<p>NeMo’s Experiment Manager leverages PyTorch Lightning for model checkpointing,
TensorBoard Logging, and Weights and Biases logging. The Experiment Manager is included by default
in all NeMo example scripts.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">exp_manager</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;exp_manager&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
</pre></div>
</div>
<p>And is configurable via .yaml with Hydra.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>exp_manager:
    exp_dir: null
    name: *name
    create_tensorboard_logger: True
    create_checkpoint_callback: True
</pre></div>
</div>
<p>Optionally launch Tensorboard to view training results in ./nemo_experiments (by default).</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>tensorboard --bind_all --logdir nemo_experiments
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="automatic-speech-recognition-asr">
<h3>Automatic Speech Recognition (ASR)<a class="headerlink" href="#automatic-speech-recognition-asr" title="Permalink to this headline">¶</a></h3>
<p>Everything needed to train Convolutional ASR models is included with NeMo.
NeMo supports multiple Speech Recognition architectures, including Jasper and QuartzNet.
<a class="reference external" href="https://ngc.nvidia.com/catalog/models/nvidia:nemospeechmodels">NeMo Speech Models</a>
can be trained from scratch on custom datasets or
fine-tuned using pre-trained checkpoints trained on thousands of hours of audio
that can be restored for immediate use.</p>
<p>Some typical ASR tasks are included with NeMo:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/NVIDIA/NeMo/blob/v1.0.0b1/tutorials/asr/01_ASR_with_NeMo.ipynb">Audio transcription</a></p></li>
<li><p><a class="reference external" href="https://github.com/NVIDIA/NeMo/blob/v1.0.0b1/examples/asr/speech_to_text_bpe.py">Byte Pair/Word Piece Training</a></p></li>
<li><p><a class="reference external" href="https://github.com/NVIDIA/NeMo/blob/v1.0.0b1/tutorials/asr/03_Speech_Commands.ipynb">Speech Commands</a></p></li>
<li><p><a class="reference external" href="https://github.com/NVIDIA/NeMo/blob/v1.0.0b1/tutorials/asr/06_Voice_Activiy_Detection.ipynb">Voice Activity Detection</a></p></li>
<li><p><a class="reference external" href="https://github.com/NVIDIA/NeMo/blob/v1.0.0b1/examples/speaker_recognition/speaker_reco.py">Speaker Recognition</a></p></li>
</ul>
<p>See this <a class="reference external" href="https://github.com/NVIDIA/NeMo/blob/v1.0.0b1/tutorials/asr/01_ASR_with_NeMo.ipynb">asr notebook</a>
for a full tutorial on doing ASR with NeMo, PyTorch Lightning, and Hydra.</p>
<section id="specify-asr-model-configurations-with-yaml-file">
<h4>Specify ASR Model Configurations with YAML File<a class="headerlink" href="#specify-asr-model-configurations-with-yaml-file" title="Permalink to this headline">¶</a></h4>
<p>NeMo Models and the PyTorch Lightning Trainer can be fully configured from .yaml files using Hydra.</p>
<p>See this <a class="reference external" href="https://github.com/NVIDIA/NeMo/blob/v1.0.0b1/examples/asr/conf/config.yaml">asr config</a>
for the entire speech to text .yaml file.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># configure the PyTorch Lightning Trainer</span><span class="w"></span>
<span class="nt">trainer</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">gpus</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span><span class="w"> </span><span class="c1"># number of gpus</span><span class="w"></span>
<span class="w">    </span><span class="nt">max_epochs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span><span class="w"></span>
<span class="w">    </span><span class="nt">max_steps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span><span class="w"> </span><span class="c1"># computed at runtime if not set</span><span class="w"></span>
<span class="w">    </span><span class="nt">num_nodes</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"></span>
<span class="w">    </span><span class="nt">accelerator</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ddp</span><span class="w"></span>
<span class="w">    </span><span class="l l-Scalar l-Scalar-Plain">...</span><span class="w"></span>
<span class="c1"># configure the ASR model</span><span class="w"></span>
<span class="nt">model</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="l l-Scalar l-Scalar-Plain">...</span><span class="w"></span>
<span class="w">    </span><span class="l l-Scalar l-Scalar-Plain">encoder</span><span class="p p-Indicator">:</span><span class="w"></span>
<span class="w">        </span><span class="nt">cls</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nemo.collections.asr.modules.ConvASREncoder</span><span class="w"></span>
<span class="w">        </span><span class="nt">params</span><span class="p">:</span><span class="w"></span>
<span class="w">            </span><span class="nt">feat_in</span><span class="p">:</span><span class="w"> </span><span class="nv">*n_mels</span><span class="w"></span>
<span class="w">            </span><span class="nt">activation</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">relu</span><span class="w"></span>
<span class="w">            </span><span class="nt">conv_mask</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span><span class="w"></span>

<span class="w">        </span><span class="nt">jasper</span><span class="p">:</span><span class="w"></span>
<span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">filters</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">128</span><span class="w"></span>
<span class="w">            </span><span class="nt">repeat</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"></span>
<span class="w">            </span><span class="nt">kernel</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">11</span><span class="p p-Indicator">]</span><span class="w"></span>
<span class="w">            </span><span class="nt">stride</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">1</span><span class="p p-Indicator">]</span><span class="w"></span>
<span class="w">            </span><span class="nt">dilation</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">1</span><span class="p p-Indicator">]</span><span class="w"></span>
<span class="w">            </span><span class="nt">dropout</span><span class="p">:</span><span class="w"> </span><span class="nv">*dropout</span><span class="w"></span>
<span class="w">            </span><span class="l l-Scalar l-Scalar-Plain">...</span><span class="w"></span>
<span class="w">    </span><span class="c1"># all other configuration, data, optimizer, preprocessor, etc</span><span class="w"></span>
<span class="w">    </span><span class="l l-Scalar l-Scalar-Plain">...</span><span class="w"></span>
</pre></div>
</div>
</section>
<section id="developing-asr-model-from-scratch">
<h4>Developing ASR Model From Scratch<a class="headerlink" href="#developing-asr-model-from-scratch" title="Permalink to this headline">¶</a></h4>
<p><a class="reference external" href="https://github.com/NVIDIA/NeMo/blob/v1.0.0b1/examples/asr/speech_to_text.py">speech_to_text.py</a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># hydra_runner calls hydra.main and is useful for multi-node experiments</span>
<span class="nd">@hydra_runner</span><span class="p">(</span><span class="n">config_path</span><span class="o">=</span><span class="s2">&quot;conf&quot;</span><span class="p">,</span> <span class="n">config_name</span><span class="o">=</span><span class="s2">&quot;config&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">cfg</span><span class="p">):</span>
    <span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="o">**</span><span class="n">cfg</span><span class="o">.</span><span class="n">trainer</span><span class="p">)</span>
    <span class="n">asr_model</span> <span class="o">=</span> <span class="n">EncDecCTCModel</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">trainer</span><span class="p">)</span>
    <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">asr_model</span><span class="p">)</span>
</pre></div>
</div>
<p>Hydra makes every aspect of the NeMo model,
including the PyTorch Lightning Trainer, customizable from the command line.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python NeMo/examples/asr/speech_to_text.py --config-name<span class="o">=</span>quartznet_15x5 <span class="se">\</span>
    trainer.accelerator<span class="o">=</span>gpu <span class="se">\</span>
    trainer.devices<span class="o">=</span><span class="m">4</span> <span class="se">\</span>
    trainer.max_epochs<span class="o">=</span><span class="m">128</span> <span class="se">\</span>
    +trainer.precision<span class="o">=</span><span class="m">16</span> <span class="se">\</span>
    model.train_ds.manifest_filepath<span class="o">=</span>&lt;PATH_TO_DATA&gt;/librispeech-train-all.json <span class="se">\</span>
    model.validation_ds.manifest_filepath<span class="o">=</span>&lt;PATH_TO_DATA&gt;/librispeech-dev-other.json <span class="se">\</span>
    model.train_ds.batch_size<span class="o">=</span><span class="m">64</span> <span class="se">\</span>
    +model.validation_ds.num_workers<span class="o">=</span><span class="m">16</span> <span class="se">\</span>
    +model.train_ds.num_workers<span class="o">=</span><span class="m">16</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Training NeMo ASR models can take days/weeks so it is highly recommended to use multiple GPUs and multiple nodes with the PyTorch Lightning Trainer.</p>
</div>
</section>
<section id="using-state-of-the-art-pre-trained-asr-model">
<h4>Using State-Of-The-Art Pre-trained ASR Model<a class="headerlink" href="#using-state-of-the-art-pre-trained-asr-model" title="Permalink to this headline">¶</a></h4>
<p>Transcribe audio with QuartzNet model pretrained on ~3300 hours of audio.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">quartznet</span> <span class="o">=</span> <span class="n">EncDecCTCModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;QuartzNet15x5Base-En&quot;</span><span class="p">)</span>

<span class="n">files</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;path/to/my.wav&quot;</span><span class="p">]</span>  <span class="c1"># file duration should be less than 25 seconds</span>

<span class="k">for</span> <span class="n">fname</span><span class="p">,</span> <span class="n">transcription</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">files</span><span class="p">,</span> <span class="n">quartznet</span><span class="o">.</span><span class="n">transcribe</span><span class="p">(</span><span class="n">paths2audio_files</span><span class="o">=</span><span class="n">files</span><span class="p">)):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Audio in </span><span class="si">{</span><span class="n">fname</span><span class="si">}</span><span class="s2"> was recognized as: </span><span class="si">{</span><span class="n">transcription</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>To see the available pretrained checkpoints:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">EncDecCTCModel</span><span class="o">.</span><span class="n">list_available_models</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="nemo-asr-model-under-the-hood">
<h4>NeMo ASR Model Under the Hood<a class="headerlink" href="#nemo-asr-model-under-the-hood" title="Permalink to this headline">¶</a></h4>
<p>Any aspect of ASR training or model architecture design can easily be customized
with PyTorch Lightning since every NeMo model is a Lightning Module.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">EncDecCTCModel</span><span class="p">(</span><span class="n">ASRModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Base class for encoder decoder CTC-based models.&quot;&quot;&quot;</span>

    <span class="o">...</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_signal</span><span class="p">,</span> <span class="n">input_signal_length</span><span class="p">):</span>
        <span class="n">processed_signal</span><span class="p">,</span> <span class="n">processed_signal_len</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">preprocessor</span><span class="p">(</span>
            <span class="n">input_signal</span><span class="o">=</span><span class="n">input_signal</span><span class="p">,</span>
            <span class="n">length</span><span class="o">=</span><span class="n">input_signal_length</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># Spec augment is not applied during evaluation/testing</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">spec_augmentation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="n">processed_signal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">spec_augmentation</span><span class="p">(</span><span class="n">input_spec</span><span class="o">=</span><span class="n">processed_signal</span><span class="p">)</span>
        <span class="n">encoded</span><span class="p">,</span> <span class="n">encoded_len</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">audio_signal</span><span class="o">=</span><span class="n">processed_signal</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="n">processed_signal_len</span><span class="p">)</span>
        <span class="n">log_probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">encoder_output</span><span class="o">=</span><span class="n">encoded</span><span class="p">)</span>
        <span class="n">greedy_predictions</span> <span class="o">=</span> <span class="n">log_probs</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">log_probs</span><span class="p">,</span> <span class="n">encoded_len</span><span class="p">,</span> <span class="n">greedy_predictions</span>

    <span class="c1"># PTL-specific methods</span>
    <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_nb</span><span class="p">):</span>
        <span class="n">audio_signal</span><span class="p">,</span> <span class="n">audio_signal_len</span><span class="p">,</span> <span class="n">transcript</span><span class="p">,</span> <span class="n">transcript_len</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="n">log_probs</span><span class="p">,</span> <span class="n">encoded_len</span><span class="p">,</span> <span class="n">predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
            <span class="n">input_signal</span><span class="o">=</span><span class="n">audio_signal</span><span class="p">,</span> <span class="n">input_signal_length</span><span class="o">=</span><span class="n">audio_signal_len</span>
        <span class="p">)</span>
        <span class="n">loss_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span>
            <span class="n">log_probs</span><span class="o">=</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="n">transcript</span><span class="p">,</span> <span class="n">input_lengths</span><span class="o">=</span><span class="n">encoded_len</span><span class="p">,</span> <span class="n">target_lengths</span><span class="o">=</span><span class="n">transcript_len</span>
        <span class="p">)</span>
        <span class="n">wer_num</span><span class="p">,</span> <span class="n">wer_denom</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_wer</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">transcript</span><span class="p">,</span> <span class="n">transcript_len</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="s2">&quot;train_loss&quot;</span><span class="p">:</span> <span class="n">loss_value</span><span class="p">,</span>
                <span class="s2">&quot;training_batch_wer&quot;</span><span class="p">:</span> <span class="n">wer_num</span> <span class="o">/</span> <span class="n">wer_denom</span><span class="p">,</span>
                <span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;lr&quot;</span><span class="p">],</span>
            <span class="p">}</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">loss_value</span>
</pre></div>
</div>
</section>
<section id="neural-types-in-nemo-asr">
<h4>Neural Types in NeMo ASR<a class="headerlink" href="#neural-types-in-nemo-asr" title="Permalink to this headline">¶</a></h4>
<p>NeMo Models and Neural Modules come with Neural Type checking.
Neural type checking is extremely useful when combining many different neural
network architectures for a production-grade application.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">input_types</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">NeuralType</span><span class="p">]]:</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">preprocessor</span><span class="p">,</span> <span class="s2">&quot;_sample_rate&quot;</span><span class="p">):</span>
        <span class="n">audio_eltype</span> <span class="o">=</span> <span class="n">AudioSignal</span><span class="p">(</span><span class="n">freq</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">preprocessor</span><span class="o">.</span><span class="n">_sample_rate</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">audio_eltype</span> <span class="o">=</span> <span class="n">AudioSignal</span><span class="p">()</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;input_signal&quot;</span><span class="p">:</span> <span class="n">NeuralType</span><span class="p">((</span><span class="s2">&quot;B&quot;</span><span class="p">,</span> <span class="s2">&quot;T&quot;</span><span class="p">),</span> <span class="n">audio_eltype</span><span class="p">),</span>
        <span class="s2">&quot;input_signal_length&quot;</span><span class="p">:</span> <span class="n">NeuralType</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="s2">&quot;B&quot;</span><span class="p">),</span> <span class="n">LengthsType</span><span class="p">()),</span>
    <span class="p">}</span>


<span class="nd">@property</span>
<span class="k">def</span> <span class="nf">output_types</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">NeuralType</span><span class="p">]]:</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;outputs&quot;</span><span class="p">:</span> <span class="n">NeuralType</span><span class="p">((</span><span class="s2">&quot;B&quot;</span><span class="p">,</span> <span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="s2">&quot;D&quot;</span><span class="p">),</span> <span class="n">LogprobsType</span><span class="p">()),</span>
        <span class="s2">&quot;encoded_lengths&quot;</span><span class="p">:</span> <span class="n">NeuralType</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="s2">&quot;B&quot;</span><span class="p">),</span> <span class="n">LengthsType</span><span class="p">()),</span>
        <span class="s2">&quot;greedy_predictions&quot;</span><span class="p">:</span> <span class="n">NeuralType</span><span class="p">((</span><span class="s2">&quot;B&quot;</span><span class="p">,</span> <span class="s2">&quot;T&quot;</span><span class="p">),</span> <span class="n">LabelsType</span><span class="p">()),</span>
    <span class="p">}</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="natural-language-processing-nlp">
<h3>Natural Language Processing (NLP)<a class="headerlink" href="#natural-language-processing-nlp" title="Permalink to this headline">¶</a></h3>
<p>Everything needed to finetune BERT-like language models for NLP tasks is included with NeMo.
<a class="reference external" href="https://ngc.nvidia.com/catalog/models/nvidia:nemonlpmodels">NeMo NLP Models</a>
include <a class="reference external" href="https://github.com/huggingface/transformers">HuggingFace Transformers</a>
and <a class="reference external" href="https://github.com/NVIDIA/Megatron-LM">NVIDIA Megatron-LM</a> BERT and Bio-Megatron models.
NeMo can also be used for pretraining BERT-based language models from HuggingFace.</p>
<p>Any of the HuggingFace encoders or Megatron-LM encoders can easily be used for the NLP tasks
that are included with NeMo:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/NVIDIA/NeMo/blob/v1.0.0b1/tutorials/nlp/GLUE_Benchmark.ipynb">Glue Benchmark (All tasks)</a></p></li>
<li><p><a class="reference external" href="https://github.com/NVIDIA/NeMo/tree/v1.0.0b1/examples/nlp/intent_slot_classification">Intent Slot Classification</a></p></li>
<li><p><a class="reference external" href="https://github.com/NVIDIA/NeMo/blob/v1.0.0b1/tutorials/nlp/01_Pretrained_Language_Models_for_Downstream_Tasks.ipynb">Language Modeling (BERT Pretraining)</a></p></li>
<li><p><a class="reference external" href="https://github.com/NVIDIA/NeMo/blob/v1.0.0b1/tutorials/nlp/Question_Answering_Squad.ipynb">Question Answering</a></p></li>
<li><p><a class="reference external" href="https://github.com/NVIDIA/NeMo/tree/v1.0.0b1/examples/nlp/text_classification">Text Classification</a> (including Sentiment Analysis)</p></li>
<li><p><a class="reference external" href="https://github.com/NVIDIA/NeMo/tree/v1.0.0b1/examples/nlp/token_classification">Token Classification</a> (including Named Entity Recognition)</p></li>
<li><p><a class="reference external" href="https://github.com/NVIDIA/NeMo/blob/v1.0.0b1/tutorials/nlp/Punctuation_and_Capitalization.ipynb">Punctuation and Capitalization</a></p></li>
</ul>
<section id="named-entity-recognition-ner">
<h4>Named Entity Recognition (NER)<a class="headerlink" href="#named-entity-recognition-ner" title="Permalink to this headline">¶</a></h4>
<p>NER (or more generally token classification) is the NLP task of detecting and classifying key information (entities) in text.
This task is very popular in Healthcare and Finance. In finance, for example, it can be important to identify
geographical, geopolitical, organizational, persons, events, and natural phenomenon entities.
See this <a class="reference external" href="https://github.com/NVIDIA/NeMo/blob/v1.0.0b1/tutorials/nlp/Token_Classification_Named_Entity_Recognition.ipynb">NER notebook</a>
for a full tutorial on doing NER with NeMo, PyTorch Lightning, and Hydra.</p>
</section>
<section id="specify-ner-model-configurations-with-yaml-file">
<h4>Specify NER Model Configurations with YAML File<a class="headerlink" href="#specify-ner-model-configurations-with-yaml-file" title="Permalink to this headline">¶</a></h4>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>NeMo Models and the PyTorch Lightning Trainer can be fully configured from .yaml files using Hydra.</p>
</div>
<p>See this <a class="reference external" href="https://github.com/NVIDIA/NeMo/blob/v1.0.0b1/examples/nlp/token_classification/conf/token_classification_config.yaml">token classification config</a>
for the entire NER (token classification) .yaml file.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># configure any argument of the PyTorch Lightning Trainer</span><span class="w"></span>
<span class="nt">trainer</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">gpus</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"> </span><span class="c1"># the number of gpus, 0 for CPU</span><span class="w"></span>
<span class="w">    </span><span class="nt">num_nodes</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"></span>
<span class="w">    </span><span class="nt">max_epochs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span><span class="w"></span>
<span class="w">    </span><span class="l l-Scalar l-Scalar-Plain">...</span><span class="w"></span>
<span class="c1"># configure any aspect of the token classification model here</span><span class="w"></span>
<span class="nt">model</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">dataset</span><span class="p">:</span><span class="w"></span>
<span class="w">        </span><span class="nt">data_dir</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">???</span><span class="w"> </span><span class="c1"># /path/to/data</span><span class="w"></span>
<span class="w">        </span><span class="nt">class_balancing</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span><span class="w"> </span><span class="c1"># choose from [null, weighted_loss]. Weighted_loss enables the weighted class balancing of the loss, may be used for handling unbalanced classes</span><span class="w"></span>
<span class="w">        </span><span class="nt">max_seq_length</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">128</span><span class="w"></span>
<span class="w">        </span><span class="l l-Scalar l-Scalar-Plain">...</span><span class="w"></span>
<span class="w w-Error">  </span><span class="nt">tokenizer</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">tokenizer_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${model.language_model.pretrained_model_name}</span><span class="w"> </span><span class="c1"># or sentencepiece</span><span class="w"></span>
<span class="w">    </span><span class="nt">vocab_file</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span><span class="w"> </span><span class="c1"># path to vocab file</span><span class="w"></span>
<span class="w">    </span><span class="l l-Scalar l-Scalar-Plain">...</span><span class="w"></span>
<span class="c1"># the language model can be from HuggingFace or Megatron-LM</span><span class="w"></span>
<span class="nt">language_model</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">pretrained_model_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">bert-base-uncased</span><span class="w"></span>
<span class="w">    </span><span class="nt">lm_checkpoint</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span><span class="w"></span>
<span class="w">    </span><span class="l l-Scalar l-Scalar-Plain">...</span><span class="w"></span>
<span class="c1"># the classifier for the downstream task</span><span class="w"></span>
<span class="w w-Error">  </span><span class="nt">head</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">num_fc_layers</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span><span class="w"></span>
<span class="w">    </span><span class="nt">fc_dropout</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.5</span><span class="w"></span>
<span class="w">    </span><span class="nt">activation</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;relu&#39;</span><span class="w"></span>
<span class="w">    </span><span class="l l-Scalar l-Scalar-Plain">...</span><span class="w"></span>
<span class="c1"># all other configuration: train/val/test/ data, optimizer, experiment manager, etc</span><span class="w"></span>
<span class="nn">...</span><span class="w"></span>
</pre></div>
</div>
</section>
<section id="developing-ner-model-from-scratch">
<h4>Developing NER Model From Scratch<a class="headerlink" href="#developing-ner-model-from-scratch" title="Permalink to this headline">¶</a></h4>
<p><a class="reference external" href="https://github.com/NVIDIA/NeMo/blob/v1.0.0b1/examples/nlp/token_classification/token_classification.py">token_classification.py</a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># hydra_runner calls hydra.main and is useful for multi-node experiments</span>
<span class="nd">@hydra_runner</span><span class="p">(</span><span class="n">config_path</span><span class="o">=</span><span class="s2">&quot;conf&quot;</span><span class="p">,</span> <span class="n">config_name</span><span class="o">=</span><span class="s2">&quot;token_classification_config&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">cfg</span><span class="p">:</span> <span class="n">DictConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">trainer</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span><span class="o">**</span><span class="n">cfg</span><span class="o">.</span><span class="n">trainer</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">TokenClassificationModel</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">trainer</span><span class="o">=</span><span class="n">trainer</span><span class="p">)</span>
    <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<p>After training, we can do inference with the saved NER model using PyTorch Lightning.</p>
<p>Inference from file:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">gpu</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">cfg</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">gpus</span> <span class="o">!=</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="n">gpu</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">set_trainer</span><span class="p">(</span><span class="n">trainer</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">evaluate_from_file</span><span class="p">(</span>
    <span class="n">text_file</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">data_dir</span><span class="p">,</span> <span class="n">cfg</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">validation_ds</span><span class="o">.</span><span class="n">text_file</span><span class="p">),</span>
    <span class="n">labels_file</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">data_dir</span><span class="p">,</span> <span class="n">cfg</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">validation_ds</span><span class="o">.</span><span class="n">labels_file</span><span class="p">),</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="n">exp_dir</span><span class="p">,</span>
    <span class="n">add_confusion_matrix</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">normalize_confusion_matrix</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Or we can run inference on a few examples:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">queries</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;we bought four shirts from the nvidia gear store in santa clara.&quot;</span><span class="p">,</span> <span class="s2">&quot;Nvidia is a company in Santa Clara.&quot;</span><span class="p">]</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">add_predictions</span><span class="p">(</span><span class="n">queries</span><span class="p">)</span>

<span class="k">for</span> <span class="n">query</span><span class="p">,</span> <span class="n">result</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">queries</span><span class="p">,</span> <span class="n">results</span><span class="p">):</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Query : </span><span class="si">{</span><span class="n">query</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Result: </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Hydra makes every aspect of the NeMo model, including the PyTorch Lightning Trainer, customizable from the command line.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python token_classification.py <span class="se">\</span>
    model.language_model.pretrained_model_name<span class="o">=</span>bert-base-cased <span class="se">\</span>
    model.head.num_fc_layers<span class="o">=</span><span class="m">2</span> <span class="se">\</span>
    model.dataset.data_dir<span class="o">=</span>/path/to/my/data  <span class="se">\</span>
    trainer.max_epochs<span class="o">=</span><span class="m">5</span> <span class="se">\</span>
    trainer.accelerator<span class="o">=</span>gpu <span class="se">\</span>
    trainer.devices<span class="o">=[</span><span class="m">0</span>,1<span class="o">]</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="tokenizers">
<h4>Tokenizers<a class="headerlink" href="#tokenizers" title="Permalink to this headline">¶</a></h4>
<p>Tokenization is the process of converting natural language text into integer arrays
which can be used for machine learning.
For NLP tasks, tokenization is an essential part of data preprocessing.
NeMo supports all BERT-like model tokenizers from
<a class="reference external" href="https://huggingface.co/transformers/model_doc/auto.html#autotokenizer">HuggingFace’s AutoTokenizer</a>
and also supports <a class="reference external" href="https://github.com/google/sentencepiece">Google’s SentencePieceTokenizer</a>
which can be trained on custom data.</p>
<p>To see the list of supported tokenizers:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nemo.collections</span> <span class="kn">import</span> <span class="n">nlp</span> <span class="k">as</span> <span class="n">nemo_nlp</span>

<span class="n">nemo_nlp</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">get_tokenizer_list</span><span class="p">()</span>
</pre></div>
</div>
<p>See this <a class="reference external" href="https://github.com/NVIDIA/NeMo/blob/v1.0.0b1/tutorials/nlp/02_NLP_Tokenizers.ipynb">tokenizer notebook</a>
for a full tutorial on using tokenizers in NeMo.</p>
</section>
<section id="language-models">
<h4>Language Models<a class="headerlink" href="#language-models" title="Permalink to this headline">¶</a></h4>
<p>Language models are used to extract information from (tokenized) text.
Much of the state-of-the-art in natural language processing is achieved
by fine-tuning pretrained language models on the downstream task.</p>
<p>With NeMo, you can either <a class="reference external" href="https://github.com/NVIDIA/NeMo/blob/v1.0.0b1/examples/nlp/language_modeling/bert_pretraining.py">pretrain</a>
a BERT model on your data or use a pretrained language model from <a class="reference external" href="https://github.com/huggingface/transformers">HuggingFace Transformers</a>
or <a class="reference external" href="https://github.com/NVIDIA/Megatron-LM">NVIDIA Megatron-LM</a>.</p>
<p>To see the list of language models available in NeMo:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nemo_nlp</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">get_pretrained_lm_models_list</span><span class="p">(</span><span class="n">include_external</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>Easily switch between any language model in the above list by using <cite>.get_lm_model</cite>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nemo_nlp</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">get_lm_model</span><span class="p">(</span><span class="n">pretrained_model_name</span><span class="o">=</span><span class="s2">&quot;distilbert-base-uncased&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>See this <a class="reference external" href="https://github.com/NVIDIA/NeMo/blob/v1.0.0b1/tutorials/nlp/01_Pretrained_Language_Models_for_Downstream_Tasks.ipynb">language model notebook</a>
for a full tutorial on using pretrained language models in NeMo.</p>
</section>
<section id="using-a-pre-trained-ner-model">
<h4>Using a Pre-trained NER Model<a class="headerlink" href="#using-a-pre-trained-ner-model" title="Permalink to this headline">¶</a></h4>
<p>NeMo has pre-trained NER models that can be used
to get started with Token Classification right away.
Models are automatically downloaded from NGC,
cached locally to disk,
and loaded into GPU memory using the <cite>.from_pretrained</cite> method.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># load pre-trained NER model</span>
<span class="n">pretrained_ner_model</span> <span class="o">=</span> <span class="n">TokenClassificationModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;NERModel&quot;</span><span class="p">)</span>

<span class="c1"># define the list of queries for inference</span>
<span class="n">queries</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;we bought four shirts from the nvidia gear store in santa clara.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Nvidia is a company.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;The Adventures of Tom Sawyer by Mark Twain is an 1876 novel about a young boy growing &quot;</span>
    <span class="o">+</span> <span class="s2">&quot;up along the Mississippi River.&quot;</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">pretrained_ner_model</span><span class="o">.</span><span class="n">add_predictions</span><span class="p">(</span><span class="n">queries</span><span class="p">)</span>

<span class="k">for</span> <span class="n">query</span><span class="p">,</span> <span class="n">result</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">queries</span><span class="p">,</span> <span class="n">results</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Query : </span><span class="si">{</span><span class="n">query</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Result: </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="nemo-ner-model-under-the-hood">
<h4>NeMo NER Model Under the Hood<a class="headerlink" href="#nemo-ner-model-under-the-hood" title="Permalink to this headline">¶</a></h4>
<p>Any aspect of NLP training or model architecture design can easily be customized with PyTorch Lightning
since every NeMo model is a Lightning Module.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">TokenClassificationModel</span><span class="p">(</span><span class="n">ModelPT</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Token Classification Model with BERT, applicable for tasks such as Named Entity Recognition</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="o">...</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">token_type_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">):</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bert_model</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">token_type_ids</span><span class="o">=</span><span class="n">token_type_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span>
        <span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits</span>

    <span class="c1"># PTL-specific methods</span>
    <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Lightning calls this inside the training loop with the data from the training dataloader</span>
<span class="sd">        passed in as `batch`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">input_ids</span><span class="p">,</span> <span class="n">input_type_ids</span><span class="p">,</span> <span class="n">input_mask</span><span class="p">,</span> <span class="n">subtokens_mask</span><span class="p">,</span> <span class="n">loss_mask</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">token_type_ids</span><span class="o">=</span><span class="n">input_type_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">input_mask</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">loss_mask</span><span class="o">=</span><span class="n">loss_mask</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">({</span><span class="s2">&quot;train_loss&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;lr&quot;</span><span class="p">]})</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="o">...</span>
</pre></div>
</div>
</section>
<section id="neural-types-in-nemo-nlp">
<h4>Neural Types in NeMo NLP<a class="headerlink" href="#neural-types-in-nemo-nlp" title="Permalink to this headline">¶</a></h4>
<p>NeMo Models and Neural Modules come with Neural Type checking.
Neural type checking is extremely useful when combining many different neural network architectures
for a production-grade application.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">input_types</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">NeuralType</span><span class="p">]]:</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">bert_model</span><span class="o">.</span><span class="n">input_types</span>


<span class="nd">@property</span>
<span class="k">def</span> <span class="nf">output_types</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">NeuralType</span><span class="p">]]:</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="o">.</span><span class="n">output_types</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="text-to-speech-tts">
<h3>Text-To-Speech (TTS)<a class="headerlink" href="#text-to-speech-tts" title="Permalink to this headline">¶</a></h3>
<p>Everything needed to train TTS models and generate audio is included with NeMo.
<a class="reference external" href="https://ngc.nvidia.com/catalog/models/nvidia:nemottsmodels">NeMo TTS Models</a>
can be trained from scratch on your own data or pretrained models can be downloaded
automatically. NeMo currently supports  a two step inference procedure.
First, a model is used to generate a mel spectrogram from text.
Second, a model is used to generate audio from a mel spectrogram.</p>
<p>Mel Spectrogram Generators:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/NVIDIA/NeMo/blob/v1.0.0b1/examples/tts/tacotron2.py">Tacotron 2</a></p></li>
<li><p><a class="reference external" href="https://github.com/NVIDIA/NeMo/blob/v1.0.0b1/examples/tts/glow_tts.py">Glow-TTS</a></p></li>
</ul>
<p>Audio Generators:</p>
<ul class="simple">
<li><p>Griffin-Lim</p></li>
<li><p><a class="reference external" href="https://github.com/NVIDIA/NeMo/blob/v1.0.0b1/examples/tts/waveglow.py">WaveGlow</a></p></li>
<li><p><a class="reference external" href="https://github.com/NVIDIA/NeMo/blob/v1.0.0b1/examples/tts/squeezewave.py">SqueezeWave</a></p></li>
</ul>
<section id="specify-tts-model-configurations-with-yaml-file">
<h4>Specify TTS Model Configurations with YAML File<a class="headerlink" href="#specify-tts-model-configurations-with-yaml-file" title="Permalink to this headline">¶</a></h4>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>NeMo Models and PyTorch Lightning Trainer can be fully configured from .yaml files using Hydra.</p>
</div>
<p><a class="reference external" href="https://github.com/NVIDIA/NeMo/blob/v1.0.0b1/examples/tts/conf/glow_tts.yaml">tts/conf/glow_tts.yaml</a></p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># configure the PyTorch Lightning Trainer</span><span class="w"></span>
<span class="nt">trainer</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">gpus</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">-1</span><span class="w"> </span><span class="c1"># number of gpus</span><span class="w"></span>
<span class="w">    </span><span class="nt">max_epochs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">350</span><span class="w"></span>
<span class="w">    </span><span class="nt">num_nodes</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"></span>
<span class="w">    </span><span class="nt">accelerator</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ddp</span><span class="w"></span>
<span class="w">    </span><span class="l l-Scalar l-Scalar-Plain">...</span><span class="w"></span>

<span class="c1"># configure the TTS model</span><span class="w"></span>
<span class="nt">model</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="l l-Scalar l-Scalar-Plain">...</span><span class="w"></span>
<span class="w">    </span><span class="l l-Scalar l-Scalar-Plain">encoder</span><span class="p p-Indicator">:</span><span class="w"></span>
<span class="w">        </span><span class="nt">cls</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nemo.collections.tts.modules.glow_tts.TextEncoder</span><span class="w"></span>
<span class="w">            </span><span class="l l-Scalar l-Scalar-Plain">params</span><span class="p p-Indicator">:</span><span class="w"></span>
<span class="w">            </span><span class="nt">n_vocab</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">148</span><span class="w"></span>
<span class="w">            </span><span class="nt">out_channels</span><span class="p">:</span><span class="w"> </span><span class="nv">*n_mels</span><span class="w"></span>
<span class="w">            </span><span class="nt">hidden_channels</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">192</span><span class="w"></span>
<span class="w">            </span><span class="nt">filter_channels</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">768</span><span class="w"></span>
<span class="w">            </span><span class="nt">filter_channels_dp</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">256</span><span class="w"></span>
<span class="w">            </span><span class="l l-Scalar l-Scalar-Plain">...</span><span class="w"></span>
<span class="c1"># all other configuration, data, optimizer, parser, preprocessor, etc</span><span class="w"></span>
<span class="nn">...</span><span class="w"></span>
</pre></div>
</div>
</section>
<section id="developing-tts-model-from-scratch">
<h4>Developing TTS Model From Scratch<a class="headerlink" href="#developing-tts-model-from-scratch" title="Permalink to this headline">¶</a></h4>
<p><a class="reference external" href="https://github.com/NVIDIA/NeMo/blob/v1.0.0b1/examples/tts/glow_tts.py">tts/glow_tts.py</a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># hydra_runner calls hydra.main and is useful for multi-node experiments</span>
<span class="nd">@hydra_runner</span><span class="p">(</span><span class="n">config_path</span><span class="o">=</span><span class="s2">&quot;conf&quot;</span><span class="p">,</span> <span class="n">config_name</span><span class="o">=</span><span class="s2">&quot;glow_tts&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">cfg</span><span class="p">):</span>
    <span class="n">trainer</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span><span class="o">**</span><span class="n">cfg</span><span class="o">.</span><span class="n">trainer</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">GlowTTSModel</span><span class="p">(</span><span class="n">cfg</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">trainer</span><span class="o">=</span><span class="n">trainer</span><span class="p">)</span>
    <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<p>Hydra makes every aspect of the NeMo model, including the PyTorch Lightning Trainer, customizable from the command line.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python NeMo/examples/tts/glow_tts.py <span class="se">\</span>
    trainer.accelerator<span class="o">=</span>gpu <span class="se">\</span>
    trainer.devices<span class="o">=</span><span class="m">4</span> <span class="se">\</span>
    trainer.max_epochs<span class="o">=</span><span class="m">400</span> <span class="se">\</span>
    ...
    <span class="nv">train_dataset</span><span class="o">=</span>/path/to/train/data <span class="se">\</span>
    <span class="nv">validation_datasets</span><span class="o">=</span>/path/to/val/data <span class="se">\</span>
    model.train_ds.batch_size <span class="o">=</span> <span class="m">64</span> <span class="se">\</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Training NeMo TTS models from scratch can take days or weeks so it is highly recommended to use multiple GPUs and multiple nodes with the PyTorch Lightning Trainer.</p>
</div>
</section>
<section id="using-state-of-the-art-pre-trained-tts-model">
<h4>Using State-Of-The-Art Pre-trained TTS Model<a class="headerlink" href="#using-state-of-the-art-pre-trained-tts-model" title="Permalink to this headline">¶</a></h4>
<p>Generate speech using models trained on <cite>LJSpeech &lt;https://keithito.com/LJ-Speech-Dataset/&gt;</cite>,
around 24 hours of single speaker data.</p>
<p>See this <a class="reference external" href="https://github.com/NVIDIA/NeMo/blob/v1.0.0b1/tutorials/tts/1_TTS_inference.ipynb">TTS notebook</a>
for a full tutorial on generating speech with NeMo, PyTorch Lightning, and Hydra.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># load pretrained spectrogram model</span>
<span class="n">spec_gen</span> <span class="o">=</span> <span class="n">SpecModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;GlowTTS-22050Hz&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

<span class="c1"># load pretrained Generators</span>
<span class="n">vocoder</span> <span class="o">=</span> <span class="n">WaveGlowModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;WaveGlow-22050Hz&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">infer</span><span class="p">(</span><span class="n">spec_gen_model</span><span class="p">,</span> <span class="n">vocder_model</span><span class="p">,</span> <span class="n">str_input</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">parsed</span> <span class="o">=</span> <span class="n">spec_gen</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">text_to_generate</span><span class="p">)</span>
        <span class="n">spectrogram</span> <span class="o">=</span> <span class="n">spec_gen</span><span class="o">.</span><span class="n">generate_spectrogram</span><span class="p">(</span><span class="n">tokens</span><span class="o">=</span><span class="n">parsed</span><span class="p">)</span>
        <span class="n">audio</span> <span class="o">=</span> <span class="n">vocoder</span><span class="o">.</span><span class="n">convert_spectrogram_to_audio</span><span class="p">(</span><span class="n">spec</span><span class="o">=</span><span class="n">spectrogram</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">spectrogram</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="n">spectrogram</span> <span class="o">=</span> <span class="n">spectrogram</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">spectrogram</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="n">spectrogram</span> <span class="o">=</span> <span class="n">spectrogram</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">audio</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="n">audio</span> <span class="o">=</span> <span class="n">audio</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">spectrogram</span><span class="p">,</span> <span class="n">audio</span>


<span class="n">text_to_generate</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s2">&quot;Input what you want the model to say: &quot;</span><span class="p">)</span>
<span class="n">spec</span><span class="p">,</span> <span class="n">audio</span> <span class="o">=</span> <span class="n">infer</span><span class="p">(</span><span class="n">spec_gen</span><span class="p">,</span> <span class="n">vocoder</span><span class="p">,</span> <span class="n">text_to_generate</span><span class="p">)</span>
</pre></div>
</div>
<p>To see the available pretrained checkpoints:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># spec generator</span>
<span class="n">GlowTTSModel</span><span class="o">.</span><span class="n">list_available_models</span><span class="p">()</span>

<span class="c1"># vocoder</span>
<span class="n">WaveGlowModel</span><span class="o">.</span><span class="n">list_available_models</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="nemo-tts-model-under-the-hood">
<h4>NeMo TTS Model Under the Hood<a class="headerlink" href="#nemo-tts-model-under-the-hood" title="Permalink to this headline">¶</a></h4>
<p>Any aspect of TTS training or model architecture design can easily
be customized with PyTorch Lightning since every NeMo model is a LightningModule.</p>
<p><a class="reference external" href="https://github.com/NVIDIA/NeMo/blob/v1.0.0b1/nemo/collections/tts/models/glow_tts.py">glow_tts.py</a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GlowTTSModel</span><span class="p">(</span><span class="n">SpectrogramGenerator</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    GlowTTS model used to generate spectrograms from text</span>
<span class="sd">    Consists of a text encoder and an invertible spectrogram decoder</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="o">...</span>
    <span class="c1"># NeMo models come with neural type checking</span>
    <span class="nd">@typecheck</span><span class="p">(</span>
        <span class="n">input_types</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="n">NeuralType</span><span class="p">((</span><span class="s2">&quot;B&quot;</span><span class="p">,</span> <span class="s2">&quot;T&quot;</span><span class="p">),</span> <span class="n">TokenIndex</span><span class="p">()),</span>
            <span class="s2">&quot;x_lengths&quot;</span><span class="p">:</span> <span class="n">NeuralType</span><span class="p">((</span><span class="s2">&quot;B&quot;</span><span class="p">),</span> <span class="n">LengthsType</span><span class="p">()),</span>
            <span class="s2">&quot;y&quot;</span><span class="p">:</span> <span class="n">NeuralType</span><span class="p">((</span><span class="s2">&quot;B&quot;</span><span class="p">,</span> <span class="s2">&quot;D&quot;</span><span class="p">,</span> <span class="s2">&quot;T&quot;</span><span class="p">),</span> <span class="n">MelSpectrogramType</span><span class="p">(),</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="s2">&quot;y_lengths&quot;</span><span class="p">:</span> <span class="n">NeuralType</span><span class="p">((</span><span class="s2">&quot;B&quot;</span><span class="p">),</span> <span class="n">LengthsType</span><span class="p">(),</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="s2">&quot;gen&quot;</span><span class="p">:</span> <span class="n">NeuralType</span><span class="p">(</span><span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="s2">&quot;noise_scale&quot;</span><span class="p">:</span> <span class="n">NeuralType</span><span class="p">(</span><span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="s2">&quot;length_scale&quot;</span><span class="p">:</span> <span class="n">NeuralType</span><span class="p">(</span><span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="p">}</span>
    <span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x_lengths</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">y_lengths</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">gen</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">noise_scale</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">length_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">gen</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">glow_tts</span><span class="o">.</span><span class="n">generate_spect</span><span class="p">(</span>
                <span class="n">text</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">text_lengths</span><span class="o">=</span><span class="n">x_lengths</span><span class="p">,</span> <span class="n">noise_scale</span><span class="o">=</span><span class="n">noise_scale</span><span class="p">,</span> <span class="n">length_scale</span><span class="o">=</span><span class="n">length_scale</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">glow_tts</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">text_lengths</span><span class="o">=</span><span class="n">x_lengths</span><span class="p">,</span> <span class="n">spect</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">spect_lengths</span><span class="o">=</span><span class="n">y_lengths</span><span class="p">)</span>

    <span class="o">...</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">y_lengths</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x_lengths</span><span class="p">):</span>
        <span class="n">z</span><span class="p">,</span> <span class="n">y_m</span><span class="p">,</span> <span class="n">y_logs</span><span class="p">,</span> <span class="n">logdet</span><span class="p">,</span> <span class="n">logw</span><span class="p">,</span> <span class="n">logw_</span><span class="p">,</span> <span class="n">y_lengths</span><span class="p">,</span> <span class="n">attn</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span>
            <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">x_lengths</span><span class="o">=</span><span class="n">x_lengths</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">y_lengths</span><span class="o">=</span><span class="n">y_lengths</span><span class="p">,</span> <span class="n">gen</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>

        <span class="n">l_mle</span><span class="p">,</span> <span class="n">l_length</span><span class="p">,</span> <span class="n">logdet</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span>
            <span class="n">z</span><span class="o">=</span><span class="n">z</span><span class="p">,</span>
            <span class="n">y_m</span><span class="o">=</span><span class="n">y_m</span><span class="p">,</span>
            <span class="n">y_logs</span><span class="o">=</span><span class="n">y_logs</span><span class="p">,</span>
            <span class="n">logdet</span><span class="o">=</span><span class="n">logdet</span><span class="p">,</span>
            <span class="n">logw</span><span class="o">=</span><span class="n">logw</span><span class="p">,</span>
            <span class="n">logw_</span><span class="o">=</span><span class="n">logw_</span><span class="p">,</span>
            <span class="n">x_lengths</span><span class="o">=</span><span class="n">x_lengths</span><span class="p">,</span>
            <span class="n">y_lengths</span><span class="o">=</span><span class="n">y_lengths</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([</span><span class="n">l_mle</span><span class="p">,</span> <span class="n">l_length</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">l_mle</span><span class="p">,</span> <span class="n">l_length</span><span class="p">,</span> <span class="n">logdet</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">attn</span>

    <span class="c1"># PTL-specific methods</span>
    <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="n">y</span><span class="p">,</span> <span class="n">y_lengths</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x_lengths</span> <span class="o">=</span> <span class="n">batch</span>

        <span class="n">y</span><span class="p">,</span> <span class="n">y_lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">preprocessor</span><span class="p">(</span><span class="n">input_signal</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="n">y_lengths</span><span class="p">)</span>

        <span class="n">l_mle</span><span class="p">,</span> <span class="n">l_length</span><span class="p">,</span> <span class="n">logdet</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_lengths</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x_lengths</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">({</span><span class="s2">&quot;l_mle&quot;</span><span class="p">:</span> <span class="n">l_mle</span><span class="p">,</span> <span class="s2">&quot;l_length&quot;</span><span class="p">:</span> <span class="n">l_length</span><span class="p">,</span> <span class="s2">&quot;logdet&quot;</span><span class="p">:</span> <span class="n">logdet</span><span class="p">},</span> <span class="n">prog_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="o">...</span>
</pre></div>
</div>
</section>
<section id="neural-types-in-nemo-tts">
<h4>Neural Types in NeMo TTS<a class="headerlink" href="#neural-types-in-nemo-tts" title="Permalink to this headline">¶</a></h4>
<p>NeMo Models and Neural Modules come with Neural Type checking.
Neural type checking is extremely useful when combining many different neural network architectures
for a production-grade application.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@typecheck</span><span class="p">(</span>
    <span class="n">input_types</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="n">NeuralType</span><span class="p">((</span><span class="s2">&quot;B&quot;</span><span class="p">,</span> <span class="s2">&quot;T&quot;</span><span class="p">),</span> <span class="n">TokenIndex</span><span class="p">()),</span>
        <span class="s2">&quot;x_lengths&quot;</span><span class="p">:</span> <span class="n">NeuralType</span><span class="p">((</span><span class="s2">&quot;B&quot;</span><span class="p">),</span> <span class="n">LengthsType</span><span class="p">()),</span>
        <span class="s2">&quot;y&quot;</span><span class="p">:</span> <span class="n">NeuralType</span><span class="p">((</span><span class="s2">&quot;B&quot;</span><span class="p">,</span> <span class="s2">&quot;D&quot;</span><span class="p">,</span> <span class="s2">&quot;T&quot;</span><span class="p">),</span> <span class="n">MelSpectrogramType</span><span class="p">(),</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="s2">&quot;y_lengths&quot;</span><span class="p">:</span> <span class="n">NeuralType</span><span class="p">((</span><span class="s2">&quot;B&quot;</span><span class="p">),</span> <span class="n">LengthsType</span><span class="p">(),</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="s2">&quot;gen&quot;</span><span class="p">:</span> <span class="n">NeuralType</span><span class="p">(</span><span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="s2">&quot;noise_scale&quot;</span><span class="p">:</span> <span class="n">NeuralType</span><span class="p">(</span><span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="s2">&quot;length_scale&quot;</span><span class="p">:</span> <span class="n">NeuralType</span><span class="p">(</span><span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="p">}</span>
<span class="p">)</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x_lengths</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">y_lengths</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">gen</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">noise_scale</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">length_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="o">...</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="learn-more">
<h3>Learn More<a class="headerlink" href="#learn-more" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Watch the <a class="reference external" href="https://youtu.be/wBgpMf_KQVw">NVIDIA NeMo Intro Video</a></p></li>
<li><p>Watch the <a class="reference external" href="https://youtu.be/rFAX1-4DSr4">PyTorch Lightning and NVIDIA NeMo Discussion Video</a></p></li>
<li><p>Visit the <a class="reference external" href="https://developer.nvidia.com/nvidia-nemo">NVIDIA NeMo Developer Website</a></p></li>
<li><p>Read the <a class="reference external" href="https://medium.com/pytorch/nvidia-nemo-neural-modules-and-models-for-conversational-ai-d660480d9696">NVIDIA NeMo PyTorch Blog</a></p></li>
<li><p>Download pre-trained <a class="reference external" href="https://ngc.nvidia.com/catalog/models/nvidia:nemospeechmodels">ASR</a>, <a class="reference external" href="https://ngc.nvidia.com/catalog/models/nvidia:nemonlpmodels">NLP</a>, and <a class="reference external" href="https://ngc.nvidia.com/catalog/models/nvidia:nemospeechmodels">TTS</a> models on <a class="reference external" href="https://ngc.nvidia.com/">NVIDIA NGC</a> to quickly get started with NeMo.</p></li>
<li><p>Become an expert on Building Conversational AI applications with our <a class="reference external" href="https://github.com/NVIDIA/NeMo#tutorials">tutorials</a>, and <a class="reference external" href="https://github.com/NVIDIA/NeMo/tree/v1.0.0b1/examples">example scripts</a>,</p></li>
<li><p>See our <a class="reference external" href="https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/">developer guide</a> for more information on core NeMo concepts, ASR/NLP/TTS collections, and the NeMo API.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>NeMo tutorial notebooks can be run on <a class="reference external" href="https://colab.research.google.com/notebooks/intro.ipynb">Google Colab</a>.</p>
</div>
<p>NVIDIA <a class="reference external" href="https://github.com/NVIDIA/NeMo">NeMo</a> is actively being developed on GitHub.
<a class="reference external" href="https://github.com/NVIDIA/NeMo/blob/v1.0.0b1/CONTRIBUTING.md">Contributions</a> are welcome!</p>
</section>
</section>
</section>


             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright Copyright (c) 2018-2022, William Falcon et al...

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Conversational AI</a><ul>
<li><a class="reference internal" href="#nemo">NeMo</a><ul>
<li><a class="reference internal" href="#nemo-models">NeMo Models</a><ul>
<li><a class="reference internal" href="#installing-nemo">Installing NeMo</a></li>
<li><a class="reference internal" href="#experiment-manager">Experiment Manager</a></li>
</ul>
</li>
<li><a class="reference internal" href="#automatic-speech-recognition-asr">Automatic Speech Recognition (ASR)</a><ul>
<li><a class="reference internal" href="#specify-asr-model-configurations-with-yaml-file">Specify ASR Model Configurations with YAML File</a></li>
<li><a class="reference internal" href="#developing-asr-model-from-scratch">Developing ASR Model From Scratch</a></li>
<li><a class="reference internal" href="#using-state-of-the-art-pre-trained-asr-model">Using State-Of-The-Art Pre-trained ASR Model</a></li>
<li><a class="reference internal" href="#nemo-asr-model-under-the-hood">NeMo ASR Model Under the Hood</a></li>
<li><a class="reference internal" href="#neural-types-in-nemo-asr">Neural Types in NeMo ASR</a></li>
</ul>
</li>
<li><a class="reference internal" href="#natural-language-processing-nlp">Natural Language Processing (NLP)</a><ul>
<li><a class="reference internal" href="#named-entity-recognition-ner">Named Entity Recognition (NER)</a></li>
<li><a class="reference internal" href="#specify-ner-model-configurations-with-yaml-file">Specify NER Model Configurations with YAML File</a></li>
<li><a class="reference internal" href="#developing-ner-model-from-scratch">Developing NER Model From Scratch</a></li>
<li><a class="reference internal" href="#tokenizers">Tokenizers</a></li>
<li><a class="reference internal" href="#language-models">Language Models</a></li>
<li><a class="reference internal" href="#using-a-pre-trained-ner-model">Using a Pre-trained NER Model</a></li>
<li><a class="reference internal" href="#nemo-ner-model-under-the-hood">NeMo NER Model Under the Hood</a></li>
<li><a class="reference internal" href="#neural-types-in-nemo-nlp">Neural Types in NeMo NLP</a></li>
</ul>
</li>
<li><a class="reference internal" href="#text-to-speech-tts">Text-To-Speech (TTS)</a><ul>
<li><a class="reference internal" href="#specify-tts-model-configurations-with-yaml-file">Specify TTS Model Configurations with YAML File</a></li>
<li><a class="reference internal" href="#developing-tts-model-from-scratch">Developing TTS Model From Scratch</a></li>
<li><a class="reference internal" href="#using-state-of-the-art-pre-trained-tts-model">Using State-Of-The-Art Pre-trained TTS Model</a></li>
<li><a class="reference internal" href="#nemo-tts-model-under-the-hood">NeMo TTS Model Under the Hood</a></li>
<li><a class="reference internal" href="#neural-types-in-nemo-tts">Neural Types in NeMo TTS</a></li>
</ul>
</li>
<li><a class="reference internal" href="#learn-more">Learn More</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
         <script src="../_static/jquery.js"></script>
         <script src="../_static/underscore.js"></script>
         <script src="../_static/doctools.js"></script>
         <script src="../_static/clipboard.min.js"></script>
         <script src="../_static/copybutton.js"></script>
         <script src="../_static/copybutton.js"></script>
         <script>let toggleHintShow = 'Click to show';</script>
         <script>let toggleHintHide = 'Click to hide';</script>
         <script>let toggleOpenOnPrint = 'true';</script>
         <script src="../_static/togglebutton.js"></script>
         <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Best practices', 'Optional Extensions', 'Tutorials', 'API References', 'Bolts', 'Examples', 'Partner Domain Frameworks', 'Community'];
</script>



  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <!--
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch-lightning.rtfd.io/en/latest">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch-lightning.readthedocs.io/en/latest/#tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch-lightning.readthedocs.io/en/latest/#community-examples">View Resources</a>
        </div>
        -->
      </div>
    </div>
  </div>

  <!--
  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch-lightning.rtfd.io/en/latest/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch-lightning.rtfd.io/en/latest/">PyTorch</a></li>
            <li><a href="https://pytorch-lightning.readthedocs.io/en/latest/starter/introduction.html">Get Started</a></li>
            <li><a href="https://pytorch-lightning.rtfd.io/en/latest/">Features</a></li>
            <li><a href="">Ecosystem</a></li>
            <li><a href="https://www.pytorchlightning.ai/blog">Blog</a></li>
            <li><a href="https://github.com/PyTorchLightning/pytorch-lightning/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch-lightning.readthedocs.io/en/latest/#community-examples">Resources</a></li>
            <li><a href="https://pytorch-lightning.readthedocs.io/en/latest/#tutorials">Tutorials</a></li>
            <li><a href="https://pytorch-lightning.rtfd.io/en/latest">Docs</a></li>
            <li><a href="https://www.pytorchlightning.ai/community" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/PyTorchLightning/pytorch-lightning/issues" target="_blank">Github Issues</a></li>
            <li><a href="" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Stay Connected</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/PyTorchLightnin" target="_blank" class="twitter"></a>
            <a href="" target="_blank" class="youtube"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>
  -->

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. Read PyTorch Lightning's <a href="https://pytorchlightning.ai/privacy-policy">Privacy Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch-lightning.rtfd.io/en/latest/" aria-label="PyTorch Lightning"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <!-- <li>
            <a href="https://pytorch-lightning.readthedocs.io/en/latest/starter/introduction.html">Get Started</a>
          </li>

          <li>
            <a href="https://www.pytorchlightning.ai/blog">Blog</a>
          </li>

          <li class="resources-mobile-menu-title">
            Ecosystem
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch-lightning.readthedocs.io/en/stable/">PyTorch Lightning</a>
            </li>

            <li>
              <a href="https://torchmetrics.readthedocs.io/en/stable/">TorchMetrics</a>
            </li>

            <li>
              <a href="https://lightning-flash.readthedocs.io/en/stable/">Lightning Flash</a>
            </li>

            <li>
              <a href="https://lightning-transformers.readthedocs.io/en/stable/">Lightning Transformers</a>
            </li>

            <li>
              <a href="https://lightning-bolts.readthedocs.io/en/stable/">Lightning Bolts</a>
            </li>
          </ul> -->

          <!--<li class="resources-mobile-menu-title">
            Resources
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch-lightning.readthedocs.io/en/latest/#community-examples">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch-lightning.rtfd.io/en/latest/">About</a>
            </li>

            <li>
              <a href="">Models (Beta)</a>
            </li>

            <li>
              <a href="https://www.pytorchlightning.ai/community">Community</a>
            </li>

            <li>
              <a href="https://github.com/PyTorchLightning/pytorch-lightning/discussions">Forums</a>
            </li>
          </ul>-->

          <!-- <li>
            <a href="https://github.com/PyTorchLightning/pytorch-lightning">Github</a>
          </li> -->

          <!-- <li>
            <a href="https://www.grid.ai/">Grid.ai</a>
          </li> -->
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>

 </body>
</html>