


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>

  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Train 1 trillion+ parameter models &mdash; PyTorch Lightning 1.7.0dev documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/icon.svg"/>
  
  
  
    <link rel="canonical" href="https://pytorch-lightning.readthedocs.io/en/stable//advanced/model_parallel.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../_static/togglebutton.css" type="text/css" />
  <link rel="stylesheet" href="../_static/main.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sphinx_paramlinks.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Train on the cloud" href="../clouds/cloud_training.html" />
    <link rel="prev" title="Run on an on-prem cluster" href="../clouds/cluster.html" />
  
  <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-82W25RV60Q"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-82W25RV60Q');
    </script>
  
  <!-- End Google Analytics -->
  

  
  <script src="../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/UCity/UCity-Light.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/UCity/UCity-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/UCity/UCity-Semibold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/Inconsolata/Inconsolata.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <script defer src="https://use.fontawesome.com/releases/v6.1.1/js/all.js" integrity="sha384-xBXmu0dk1bEoiwd71wOonQLyH+VpgR1XcDH3rtxrLww5ajNTuMvBdL5SOiFZnNdp" crossorigin="anonymous"></script>
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch-lightning.rtfd.io/en/latest/" aria-label="PyTorch Lightning">
      <!--  <img class="call-to-action-img" src="../_static/images/logo-lightning-icon.png"/> -->
      </a>

      <div class="main-menu">
        <ul>
          <!-- <li>
            <a href="https://pytorch-lightning.readthedocs.io/en/latest/starter/introduction.html">Get Started</a>
          </li> -->

          <!-- <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch-lightning.readthedocs.io/en/stable/">
                  <span class="dropdown-title">PyTorch Lightning</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://torchmetrics.readthedocs.io/en/stable/">
                  <span class="dropdown-title">TorchMetrics</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning-flash.readthedocs.io/en/stable/">
                  <span class="dropdown-title">Lightning Flash</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning-transformers.readthedocs.io/en/stable/">
                  <span class="dropdown-title">Lightning Transformers</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning-bolts.readthedocs.io/en/stable/">
                  <span class="dropdown-title">Lightning Bolts</span>
                </a>
            </div>
          </li> -->

          <!--<li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://www.pytorchlightning.ai/community">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch-lightning.readthedocs.io/en/latest/#community-examples">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://github.com/PyTorchLightning/pytorch-lightning/discussions" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>-->

          <!-- <li>
            <a href="https://github.com/PyTorchLightning/pytorch-lightning">GitHub</a>
          </li>

          <li>
            <a href="https://www.grid.ai/">Train on the cloud</a>
          </li> -->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  1.7.0dev
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../starter/introduction.html">Lightning 15분 만에 배워보기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../starter/converting.html">Organize existing PyTorch into Lightning</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Level Up</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../levels/core_skills.html">Basic skills</a></li>
<li class="toctree-l1"><a class="reference internal" href="../levels/intermediate.html">Intermediate skills</a></li>
<li class="toctree-l1"><a class="reference internal" href="../levels/advanced.html">Advanced skills</a></li>
<li class="toctree-l1"><a class="reference internal" href="../levels/expert.html">Expert skills</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Core API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../common/lightning_module.html">LightningModule</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/trainer.html">Trainer</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Common Workflows</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../common/evaluation.html">Avoid overfitting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model/build_model.html">Build a Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/hyperparameters.html">Configure hyperparameters from the CLI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/progress_bar.html">Customize the progress bar</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/production.html">Deploy models into production</a></li>
<li class="toctree-l1"><a class="reference internal" href="training_tricks.html">Effective Training Techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cli/lightning_cli.html">Eliminate config boilerplate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tuning/profiler.html">Find bottlenecks in your code</a></li>
<li class="toctree-l1"><a class="reference internal" href="transfer_learning.html">Finetune a model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../visualize/logging_intermediate.html">Manage experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../clouds/cluster.html">Run on an on-prem cluster</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Train 1 trillion+ parameter models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../clouds/cloud_training.html">Train on the cloud</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/checkpointing.html">Save and load model progress</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/precision.html">Save memory with half-precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accelerators/gpu.html">Train on single or multiple GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accelerators/hpu.html">Train on single or multiple HPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accelerators/ipu.html">Train on single or multiple IPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accelerators/tpu.html">Train on single or multiple TPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model/own_your_loop.html">Use a pure PyTorch training loop</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Glossary</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../extensions/accelerator.html">Accelerators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../extensions/callbacks.html">Callback</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/checkpointing.html">Checkpointing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../clouds/cluster.html">Cluster</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/checkpointing_advanced.html">Cloud checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/console_logs.html">Console Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/debugging.html">Debugging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/early_stopping.html">Early stopping</a></li>
<li class="toctree-l1"><a class="reference internal" href="../visualize/experiment_managers.html">Experiment manager (Logger)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../clouds/fault_tolerant_training.html">Fault tolerant training</a></li>
<li class="toctree-l1"><a class="reference external" href="https://lightning-flash.readthedocs.io/en/stable/">Flash</a></li>
<li class="toctree-l1"><a class="reference internal" href="../clouds/cloud_training.html">Grid AI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accelerators/gpu.html">GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/precision.html">Half precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accelerators/hpu.html">HPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/production_intermediate.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accelerators/ipu.html">IPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cli/lightning_cli.html">Lightning CLI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model/build_model_expert.html">Raw PyTorch loop (expert)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model/build_model_expert.html#lightninglite-stepping-stone-to-lightning">LightningLite (Stepping Stone to Lightning)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data/datamodule.html">LightningDataModule</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/lightning_module.html">LightningModule</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch-lightning.readthedocs.io/en/stable/ecosystem/transformers.html">Lightning Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../visualize/loggers.html">Log</a></li>
<li class="toctree-l1"><a class="reference internal" href="../extensions/loops.html">Loops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accelerators/tpu.html">TPU</a></li>
<li class="toctree-l1"><a class="reference external" href="https://torchmetrics.readthedocs.io/en/stable/">Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model/build_model.html">Model</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Model Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../extensions/plugins.html">Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/progress_bar.html">Progress bar</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/production_advanced.html">Production</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/production_basic.html">Predict</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tuning/profiler.html">Profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="pruning_quantization.html">Pruning and Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/remote_fs.html">Remote filesystem and FSSPEC</a></li>
<li class="toctree-l1"><a class="reference internal" href="strategy_registry.html">Strategy registry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../starter/style_guide.html">Style guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../clouds/run_intermediate.html">Sweep</a></li>
<li class="toctree-l1"><a class="reference internal" href="training_tricks.html">SWA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../clouds/cluster_advanced.html">SLURM</a></li>
<li class="toctree-l1"><a class="reference internal" href="transfer_learning.html">Transfer learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../common/trainer.html">Trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../clouds/cluster_intermediate_2.html">Torch distributed</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Hands-on Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.youtube.com/playlist?list=PLaMu-SDt_RB5NUm67hU2pdE75j6KaIOv2">PyTorch Lightning 101 class</a></li>
<li class="toctree-l1"><a class="reference external" href="https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09">From PyTorch to PyTorch Lightning [Blog]</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.youtube.com/watch?v=QHww1JH7IDU">From PyTorch to PyTorch Lightning [Video]</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>Train 1 trillion+ parameter models</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/advanced/model_parallel.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <section id="train-1-trillion-parameter-models">
<span id="model-parallel"></span><h1>Train 1 trillion+ parameter models<a class="headerlink" href="#train-1-trillion-parameter-models" title="Permalink to this headline">¶</a></h1>
<p>When training large models, fitting larger batch sizes, or trying to increase throughput using multi-GPU compute, Lightning provides advanced optimized distributed training strategies to support these cases and offer substantial improvements in memory usage.</p>
<p>In many cases these strategies are some flavour of model parallelism however we only introduce concepts at a high level to get you started. Refer to the <a class="reference external" href="https://fairscale.readthedocs.io/en/latest/deep_dive/oss_sdp_fsdp.html">FairScale documentation</a> for more information about model parallelism.</p>
<p>Note that some of the extreme memory saving configurations will affect the speed of training. This Speed/Memory trade-off in most cases can be adjusted.</p>
<p>Some of these memory-efficient strategies rely on offloading onto other forms of memory, such as CPU RAM or NVMe. This means you can even see memory benefits on a <strong>single GPU</strong>, using a strategy such as <a class="reference internal" href="#deepspeed-zero-stage-3-offload"><span class="std std-ref">DeepSpeed ZeRO Stage 3 Offload</span></a>.</p>
<p>Check out this amazing video explaining model parallelism and how it works behind the scenes:</p>
<iframe width="540" height="300" src="https://www.youtube.com/embed/w_CKzh5C1K4" frameborder="0"
allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><section id="choosing-an-advanced-distributed-gpu-strategy">
<h2>Choosing an Advanced Distributed GPU Strategy<a class="headerlink" href="#choosing-an-advanced-distributed-gpu-strategy" title="Permalink to this headline">¶</a></h2>
<p>If you would like to stick with PyTorch DDP, see <a class="reference internal" href="#ddp-optimizations"><span class="std std-ref">DDP Optimizations</span></a>.</p>
<p>Unlike <code class="xref py py-class docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> (DDP) where the maximum trainable model size and batch size do not change with respect to the number of GPUs, memory-optimized strategies can accommodate bigger models and larger batches as more GPUs are used. This means as you scale up the number of GPUs, you can reach the number of model parameters you’d like to train.</p>
<p>There are many considerations when choosing a strategy as described below. In addition, check out the visualization of various strategy benchmarks using <a class="reference external" href="https://github.com/SeanNaren/minGPT">minGPT</a> <a class="reference external" href="https://share.streamlit.io/seannaren/mingpt/streamlit/app.py">here</a>.</p>
<section id="pre-training-vs-fine-tuning">
<h3>Pre-training vs Fine-tuning<a class="headerlink" href="#pre-training-vs-fine-tuning" title="Permalink to this headline">¶</a></h3>
<p>When fine-tuning, we often use a magnitude less data compared to pre-training a model. This is important when choosing a distributed strategy as usually for pre-training, <strong>we are compute-bound</strong>.
This means we cannot sacrifice throughput as much as if we were fine-tuning, because in fine-tuning the data requirement is smaller.</p>
<p>Overall:</p>
<ul class="simple">
<li><p>When <strong>fine-tuning</strong> a model, use advanced memory efficient strategies such as <a class="reference internal" href="#deepspeed-zero-stage-3"><span class="std std-ref">DeepSpeed ZeRO Stage 3</span></a> or <a class="reference internal" href="#deepspeed-zero-stage-3-offload"><span class="std std-ref">DeepSpeed ZeRO Stage 3 Offload</span></a>, allowing you to fine-tune larger models if you are limited on compute</p></li>
<li><p>When <strong>pre-training</strong> a model, use simpler optimizations such <a class="reference internal" href="#sharded-training"><span class="std std-ref">Sharded Training</span></a>, <a class="reference internal" href="#deepspeed-zero-stage-2"><span class="std std-ref">DeepSpeed ZeRO Stage 2</span></a> or <a class="reference internal" href="#fully-sharded-training"><span class="std std-ref">Fully Sharded Training</span></a>, scaling the number of GPUs to reach larger parameter sizes</p></li>
<li><p>For both fine-tuning and pre-training, use <a class="reference internal" href="#deepspeed-activation-checkpointing"><span class="std std-ref">DeepSpeed Activation Checkpointing</span></a> or <a class="reference internal" href="#fairscale-activation-checkpointing"><span class="std std-ref">FairScale Activation Checkpointing</span></a> as the throughput degradation is not significant</p></li>
</ul>
<p>For example when using 128 GPUs, you can <strong>pre-train</strong> large 10 to 20 Billion parameter models using <a class="reference internal" href="#deepspeed-zero-stage-2"><span class="std std-ref">DeepSpeed ZeRO Stage 2</span></a> without having to take a performance hit with more advanced optimized multi-gpu strategy.</p>
<p>But for <strong>fine-tuning</strong> a model, you can reach 10 to 20 Billion parameter models using <a class="reference internal" href="#deepspeed-zero-stage-3-offload"><span class="std std-ref">DeepSpeed ZeRO Stage 3 Offload</span></a> on a <strong>single GPU</strong>. This does come with a significant throughput hit, which needs to be weighed accordingly.</p>
</section>
<section id="when-shouldn-t-i-use-an-optimized-distributed-strategy">
<h3>When Shouldn’t I use an Optimized Distributed Strategy?<a class="headerlink" href="#when-shouldn-t-i-use-an-optimized-distributed-strategy" title="Permalink to this headline">¶</a></h3>
<p>Sharding techniques help when model sizes are fairly large; roughly 500M+ parameters is where we’ve seen benefits. However, in the following cases, we recommend sticking to ordinary distributed strategies
* When your model is small (ResNet50 of around 80M Parameters), unless you are using unusually large batch sizes or inputs.
* Due to high distributed communication between devices, if running on a slow network/interconnect, the training might be much slower than expected and then it’s up to you to determince the tradeoff here.</p>
<hr class="docutils" />
</section>
</section>
<section id="sharded-training">
<span id="id1"></span><h2>Sharded Training<a class="headerlink" href="#sharded-training" title="Permalink to this headline">¶</a></h2>
<p>Lightning integration of optimizer sharded training provided by <a class="reference external" href="https://github.com/facebookresearch/fairscale">FairScale</a>.
The technique can be found within <a class="reference external" href="https://arxiv.org/abs/1910.02054">DeepSpeed ZeRO</a> and
<a class="reference external" href="https://www.microsoft.com/en-us/research/blog/zero-2-deepspeed-shattering-barriers-of-deep-learning-speed-scale/">ZeRO-2</a>,
however the implementation is built from the ground up to be PyTorch compatible and standalone.
Sharded Training allows you to maintain GPU scaling efficiency, whilst reducing memory overhead drastically. In short, expect near-normal linear scaling (if your network allows), and significantly reduced memory usage when training large models.</p>
<p>Sharded Training still utilizes Data Parallel Training under the hood, except optimizer states and gradients are sharded across GPUs.
This means the memory overhead per GPU is lower, as each GPU only has to maintain a partition of your optimizer state and gradients.</p>
<p>The benefits vary by model and parameter sizes, but we’ve recorded up to a 63% memory reduction per GPU allowing us to double our model sizes. Because of efficient communication,
these benefits in multi-GPU setups are almost free and throughput scales well with multi-node setups.</p>
<p>It is highly recommended to use Sharded Training in multi-GPU environments where memory is limited, or where training larger models are beneficial (500M+ parameter models).
A technical note: as batch size scales, storing activations for the backwards pass becomes the bottleneck in training. As a result, sharding optimizer state and gradients becomes less impactful.
Use <a class="reference internal" href="#fairscale-activation-checkpointing"><span class="std std-ref">FairScale Activation Checkpointing</span></a> to see even more benefit at the cost of some throughput.</p>
<p>To use Sharded Training, you need to first install FairScale using the command below.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install fairscale
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># train using Sharded DDP</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;ddp_sharded&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Sharded Training can work across all DDP variants by adding the additional <code class="docutils literal notranslate"><span class="pre">--strategy</span> <span class="pre">ddp_sharded</span></code> flag via command line using a PyTorch Lightning script.</p>
<p>Internally we re-initialize your optimizers and shard them across your machines and processes. We handle all communication using PyTorch distributed, so no code changes are required.</p>
<hr class="docutils" />
</section>
<section id="fully-sharded-training">
<span id="id2"></span><h2>Fully Sharded Training<a class="headerlink" href="#fully-sharded-training" title="Permalink to this headline">¶</a></h2>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Fully Sharded Training is in beta and the API is subject to change. Please create an <a class="reference external" href="https://github.com/PyTorchLightning/pytorch-lightning/issues">issue</a> if you run into any issues.</p>
</div>
<p><a class="reference external" href="https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html">Fully Sharded</a> shards optimizer state, gradients and parameters across data parallel workers. This allows you to fit much larger models onto multiple GPUs into memory.</p>
<p>Fully Sharded Training alleviates the need to worry about balancing layers onto specific devices using some form of pipe parallelism, and optimizes for distributed communication with minimal effort.</p>
<section id="shard-parameters-to-reach-10-billion-parameters">
<h3>Shard Parameters to Reach 10+ Billion Parameters<a class="headerlink" href="#shard-parameters-to-reach-10-billion-parameters" title="Permalink to this headline">¶</a></h3>
<p>To reach larger parameter sizes and be memory efficient, we have to shard parameters. There are various ways to enable this.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Currently Fully Sharded Training relies on the user to wrap the model with Fully Sharded within the <code class="docutils literal notranslate"><span class="pre">LightningModule</span></code>.
This means you must create a single model that is treated as a <code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code> within the <code class="docutils literal notranslate"><span class="pre">LightningModule</span></code>.
This is a limitation of Fully Sharded Training that will be resolved in the future.</p>
</div>
</section>
<section id="enabling-module-sharding-for-maximum-memory-efficiency">
<h3>Enabling Module Sharding for Maximum Memory Efficiency<a class="headerlink" href="#enabling-module-sharding-for-maximum-memory-efficiency" title="Permalink to this headline">¶</a></h3>
<p>To activate parameter sharding, you must wrap your model using provided <code class="docutils literal notranslate"><span class="pre">wrap</span></code> or <code class="docutils literal notranslate"><span class="pre">auto_wrap</span></code> functions as described below. Internally in Lightning, we enable a context manager around the <code class="docutils literal notranslate"><span class="pre">configure_sharded_model</span></code> function to make sure the <code class="docutils literal notranslate"><span class="pre">wrap</span></code> and <code class="docutils literal notranslate"><span class="pre">auto_wrap</span></code> parameters are passed correctly.</p>
<p>When not using Fully Sharded these wrap functions are a no-op. This means once the changes have been made, there is no need to remove the changes for other strategies.</p>
<p><code class="docutils literal notranslate"><span class="pre">auto_wrap</span></code> will recursively wrap <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> within the <code class="docutils literal notranslate"><span class="pre">LightningModule</span></code> with nested Fully Sharded Wrappers,
signalling that we’d like to partition these modules across data parallel devices, discarding the full weights when not required (information <code class="xref py py-class docutils literal notranslate"><span class="pre">here</span></code>).</p>
<p><code class="docutils literal notranslate"><span class="pre">auto_wrap</span></code> can have varying level of success based on the complexity of your model. <strong>Auto Wrap does not support models with shared parameters</strong>.</p>
<p><code class="docutils literal notranslate"><span class="pre">wrap</span></code> will simply wrap the module with a Fully Sharded Parallel class with the correct parameters from the Lightning context manager.</p>
<p>Below is an example of using both <code class="docutils literal notranslate"><span class="pre">wrap</span></code> and <code class="docutils literal notranslate"><span class="pre">auto_wrap</span></code> to create your model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">pytorch_lightning</span> <span class="k">as</span> <span class="nn">pl</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning</span> <span class="kn">import</span> <span class="n">Trainer</span>
<span class="kn">from</span> <span class="nn">fairscale.nn</span> <span class="kn">import</span> <span class="n">checkpoint_wrapper</span><span class="p">,</span> <span class="n">auto_wrap</span><span class="p">,</span> <span class="n">wrap</span>


<span class="k">class</span> <span class="nc">MyModel</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">final_block</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">configure_sharded_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># modules are sharded across processes</span>
        <span class="c1"># as soon as they are wrapped with ``wrap`` or ``auto_wrap``.</span>
        <span class="c1"># During the forward/backward passes, weights get synced across processes</span>
        <span class="c1"># and de-allocated once computation is complete, saving memory.</span>

        <span class="c1"># Wraps the layer in a Fully Sharded Wrapper automatically</span>
        <span class="n">linear_layer</span> <span class="o">=</span> <span class="n">wrap</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_layer</span><span class="p">)</span>

        <span class="c1"># Wraps the module recursively</span>
        <span class="c1"># based on a minimum number of parameters (default 100M parameters)</span>
        <span class="n">block</span> <span class="o">=</span> <span class="n">auto_wrap</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">block</span><span class="p">)</span>

        <span class="c1"># For best memory efficiency,</span>
        <span class="c1"># add FairScale activation checkpointing</span>
        <span class="n">final_block</span> <span class="o">=</span> <span class="n">auto_wrap</span><span class="p">(</span><span class="n">checkpoint_wrapper</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">final_block</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">linear_layer</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span> <span class="n">block</span><span class="p">,</span> <span class="n">final_block</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;fsdp&quot;</span><span class="p">,</span> <span class="n">precision</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="n">trainer</span><span class="o">.</span><span class="n">test</span><span class="p">()</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">predict</span><span class="p">()</span>
</pre></div>
</div>
<hr class="docutils" />
</section>
</section>
<section id="fairscale-activation-checkpointing">
<span id="id3"></span><h2>FairScale Activation Checkpointing<a class="headerlink" href="#fairscale-activation-checkpointing" title="Permalink to this headline">¶</a></h2>
<p>Activation checkpointing frees activations from memory as soon as they are not needed during the forward pass. They are then re-computed for the backwards pass as needed. Activation checkpointing is very useful when you have intermediate layers that produce large activations.</p>
<p>FairScales’ checkpointing wrapper also handles batch norm layers correctly unlike the PyTorch implementation, ensuring stats are tracked correctly due to the multiple forward passes.</p>
<p>This saves memory when training larger models however requires wrapping modules you’d like to use activation checkpointing on. See <code class="xref py py-class docutils literal notranslate"><span class="pre">here</span></code> for more information.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Ensure to not wrap the entire model with activation checkpointing. This is not the intended usage of activation checkpointing, and will lead to failures as seen in <a class="reference external" href="https://github.com/PyTorchLightning/pytorch-lightning/discussions/9144">this discussion</a>.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_lightning</span> <span class="kn">import</span> <span class="n">Trainer</span>
<span class="kn">from</span> <span class="nn">fairscale.nn</span> <span class="kn">import</span> <span class="n">checkpoint_wrapper</span>


<span class="k">class</span> <span class="nc">MyModel</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Wrap layers using checkpoint_wrapper</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block_1</span> <span class="o">=</span> <span class="n">checkpoint_wrapper</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="deepspeed">
<span id="deepspeed-advanced"></span><h2>DeepSpeed<a class="headerlink" href="#deepspeed" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The DeepSpeed strategy is in beta and the API is subject to change. Please create an <a class="reference external" href="https://github.com/PyTorchLightning/pytorch-lightning/issues">issue</a> if you run into any issues.</p>
</div>
<p><a class="reference external" href="https://github.com/microsoft/DeepSpeed">DeepSpeed</a> is a deep learning training optimization library, providing the means to train massive billion parameter models at scale.
Using the DeepSpeed strategy, we were able to <strong>train model sizes of 10 Billion parameters and above</strong>, with a lot of useful information in this <a class="reference external" href="https://github.com/huggingface/transformers/issues/9996">benchmark</a> and the <a class="reference external" href="https://www.deepspeed.ai/tutorials/megatron/">DeepSpeed docs</a>.
DeepSpeed also offers lower level training optimizations, and efficient optimizers such as <a class="reference external" href="https://www.deepspeed.ai/tutorials/onebit-adam/">1-bit Adam</a>. We recommend using DeepSpeed in environments where speed and memory optimizations are important (such as training large billion parameter models).</p>
<p>Below is a summary of all the configurations of DeepSpeed.</p>
<ul class="simple">
<li><p><a class="reference internal" href="#deepspeed-zero-stage-1"><span class="std std-ref">DeepSpeed ZeRO Stage 1</span></a> - <strong>Shard optimizer states</strong>, remains at speed parity with DDP whilst providing memory improvement</p></li>
<li><p><a class="reference internal" href="#deepspeed-zero-stage-2"><span class="std std-ref">DeepSpeed ZeRO Stage 2</span></a> - <strong>Shard optimizer states and gradients</strong>, remains at speed parity with DDP whilst providing even more memory improvement</p></li>
<li><p><a class="reference internal" href="#deepspeed-zero-stage-2-offload"><span class="std std-ref">DeepSpeed ZeRO Stage 2 Offload</span></a> - <strong>Offload optimizer states and gradients to CPU</strong>. Increases distributed communication volume and GPU-CPU device transfer, but provides significant memory improvement</p></li>
<li><p><a class="reference internal" href="#deepspeed-zero-stage-3"><span class="std std-ref">DeepSpeed ZeRO Stage 3</span></a> - <strong>Shard optimizer states, gradients, parameters and optionally activations</strong>. Increases distributed communication volume, but provides even more memory improvement</p></li>
<li><p><a class="reference internal" href="#deepspeed-zero-stage-3-offload"><span class="std std-ref">DeepSpeed ZeRO Stage 3 Offload</span></a> - <strong>Offload optimizer states, gradients, parameters and optionally activations to CPU</strong>. Increases distributed communication volume and GPU-CPU device transfer, but even more significant memory improvement.</p></li>
<li><p><a class="reference internal" href="#deepspeed-activation-checkpointing"><span class="std std-ref">DeepSpeed Activation Checkpointing</span></a> - <strong>Free activations after forward pass</strong>. Increases computation, but provides memory improvement for all stages.</p></li>
</ul>
<p>To use DeepSpeed, you first need to install DeepSpeed using the commands below.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install deepspeed
</pre></div>
</div>
<p>If you run into an issue with the install or later in training, ensure that the CUDA version of the PyTorch you’ve installed matches your locally installed CUDA (you can see which one has been recognized by running <code class="docutils literal notranslate"><span class="pre">nvcc</span> <span class="pre">--version</span></code>).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>DeepSpeed currently only supports single optimizer, single scheduler within the training loop.</p>
<p>When saving a checkpoint we rely on DeepSpeed which saves a directory containing the model and various components.</p>
</div>
<section id="deepspeed-zero-stage-1">
<span id="id5"></span><h3>DeepSpeed ZeRO Stage 1<a class="headerlink" href="#deepspeed-zero-stage-1" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://www.deepspeed.ai/tutorials/zero/#zero-overview">DeepSpeed ZeRO Stage 1</a> partitions your optimizer states (Stage 1) across your GPUs to reduce memory.</p>
<p>It is recommended to skip Stage 1 and use Stage 2, which comes with larger memory improvements and still remains efficient. Stage 1 is useful to pair with certain optimizations such as <a class="reference external" href="https://github.com/pytorch/ort">Torch ORT</a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_lightning</span> <span class="kn">import</span> <span class="n">Trainer</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;deepspeed_stage_1&quot;</span><span class="p">,</span> <span class="n">precision</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="deepspeed-zero-stage-2">
<span id="id7"></span><h3>DeepSpeed ZeRO Stage 2<a class="headerlink" href="#deepspeed-zero-stage-2" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://www.deepspeed.ai/tutorials/zero/#zero-overview">DeepSpeed ZeRO Stage 2</a> partitions your optimizer states (Stage 1) and your gradients (Stage 2) across your GPUs to reduce memory. In most cases, this is more efficient or at parity with DDP, primarily due to the optimized custom communications written by the DeepSpeed team.
As a result, benefits can also be seen on a single GPU. Do note that the default bucket sizes allocate around <code class="docutils literal notranslate"><span class="pre">3.6GB</span></code> of VRAM to use during distributed communications, which can be tweaked when instantiating the strategy described in a few sections below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_lightning</span> <span class="kn">import</span> <span class="n">Trainer</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;deepspeed_stage_2&quot;</span><span class="p">,</span> <span class="n">precision</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python train.py --strategy deepspeed_stage_2 --precision <span class="m">16</span> --accelerator <span class="s1">&#39;gpu&#39;</span> --devices <span class="m">4</span>
</pre></div>
</div>
</section>
<section id="deepspeed-zero-stage-2-offload">
<span id="id9"></span><h3>DeepSpeed ZeRO Stage 2 Offload<a class="headerlink" href="#deepspeed-zero-stage-2-offload" title="Permalink to this headline">¶</a></h3>
<p>Below we show an example of running <a class="reference external" href="https://www.deepspeed.ai/tutorials/zero-offload/">ZeRO-Offload</a>. ZeRO-Offload leverages the host CPU to offload optimizer memory/computation, reducing the overall memory consumption.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_lightning</span> <span class="kn">import</span> <span class="n">Trainer</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;deepspeed_stage_2_offload&quot;</span><span class="p">,</span> <span class="n">precision</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<p>This can also be done via the command line using a PyTorch Lightning script:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python train.py --strategy deepspeed_stage_2_offload --precision <span class="m">16</span> --accelerator <span class="s1">&#39;gpu&#39;</span> --devices <span class="m">4</span>
</pre></div>
</div>
<p>You can also modify the ZeRO-Offload parameters via the strategy as below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_lightning</span> <span class="kn">import</span> <span class="n">Trainer</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.strategies</span> <span class="kn">import</span> <span class="n">DeepSpeedStrategy</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
    <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span>
    <span class="n">devices</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">strategy</span><span class="o">=</span><span class="n">DeepSpeedStrategy</span><span class="p">(</span><span class="n">offload_optimizer</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">allgather_bucket_size</span><span class="o">=</span><span class="mf">5e8</span><span class="p">,</span> <span class="n">reduce_bucket_size</span><span class="o">=</span><span class="mf">5e8</span><span class="p">),</span>
    <span class="n">precision</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We suggest tuning the <code class="docutils literal notranslate"><span class="pre">allgather_bucket_size</span></code> parameter and <code class="docutils literal notranslate"><span class="pre">reduce_bucket_size</span></code> parameter to find optimum parameters based on your model size.
These control how large a buffer we limit the model to using when reducing gradients/gathering updated parameters. Smaller values will result in less memory, but tradeoff with speed.</p>
<p>DeepSpeed allocates a reduce buffer size <a class="reference external" href="https://github.com/microsoft/DeepSpeed/blob/fead387f7837200fefbaba3a7b14709072d8d2cb/deepspeed/runtime/zero/stage_1_and_2.py#L2188">multiplied by 1.5x</a> so take that into consideration when tweaking the parameters.</p>
<p>The strategy sets a reasonable default of <code class="docutils literal notranslate"><span class="pre">2e8</span></code>, which should work for most low VRAM GPUs (less than <code class="docutils literal notranslate"><span class="pre">7GB</span></code>), allocating roughly <code class="docutils literal notranslate"><span class="pre">3.6GB</span></code> of VRAM as buffer. Higher VRAM GPUs should aim for values around <code class="docutils literal notranslate"><span class="pre">5e8</span></code>.</p>
</div>
<p>For even more speed benefit, DeepSpeed offers an optimized CPU version of ADAM called <a class="reference external" href="https://deepspeed.readthedocs.io/en/latest/optimizers.html#adam-cpu">DeepSpeedCPUAdam</a> to run the offloaded computation, which is faster than the standard PyTorch implementation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pytorch_lightning</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning</span> <span class="kn">import</span> <span class="n">Trainer</span>
<span class="kn">from</span> <span class="nn">deepspeed.ops.adam</span> <span class="kn">import</span> <span class="n">DeepSpeedCPUAdam</span>


<span class="k">class</span> <span class="nc">MyModel</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="o">...</span>

    <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># DeepSpeedCPUAdam provides 5x to 7x speedup over torch.optim.adam(w)</span>
        <span class="k">return</span> <span class="n">DeepSpeedCPUAdam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;deepspeed_stage_2_offload&quot;</span><span class="p">,</span> <span class="n">precision</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="deepspeed-zero-stage-3">
<span id="id10"></span><h3>DeepSpeed ZeRO Stage 3<a class="headerlink" href="#deepspeed-zero-stage-3" title="Permalink to this headline">¶</a></h3>
<p>DeepSpeed ZeRO Stage 3 shards the optimizer states, gradients and the model parameters (also optionally activations). Sharding model parameters and activations comes with an increase in distributed communication, however allows you to scale your models massively from one GPU to multiple GPUs.
<strong>The DeepSpeed team report the ability to fine-tune models with over 40B parameters on a single GPU and over 2 Trillion parameters on 512 GPUs.</strong> For more information we suggest checking the <a class="reference external" href="https://www.deepspeed.ai/news/2021/03/07/zero3-offload.html">DeepSpeed ZeRO-3 Offload documentation</a>.</p>
<p>We’ve ran benchmarks for all these features and given a simple example of how all these features work in Lightning, which you can see at <a class="reference external" href="https://github.com/SeanNaren/minGPT/tree/stage3">minGPT</a>.</p>
<p>To reach the highest memory efficiency or model size, you must:</p>
<ol class="arabic simple">
<li><p>Use the DeepSpeed strategy with the stage 3 parameter</p></li>
<li><p>Use CPU Offloading to offload weights to CPU, plus have a reasonable amount of CPU RAM to offload onto</p></li>
<li><p>Use DeepSpeed Activation Checkpointing to shard activations</p></li>
</ol>
<p>Below we describe how to enable all of these to see benefit. <strong>With all these improvements we reached 45 Billion parameters training a GPT model on 8 GPUs with ~1TB of CPU RAM available</strong>.</p>
<p>Also please have a look at our <a class="reference internal" href="#deepspeed-zero-stage-3-tips"><span class="std std-ref">DeepSpeed ZeRO Stage 3 Tips</span></a> which contains a lot of helpful information when configuring your own models.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When saving a model using DeepSpeed and Stage 3, model states and optimizer states will be saved in separate sharded states (based on the world size). See <a class="reference internal" href="#deepspeed-zero-stage-3-single-file"><span class="std std-ref">Collating Single File Checkpoint for DeepSpeed ZeRO Stage 3</span></a> to obtain a single checkpoint file.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_lightning</span> <span class="kn">import</span> <span class="n">Trainer</span>
<span class="kn">from</span> <span class="nn">deepspeed.ops.adam</span> <span class="kn">import</span> <span class="n">FusedAdam</span>


<span class="k">class</span> <span class="nc">MyModel</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="o">...</span>

    <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">FusedAdam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;deepspeed_stage_3&quot;</span><span class="p">,</span> <span class="n">precision</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="n">trainer</span><span class="o">.</span><span class="n">test</span><span class="p">()</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">predict</span><span class="p">()</span>
</pre></div>
</div>
<p>You can also use the Lightning Trainer to run predict or evaluate with DeepSpeed once the model has been trained.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_lightning</span> <span class="kn">import</span> <span class="n">Trainer</span>


<span class="k">class</span> <span class="nc">MyModel</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="o">...</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;deepspeed_stage_3&quot;</span><span class="p">,</span> <span class="n">precision</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">ckpt_path</span><span class="o">=</span><span class="s2">&quot;my_saved_deepspeed_checkpoint.ckpt&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="shard-model-instantly-to-reduce-initialization-time-memory">
<h3>Shard Model Instantly to Reduce Initialization Time/Memory<a class="headerlink" href="#shard-model-instantly-to-reduce-initialization-time-memory" title="Permalink to this headline">¶</a></h3>
<p>When instantiating really large models, it is sometimes necessary to shard the model layers instantly.</p>
<p>This is the case if layers may not fit on one single machines CPU or GPU memory, but would fit once sharded across multiple machines.
We expose a hook that layers initialized within the hook will be sharded instantly on a per layer basis, allowing you to instantly shard models.</p>
<p>This reduces the time taken to initialize very large models, as well as ensure we do not run out of memory when instantiating larger models. For more information you can refer to the DeepSpeed docs for <a class="reference external" href="https://deepspeed.readthedocs.io/en/latest/zero3.html">Constructing Massive Models</a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning</span> <span class="kn">import</span> <span class="n">Trainer</span>
<span class="kn">from</span> <span class="nn">deepspeed.ops.adam</span> <span class="kn">import</span> <span class="n">FusedAdam</span>


<span class="k">class</span> <span class="nc">MyModel</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="o">...</span>

    <span class="k">def</span> <span class="nf">configure_sharded_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Created within sharded model context, modules are instantly sharded across processes</span>
        <span class="c1"># as soon as they are made.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">FusedAdam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;deepspeed_stage_3&quot;</span><span class="p">,</span> <span class="n">precision</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="n">trainer</span><span class="o">.</span><span class="n">test</span><span class="p">()</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">predict</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="deepspeed-zero-stage-3-offload">
<span id="id11"></span><h3>DeepSpeed ZeRO Stage 3 Offload<a class="headerlink" href="#deepspeed-zero-stage-3-offload" title="Permalink to this headline">¶</a></h3>
<p>DeepSpeed ZeRO Stage 3 Offloads optimizer state, gradients to the host CPU to reduce memory usage as ZeRO Stage 2 does, however additionally allows you to offload the parameters as well for even more memory saving.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When saving a model using DeepSpeed and Stage 3, model states and optimizer states will be saved in separate sharded states (based on the world size). See <a class="reference internal" href="#deepspeed-zero-stage-3-single-file"><span class="std std-ref">Collating Single File Checkpoint for DeepSpeed ZeRO Stage 3</span></a> to obtain a single checkpoint file.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_lightning</span> <span class="kn">import</span> <span class="n">Trainer</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.strategies</span> <span class="kn">import</span> <span class="n">DeepSpeedStrategy</span>

<span class="c1"># Enable CPU Offloading</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;deepspeed_stage_3_offload&quot;</span><span class="p">,</span> <span class="n">precision</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># Enable CPU Offloading, and offload parameters to CPU</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
    <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span>
    <span class="n">devices</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">strategy</span><span class="o">=</span><span class="n">DeepSpeedStrategy</span><span class="p">(</span>
        <span class="n">stage</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="n">offload_optimizer</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">offload_parameters</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">precision</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="deepspeed-infinity-nvme-offloading">
<h3>DeepSpeed Infinity (NVMe Offloading)<a class="headerlink" href="#deepspeed-infinity-nvme-offloading" title="Permalink to this headline">¶</a></h3>
<p>Additionally, DeepSpeed supports offloading to NVMe drives for even larger models, utilizing the large memory space found in NVMes. DeepSpeed <a class="reference external" href="https://www.microsoft.com/en-us/research/blog/zero-infinity-and-deepspeed-unlocking-unprecedented-model-scale-for-deep-learning-training/">reports</a> the ability to fine-tune 1 Trillion+ parameters using NVMe Offloading on one 8 GPU machine. Below shows how to enable this, assuming the NVMe drive is mounted in a directory called <code class="docutils literal notranslate"><span class="pre">/local_nvme</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_lightning</span> <span class="kn">import</span> <span class="n">Trainer</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.strategies</span> <span class="kn">import</span> <span class="n">DeepSpeedStrategy</span>

<span class="c1"># Enable CPU Offloading</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;deepspeed_stage_3_offload&quot;</span><span class="p">,</span> <span class="n">precision</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># Enable CPU Offloading, and offload parameters to CPU</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
    <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span>
    <span class="n">devices</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">strategy</span><span class="o">=</span><span class="n">DeepSpeedStrategy</span><span class="p">(</span>
        <span class="n">stage</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="n">offload_optimizer</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">offload_parameters</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">remote_device</span><span class="o">=</span><span class="s2">&quot;nvme&quot;</span><span class="p">,</span>
        <span class="n">offload_params_device</span><span class="o">=</span><span class="s2">&quot;nvme&quot;</span><span class="p">,</span>
        <span class="n">offload_optimizer_device</span><span class="o">=</span><span class="s2">&quot;nvme&quot;</span><span class="p">,</span>
        <span class="n">nvme_path</span><span class="o">=</span><span class="s2">&quot;/local_nvme&quot;</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">precision</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<p>When offloading to NVMe you may notice that the speed is slow. There are parameters that need to be tuned based on the drives that you are using. Running the <a class="reference external" href="https://github.com/microsoft/DeepSpeed/blob/master/csrc/aio/py_test/aio_bench_perf_sweep.py">aio_bench_perf_sweep.py</a> script can help you to find optimum parameters. See the <a class="reference external" href="https://github.com/microsoft/DeepSpeed/issues/998">issue</a> for more information on how to parse the information.</p>
</section>
<section id="deepspeed-activation-checkpointing">
<span id="id12"></span><h3>DeepSpeed Activation Checkpointing<a class="headerlink" href="#deepspeed-activation-checkpointing" title="Permalink to this headline">¶</a></h3>
<p>Activation checkpointing frees activations from memory as soon as they are not needed during the forward pass.
They are then re-computed for the backwards pass as needed.</p>
<p>Activation checkpointing is very useful when you have intermediate layers that produce large activations.</p>
<p>This saves memory when training larger models, however requires using a checkpoint function to run modules as shown below.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Ensure to not wrap the entire model with activation checkpointing. This is not the intended usage of activation checkpointing, and will lead to failures as seen in <a class="reference external" href="https://github.com/PyTorchLightning/pytorch-lightning/discussions/9144">this discussion</a>.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_lightning</span> <span class="kn">import</span> <span class="n">Trainer</span>
<span class="kn">import</span> <span class="nn">deepspeed</span>


<span class="k">class</span> <span class="nc">MyModel</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="o">...</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Use the DeepSpeed checkpointing function instead of calling the module directly</span>
        <span class="c1"># checkpointing self.block_1 means the activations are deleted after use,</span>
        <span class="c1"># and re-calculated during the backward passes</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">deepspeed</span><span class="o">.</span><span class="n">checkpointing</span><span class="o">.</span><span class="n">checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">block_1</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_lightning</span> <span class="kn">import</span> <span class="n">Trainer</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.strategies</span> <span class="kn">import</span> <span class="n">DeepSpeedStrategy</span>
<span class="kn">import</span> <span class="nn">deepspeed</span>


<span class="k">class</span> <span class="nc">MyModel</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="o">...</span>

    <span class="k">def</span> <span class="nf">configure_sharded_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Use the DeepSpeed checkpointing function instead of calling the module directly</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">deepspeed</span><span class="o">.</span><span class="n">checkpointing</span><span class="o">.</span><span class="n">checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">block_1</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;deepspeed_stage_3_offload&quot;</span><span class="p">,</span> <span class="n">precision</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>

<span class="c1"># Enable CPU Activation Checkpointing</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
    <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span>
    <span class="n">devices</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">strategy</span><span class="o">=</span><span class="n">DeepSpeedStrategy</span><span class="p">(</span>
        <span class="n">stage</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="n">offload_optimizer</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># Enable CPU Offloading</span>
        <span class="n">cpu_checkpointing</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># (Optional) offload activations to CPU</span>
    <span class="p">),</span>
    <span class="n">precision</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="deepspeed-zero-stage-3-tips">
<span id="id13"></span><h3>DeepSpeed ZeRO Stage 3 Tips<a class="headerlink" href="#deepspeed-zero-stage-3-tips" title="Permalink to this headline">¶</a></h3>
<p>Here is some helpful information when setting up DeepSpeed ZeRO Stage 3 with Lightning.</p>
<ul class="simple">
<li><p>If you’re using Adam or AdamW, ensure to use FusedAdam or DeepSpeedCPUAdam (for CPU Offloading) rather than the default torch optimizers as they come with large speed benefits</p></li>
<li><p>Treat your GPU/CPU memory as one large pool. In some cases, you may not want to offload certain things (like activations) to provide even more space to offload model parameters</p></li>
<li><p>When offloading to the CPU, make sure to bump up the batch size as GPU memory will be freed</p></li>
<li><p>We also support sharded checkpointing. By passing <code class="docutils literal notranslate"><span class="pre">save_full_weights=False</span></code> to the <code class="docutils literal notranslate"><span class="pre">DeepSpeedStrategy</span></code>, we’ll save shards of the model which allows you to save extremely large models. However to load the model and run test/validation/predict you must use the Trainer object.</p></li>
</ul>
</section>
<section id="collating-single-file-checkpoint-for-deepspeed-zero-stage-3">
<span id="deepspeed-zero-stage-3-single-file"></span><h3>Collating Single File Checkpoint for DeepSpeed ZeRO Stage 3<a class="headerlink" href="#collating-single-file-checkpoint-for-deepspeed-zero-stage-3" title="Permalink to this headline">¶</a></h3>
<p>After training using ZeRO Stage 3, you’ll notice that your checkpoints are a directory of sharded model and optimizer states. If you’d like to collate a single file from the checkpoint directory please use the below command, which handles all the Lightning states additionally when collating the file.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_lightning.utilities.deepspeed</span> <span class="kn">import</span> <span class="n">convert_zero_checkpoint_to_fp32_state_dict</span>

<span class="c1"># lightning deepspeed has saved a directory instead of a file</span>
<span class="n">save_path</span> <span class="o">=</span> <span class="s2">&quot;lightning_logs/version_0/checkpoints/epoch=0-step=0.ckpt/&quot;</span>
<span class="n">output_path</span> <span class="o">=</span> <span class="s2">&quot;lightning_model.pt&quot;</span>
<span class="n">convert_zero_checkpoint_to_fp32_state_dict</span><span class="p">(</span><span class="n">save_path</span><span class="p">,</span> <span class="n">output_path</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This single file checkpoint does not include the optimizer/lr-scheduler states. This means we cannot restore training via the <code class="docutils literal notranslate"><span class="pre">trainer.fit(ckpt_path=)</span></code> call. Ensure to keep the sharded checkpoint directory if this is required.</p>
</div>
</section>
<section id="custom-deepspeed-config">
<h3>Custom DeepSpeed Config<a class="headerlink" href="#custom-deepspeed-config" title="Permalink to this headline">¶</a></h3>
<p>In some cases you may want to define your own DeepSpeed Config, to access all parameters defined. We’ve exposed most of the important parameters, however, there may be debugging parameters to enable. Also, DeepSpeed allows the use of custom DeepSpeed optimizers and schedulers defined within a config file that is supported.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>All strategy default parameters will be ignored when a config object is passed.
All compatible arguments can be seen in the <a class="reference external" href="https://www.deepspeed.ai/docs/config-json/">DeepSpeed docs</a>.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_lightning</span> <span class="kn">import</span> <span class="n">Trainer</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.strategies</span> <span class="kn">import</span> <span class="n">DeepSpeedStrategy</span>

<span class="n">deepspeed_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;zero_allow_untested_optimizer&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="s2">&quot;optimizer&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;OneBitAdam&quot;</span><span class="p">,</span>
        <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="mf">3e-5</span><span class="p">,</span>
            <span class="s2">&quot;betas&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.998</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">],</span>
            <span class="s2">&quot;eps&quot;</span><span class="p">:</span> <span class="mf">1e-5</span><span class="p">,</span>
            <span class="s2">&quot;weight_decay&quot;</span><span class="p">:</span> <span class="mf">1e-9</span><span class="p">,</span>
            <span class="s2">&quot;cuda_aware&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">},</span>
    <span class="s2">&quot;scheduler&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;WarmupLR&quot;</span><span class="p">,</span>
        <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;last_batch_iteration&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
            <span class="s2">&quot;warmup_min_lr&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
            <span class="s2">&quot;warmup_max_lr&quot;</span><span class="p">:</span> <span class="mf">3e-5</span><span class="p">,</span>
            <span class="s2">&quot;warmup_num_steps&quot;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">},</span>
    <span class="s2">&quot;zero_optimization&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;stage&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>  <span class="c1"># Enable Stage 2 ZeRO (Optimizer/Gradient state partitioning)</span>
        <span class="s2">&quot;offload_optimizer&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>  <span class="c1"># Enable Offloading optimizer state/calculation to the host CPU</span>
        <span class="s2">&quot;contiguous_gradients&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>  <span class="c1"># Reduce gradient fragmentation.</span>
        <span class="s2">&quot;overlap_comm&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>  <span class="c1"># Overlap reduce/backward operation of gradients for speed.</span>
        <span class="s2">&quot;allgather_bucket_size&quot;</span><span class="p">:</span> <span class="mf">2e8</span><span class="p">,</span>  <span class="c1"># Number of elements to all gather at once.</span>
        <span class="s2">&quot;reduce_bucket_size&quot;</span><span class="p">:</span> <span class="mf">2e8</span><span class="p">,</span>  <span class="c1"># Number of elements we reduce/allreduce at once.</span>
    <span class="p">},</span>
<span class="p">}</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="n">DeepSpeedStrategy</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">deepspeed_config</span><span class="p">),</span> <span class="n">precision</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<p>We support taking the config as a json formatted file:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_lightning</span> <span class="kn">import</span> <span class="n">Trainer</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.strategies</span> <span class="kn">import</span> <span class="n">DeepSpeedStrategy</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
    <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="n">DeepSpeedStrategy</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="s2">&quot;/path/to/deepspeed_config.json&quot;</span><span class="p">),</span> <span class="n">precision</span><span class="o">=</span><span class="mi">16</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<p>You can use also use an environment variable via your PyTorch Lightning script:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">PL_DEEPSPEED_CONFIG_PATH</span><span class="o">=</span>/path/to/deepspeed_config.json python train.py --strategy deepspeed
</pre></div>
</div>
<hr class="docutils" />
</section>
</section>
<section id="ddp-optimizations">
<span id="id14"></span><h2>DDP Optimizations<a class="headerlink" href="#ddp-optimizations" title="Permalink to this headline">¶</a></h2>
<section id="when-using-ddp-strategies-set-find-unused-parameters-false">
<h3>When Using DDP Strategies, Set find_unused_parameters=False<a class="headerlink" href="#when-using-ddp-strategies-set-find-unused-parameters-false" title="Permalink to this headline">¶</a></h3>
<p>By default, we have set <code class="docutils literal notranslate"><span class="pre">find_unused_parameters=True</span></code> for compatibility reasons that have been observed in the past (refer to the <a class="reference external" href="https://github.com/PyTorchLightning/pytorch-lightning/discussions/6219">discussion</a> for more details).
When enabled, it can result in a performance hit and can be disabled in most cases. Read more about it <a class="reference external" href="https://pytorch.org/docs/stable/notes/ddp.html#internal-design">here</a>.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>It applies to all DDP strategies that support <code class="docutils literal notranslate"><span class="pre">find_unused_parameters</span></code> as input.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_lightning.strategies</span> <span class="kn">import</span> <span class="n">DDPStrategy</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span>
    <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span>
    <span class="n">devices</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">strategy</span><span class="o">=</span><span class="n">DDPStrategy</span><span class="p">(</span><span class="n">find_unused_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_lightning.strategies</span> <span class="kn">import</span> <span class="n">DDPSpawnStrategy</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span>
    <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span>
    <span class="n">devices</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">strategy</span><span class="o">=</span><span class="n">DDPSpawnStrategy</span><span class="p">(</span><span class="n">find_unused_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="ddp-static-graph">
<h3>DDP Static Graph<a class="headerlink" href="#ddp-static-graph" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://pytorch.org/blog/pytorch-1.11-released/#stable-ddp-static-graph">DDP static graph</a> assumes that your model
employs the same set of used/unused parameters in every iteration, so that it can deterministically know the flow of
training and apply special optimizations during runtime.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>DDP static graph support requires PyTorch&gt;=1.11.0</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_lightning</span> <span class="kn">import</span> <span class="n">Trainer</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.strategies</span> <span class="kn">import</span> <span class="n">DDPStrategy</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">devices</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="n">DDPStrategy</span><span class="p">(</span><span class="n">static_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="when-using-ddp-on-a-multi-node-cluster-set-nccl-parameters">
<h3>When Using DDP on a Multi-node Cluster, Set NCCL Parameters<a class="headerlink" href="#when-using-ddp-on-a-multi-node-cluster-set-nccl-parameters" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://developer.nvidia.com/nccl">NCCL</a> is the NVIDIA Collective Communications Library that is used by PyTorch to handle communication across nodes and GPUs. There are reported benefits in terms of speedups when adjusting NCCL parameters as seen in this <a class="reference external" href="https://github.com/PyTorchLightning/pytorch-lightning/issues/7179">issue</a>. In the issue, we see a 30% speed improvement when training the Transformer XLM-RoBERTa and a 15% improvement in training with Detectron2.</p>
<p>NCCL parameters can be adjusted via environment variables.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>AWS and GCP already set default values for these on their clusters. This is typically useful for custom cluster setups.</p>
</div>
<ul class="simple">
<li><p><a class="reference external" href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/env.html#nccl-nsocks-perthread">NCCL_NSOCKS_PERTHREAD</a></p></li>
<li><p><a class="reference external" href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/env.html#nccl-socket-nthreads">NCCL_SOCKET_NTHREADS</a></p></li>
<li><p><a class="reference external" href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/env.html#nccl-min-nchannels">NCCL_MIN_NCHANNELS</a></p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">NCCL_NSOCKS_PERTHREAD</span><span class="o">=</span><span class="m">4</span>
<span class="nb">export</span> <span class="nv">NCCL_SOCKET_NTHREADS</span><span class="o">=</span><span class="m">2</span>
</pre></div>
</div>
</section>
<section id="gradients-as-bucket-view">
<h3>Gradients as Bucket View<a class="headerlink" href="#gradients-as-bucket-view" title="Permalink to this headline">¶</a></h3>
<p>Enabling <code class="docutils literal notranslate"><span class="pre">gradient_as_bucket_view=True</span></code> in the <code class="docutils literal notranslate"><span class="pre">DDPStrategy</span></code> will make gradients views point to different offsets of the <code class="docutils literal notranslate"><span class="pre">allreduce</span></code> communication buckets. See <code class="xref py py-class docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> for more information.</p>
<p>This can reduce peak memory usage and throughput as saved memory will be equal to the total gradient memory + removes the need to copy gradients to the <code class="docutils literal notranslate"><span class="pre">allreduce</span></code> communication buckets.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When <code class="docutils literal notranslate"><span class="pre">gradient_as_bucket_view=True</span></code> you cannot call <code class="docutils literal notranslate"><span class="pre">detach_()</span></code> on gradients. If hitting such errors, please fix it by referring to the <code class="xref py py-meth docutils literal notranslate"><span class="pre">zero_grad()</span></code> function in <code class="docutils literal notranslate"><span class="pre">torch/optim/optimizer.py</span></code> as a solution (<a class="reference external" href="https://pytorch.org/docs/master/_modules/torch/nn/parallel/distributed.html#DistributedDataParallel">source</a>).</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_lightning</span> <span class="kn">import</span> <span class="n">Trainer</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.strategies</span> <span class="kn">import</span> <span class="n">DDPStrategy</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="n">DDPStrategy</span><span class="p">(</span><span class="n">gradient_as_bucket_view</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="ddp-communication-hooks">
<h3>DDP Communication Hooks<a class="headerlink" href="#ddp-communication-hooks" title="Permalink to this headline">¶</a></h3>
<p>DDP Communication hooks is an interface to control how gradients are communicated across workers, overriding the standard allreduce in DistributedDataParallel. This allows you to enable performance improving communication hooks when using multiple nodes.</p>
<p>Enable <a class="reference external" href="https://pytorch.org/docs/stable/ddp_comm_hooks.html#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_hook">FP16 Compress Hook for multi-node throughput improvement</a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_lightning</span> <span class="kn">import</span> <span class="n">Trainer</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.strategies</span> <span class="kn">import</span> <span class="n">DDPStrategy</span>
<span class="kn">from</span> <span class="nn">torch.distributed.algorithms.ddp_comm_hooks</span> <span class="kn">import</span> <span class="n">default_hooks</span> <span class="k">as</span> <span class="n">default</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="n">DDPStrategy</span><span class="p">(</span><span class="n">ddp_comm_hook</span><span class="o">=</span><span class="n">default</span><span class="o">.</span><span class="n">fp16_compress_hook</span><span class="p">))</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<p>Enable <a class="reference external" href="https://pytorch.org/docs/stable/ddp_comm_hooks.html#powersgd-communication-hook">PowerSGD for multi-node throughput improvement</a>:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>PowerSGD typically requires extra memory of the same size as the model’s gradients to enable error feedback, which can compensate for biased compressed communication and improve accuracy (<a class="reference external" href="https://pytorch.org/docs/stable/ddp_comm_hooks.html#powersgd-hooks">source</a>).</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_lightning</span> <span class="kn">import</span> <span class="n">Trainer</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.strategies</span> <span class="kn">import</span> <span class="n">DDPStrategy</span>
<span class="kn">from</span> <span class="nn">torch.distributed.algorithms.ddp_comm_hooks</span> <span class="kn">import</span> <span class="n">powerSGD_hook</span> <span class="k">as</span> <span class="n">powerSGD</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
    <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span>
    <span class="n">devices</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">strategy</span><span class="o">=</span><span class="n">DDPStrategy</span><span class="p">(</span>
        <span class="n">ddp_comm_state</span><span class="o">=</span><span class="n">powerSGD</span><span class="o">.</span><span class="n">PowerSGDState</span><span class="p">(</span>
            <span class="n">process_group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">matrix_approximation_rank</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">start_powerSGD_iter</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">ddp_comm_hook</span><span class="o">=</span><span class="n">powerSGD</span><span class="o">.</span><span class="n">powerSGD_hook</span><span class="p">,</span>
    <span class="p">),</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<p>Combine hooks for accumulated benefit:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>DDP communication wrappers support requires PyTorch&gt;=1.9.0</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_lightning</span> <span class="kn">import</span> <span class="n">Trainer</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.strategies</span> <span class="kn">import</span> <span class="n">DDPStrategy</span>
<span class="kn">from</span> <span class="nn">torch.distributed.algorithms.ddp_comm_hooks</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">default_hooks</span> <span class="k">as</span> <span class="n">default</span><span class="p">,</span>
    <span class="n">powerSGD_hook</span> <span class="k">as</span> <span class="n">powerSGD</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
    <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span>
    <span class="n">devices</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">strategy</span><span class="o">=</span><span class="n">DDPStrategy</span><span class="p">(</span>
        <span class="n">ddp_comm_state</span><span class="o">=</span><span class="n">powerSGD</span><span class="o">.</span><span class="n">PowerSGDState</span><span class="p">(</span>
            <span class="n">process_group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">matrix_approximation_rank</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">start_powerSGD_iter</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">ddp_comm_hook</span><span class="o">=</span><span class="n">powerSGD</span><span class="o">.</span><span class="n">powerSGD_hook</span><span class="p">,</span>
        <span class="n">ddp_comm_wrapper</span><span class="o">=</span><span class="n">default</span><span class="o">.</span><span class="n">fp16_compress_wrapper</span><span class="p">,</span>
    <span class="p">),</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<p>When using Post-localSGD, you must also pass <code class="docutils literal notranslate"><span class="pre">model_averaging_period</span></code> to allow for model parameter averaging:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Post-localSGD support requires PyTorch&gt;=1.10.0</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_lightning</span> <span class="kn">import</span> <span class="n">Trainer</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.strategies</span> <span class="kn">import</span> <span class="n">DDPStrategy</span>
<span class="kn">from</span> <span class="nn">torch.distributed.algorithms.ddp_comm_hooks</span> <span class="kn">import</span> <span class="n">post_localSGD_hook</span> <span class="k">as</span> <span class="n">post_localSGD</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
    <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span>
    <span class="n">devices</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">strategy</span><span class="o">=</span><span class="n">DDPStrategy</span><span class="p">(</span>
        <span class="n">ddp_comm_state</span><span class="o">=</span><span class="n">post_localSGD</span><span class="o">.</span><span class="n">PostLocalSGDState</span><span class="p">(</span>
            <span class="n">process_group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">subgroup</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">start_localSGD_iter</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">ddp_comm_hook</span><span class="o">=</span><span class="n">post_localSGD</span><span class="o">.</span><span class="n">post_localSGD_hook</span><span class="p">,</span>
        <span class="n">model_averaging_period</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="p">),</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../clouds/cloud_training.html" class="btn btn-neutral float-right" title="Train on the cloud" accesskey="n" rel="next">Next <img src="../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="../clouds/cluster.html" class="btn btn-neutral" title="Run on an on-prem cluster" accesskey="p" rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright Copyright (c) 2018-2022, William Falcon et al...

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Train 1 trillion+ parameter models</a><ul>
<li><a class="reference internal" href="#choosing-an-advanced-distributed-gpu-strategy">Choosing an Advanced Distributed GPU Strategy</a><ul>
<li><a class="reference internal" href="#pre-training-vs-fine-tuning">Pre-training vs Fine-tuning</a></li>
<li><a class="reference internal" href="#when-shouldn-t-i-use-an-optimized-distributed-strategy">When Shouldn’t I use an Optimized Distributed Strategy?</a></li>
</ul>
</li>
<li><a class="reference internal" href="#sharded-training">Sharded Training</a></li>
<li><a class="reference internal" href="#fully-sharded-training">Fully Sharded Training</a><ul>
<li><a class="reference internal" href="#shard-parameters-to-reach-10-billion-parameters">Shard Parameters to Reach 10+ Billion Parameters</a></li>
<li><a class="reference internal" href="#enabling-module-sharding-for-maximum-memory-efficiency">Enabling Module Sharding for Maximum Memory Efficiency</a></li>
</ul>
</li>
<li><a class="reference internal" href="#fairscale-activation-checkpointing">FairScale Activation Checkpointing</a></li>
<li><a class="reference internal" href="#deepspeed">DeepSpeed</a><ul>
<li><a class="reference internal" href="#deepspeed-zero-stage-1">DeepSpeed ZeRO Stage 1</a></li>
<li><a class="reference internal" href="#deepspeed-zero-stage-2">DeepSpeed ZeRO Stage 2</a></li>
<li><a class="reference internal" href="#deepspeed-zero-stage-2-offload">DeepSpeed ZeRO Stage 2 Offload</a></li>
<li><a class="reference internal" href="#deepspeed-zero-stage-3">DeepSpeed ZeRO Stage 3</a></li>
<li><a class="reference internal" href="#shard-model-instantly-to-reduce-initialization-time-memory">Shard Model Instantly to Reduce Initialization Time/Memory</a></li>
<li><a class="reference internal" href="#deepspeed-zero-stage-3-offload">DeepSpeed ZeRO Stage 3 Offload</a></li>
<li><a class="reference internal" href="#deepspeed-infinity-nvme-offloading">DeepSpeed Infinity (NVMe Offloading)</a></li>
<li><a class="reference internal" href="#deepspeed-activation-checkpointing">DeepSpeed Activation Checkpointing</a></li>
<li><a class="reference internal" href="#deepspeed-zero-stage-3-tips">DeepSpeed ZeRO Stage 3 Tips</a></li>
<li><a class="reference internal" href="#collating-single-file-checkpoint-for-deepspeed-zero-stage-3">Collating Single File Checkpoint for DeepSpeed ZeRO Stage 3</a></li>
<li><a class="reference internal" href="#custom-deepspeed-config">Custom DeepSpeed Config</a></li>
</ul>
</li>
<li><a class="reference internal" href="#ddp-optimizations">DDP Optimizations</a><ul>
<li><a class="reference internal" href="#when-using-ddp-strategies-set-find-unused-parameters-false">When Using DDP Strategies, Set find_unused_parameters=False</a></li>
<li><a class="reference internal" href="#ddp-static-graph">DDP Static Graph</a></li>
<li><a class="reference internal" href="#when-using-ddp-on-a-multi-node-cluster-set-nccl-parameters">When Using DDP on a Multi-node Cluster, Set NCCL Parameters</a></li>
<li><a class="reference internal" href="#gradients-as-bucket-view">Gradients as Bucket View</a></li>
<li><a class="reference internal" href="#ddp-communication-hooks">DDP Communication Hooks</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
         <script src="../_static/jquery.js"></script>
         <script src="../_static/underscore.js"></script>
         <script src="../_static/doctools.js"></script>
         <script src="../_static/clipboard.min.js"></script>
         <script src="../_static/copybutton.js"></script>
         <script src="../_static/copybutton.js"></script>
         <script>let toggleHintShow = 'Click to show';</script>
         <script>let toggleHintHide = 'Click to hide';</script>
         <script>let toggleOpenOnPrint = 'true';</script>
         <script src="../_static/togglebutton.js"></script>
         <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Best practices', 'Optional Extensions', 'Tutorials', 'API References', 'Bolts', 'Examples', 'Partner Domain Frameworks', 'Community'];
</script>



  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <!--
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch-lightning.rtfd.io/en/latest">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch-lightning.readthedocs.io/en/latest/#tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch-lightning.readthedocs.io/en/latest/#community-examples">View Resources</a>
        </div>
        -->
      </div>
    </div>
  </div>

  <!--
  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch-lightning.rtfd.io/en/latest/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch-lightning.rtfd.io/en/latest/">PyTorch</a></li>
            <li><a href="https://pytorch-lightning.readthedocs.io/en/latest/starter/introduction.html">Get Started</a></li>
            <li><a href="https://pytorch-lightning.rtfd.io/en/latest/">Features</a></li>
            <li><a href="">Ecosystem</a></li>
            <li><a href="https://www.pytorchlightning.ai/blog">Blog</a></li>
            <li><a href="https://github.com/PyTorchLightning/pytorch-lightning/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch-lightning.readthedocs.io/en/latest/#community-examples">Resources</a></li>
            <li><a href="https://pytorch-lightning.readthedocs.io/en/latest/#tutorials">Tutorials</a></li>
            <li><a href="https://pytorch-lightning.rtfd.io/en/latest">Docs</a></li>
            <li><a href="https://www.pytorchlightning.ai/community" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/PyTorchLightning/pytorch-lightning/issues" target="_blank">Github Issues</a></li>
            <li><a href="" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Stay Connected</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/PyTorchLightnin" target="_blank" class="twitter"></a>
            <a href="" target="_blank" class="youtube"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>
  -->

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. Read PyTorch Lightning's <a href="https://pytorchlightning.ai/privacy-policy">Privacy Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch-lightning.rtfd.io/en/latest/" aria-label="PyTorch Lightning"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <!-- <li>
            <a href="https://pytorch-lightning.readthedocs.io/en/latest/starter/introduction.html">Get Started</a>
          </li>

          <li>
            <a href="https://www.pytorchlightning.ai/blog">Blog</a>
          </li>

          <li class="resources-mobile-menu-title">
            Ecosystem
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch-lightning.readthedocs.io/en/stable/">PyTorch Lightning</a>
            </li>

            <li>
              <a href="https://torchmetrics.readthedocs.io/en/stable/">TorchMetrics</a>
            </li>

            <li>
              <a href="https://lightning-flash.readthedocs.io/en/stable/">Lightning Flash</a>
            </li>

            <li>
              <a href="https://lightning-transformers.readthedocs.io/en/stable/">Lightning Transformers</a>
            </li>

            <li>
              <a href="https://lightning-bolts.readthedocs.io/en/stable/">Lightning Bolts</a>
            </li>
          </ul> -->

          <!--<li class="resources-mobile-menu-title">
            Resources
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch-lightning.readthedocs.io/en/latest/#community-examples">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch-lightning.rtfd.io/en/latest/">About</a>
            </li>

            <li>
              <a href="">Models (Beta)</a>
            </li>

            <li>
              <a href="https://www.pytorchlightning.ai/community">Community</a>
            </li>

            <li>
              <a href="https://github.com/PyTorchLightning/pytorch-lightning/discussions">Forums</a>
            </li>
          </ul>-->

          <!-- <li>
            <a href="https://github.com/PyTorchLightning/pytorch-lightning">Github</a>
          </li> -->

          <!-- <li>
            <a href="https://www.grid.ai/">Grid.ai</a>
          </li> -->
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>

 </body>
</html>