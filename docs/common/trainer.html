


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>

  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="google-site-verification" content="okUst94cAlWSsUsGZTB4xSS4UKTtRV8Nu5XZ9pdd3Aw" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Trainer &mdash; PyTorch Lightning 1.7.0dev documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/icon.svg"/>
  
  
  
    <link rel="canonical" href="https://pytorch-lightning.readthedocs.io/en/stable//common/trainer.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../_static/togglebutton.css" type="text/css" />
  <link rel="stylesheet" href="../_static/main.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sphinx_paramlinks.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Add validation and test datasets" href="evaluation.html" />
    <link rel="prev" title="LightningModule" href="lightning_module.html" />
  <!-- Google Analytics -->
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-82W25RV60Q"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-82W25RV60Q');
    </script>
  
  <!-- End Google Analytics -->
  

  
  <script src="../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/UCity/UCity-Light.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/UCity/UCity-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/UCity/UCity-Semibold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/Inconsolata/Inconsolata.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <script defer src="https://use.fontawesome.com/releases/v6.1.1/js/all.js" integrity="sha384-xBXmu0dk1bEoiwd71wOonQLyH+VpgR1XcDH3rtxrLww5ajNTuMvBdL5SOiFZnNdp" crossorigin="anonymous"></script>
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch-lightning.rtfd.io/en/latest/" aria-label="PyTorch Lightning">
      <!--  <img class="call-to-action-img" src="../_static/images/logo-lightning-icon.png"/> -->
      </a>

      <div class="main-menu">
        <ul>
          <!-- <li>
            <a href="https://pytorch-lightning.readthedocs.io/en/latest/starter/introduction.html">Get Started</a>
          </li> -->

          <!-- <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch-lightning.readthedocs.io/en/stable/">
                  <span class="dropdown-title">PyTorch Lightning</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://torchmetrics.readthedocs.io/en/stable/">
                  <span class="dropdown-title">TorchMetrics</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning-flash.readthedocs.io/en/stable/">
                  <span class="dropdown-title">Lightning Flash</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning-transformers.readthedocs.io/en/stable/">
                  <span class="dropdown-title">Lightning Transformers</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning-bolts.readthedocs.io/en/stable/">
                  <span class="dropdown-title">Lightning Bolts</span>
                </a>
            </div>
          </li> -->

          <!--<li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://www.pytorchlightning.ai/community">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch-lightning.readthedocs.io/en/latest/#community-examples">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://github.com/PyTorchLightning/pytorch-lightning/discussions" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>-->

          <!-- <li>
            <a href="https://github.com/PyTorchLightning/pytorch-lightning">GitHub</a>
          </li>

          <li>
            <a href="https://www.grid.ai/">Train on the cloud</a>
          </li> -->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  1.7.0dev
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../starter/introduction.html">Lightning in 15 minutes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../starter/converting.html">Organize existing PyTorch into Lightning</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Level Up</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../levels/core_skills.html">Basic skills</a></li>
<li class="toctree-l1"><a class="reference internal" href="../levels/intermediate.html">Intermediate skills</a></li>
<li class="toctree-l1"><a class="reference internal" href="../levels/advanced.html">Advanced skills</a></li>
<li class="toctree-l1"><a class="reference internal" href="../levels/expert.html">Expert skills</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Core API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="lightning_module.html">LightningModule</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Trainer</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Common Workflows</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="evaluation.html">Avoid overfitting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model/build_model.html">Build a Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="hyperparameters.html">Configure hyperparameters from the CLI</a></li>
<li class="toctree-l1"><a class="reference internal" href="progress_bar.html">Customize the progress bar</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/production.html">Deploy models into production</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/training_tricks.html">Effective Training Techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cli/lightning_cli.html">Eliminate config boilerplate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tuning/profiler.html">Find bottlenecks in your code</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/transfer_learning.html">Finetune a model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../visualize/logging_intermediate.html">Manage experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../clouds/cluster.html">Run on an on-prem cluster</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/model_parallel.html">Train 1 trillion+ parameter models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../clouds/cloud_training.html">Train on the cloud</a></li>
<li class="toctree-l1"><a class="reference internal" href="checkpointing.html">Save and load model progress</a></li>
<li class="toctree-l1"><a class="reference internal" href="precision.html">Save memory with half-precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accelerators/gpu.html">Train on single or multiple GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accelerators/hpu.html">Train on single or multiple HPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accelerators/ipu.html">Train on single or multiple IPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accelerators/tpu.html">Train on single or multiple TPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model/own_your_loop.html">Use a pure PyTorch training loop</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Glossary</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../extensions/accelerator.html">Accelerators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../extensions/callbacks.html">Callback</a></li>
<li class="toctree-l1"><a class="reference internal" href="checkpointing.html">Checkpointing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../clouds/cluster.html">Cluster</a></li>
<li class="toctree-l1"><a class="reference internal" href="checkpointing_advanced.html">Cloud checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="console_logs.html">Console Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/debugging.html">Debugging</a></li>
<li class="toctree-l1"><a class="reference internal" href="early_stopping.html">Early stopping</a></li>
<li class="toctree-l1"><a class="reference internal" href="../visualize/experiment_managers.html">Experiment manager (Logger)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../clouds/fault_tolerant_training.html">Fault tolerant training</a></li>
<li class="toctree-l1"><a class="reference external" href="https://lightning-flash.readthedocs.io/en/stable/">Flash</a></li>
<li class="toctree-l1"><a class="reference internal" href="../clouds/cloud_training.html">Grid AI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accelerators/gpu.html">GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="precision.html">Half precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accelerators/hpu.html">HPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/production_intermediate.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accelerators/ipu.html">IPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cli/lightning_cli.html">Lightning CLI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model/build_model_expert.html">Raw PyTorch loop (expert)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model/build_model_expert.html#lightninglite-stepping-stone-to-lightning">LightningLite (Stepping Stone to Lightning)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data/datamodule.html">LightningDataModule</a></li>
<li class="toctree-l1"><a class="reference internal" href="lightning_module.html">LightningModule</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch-lightning.readthedocs.io/en/stable/ecosystem/transformers.html">Lightning Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../visualize/loggers.html">Log</a></li>
<li class="toctree-l1"><a class="reference internal" href="../extensions/loops.html">Loops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accelerators/tpu.html">TPU</a></li>
<li class="toctree-l1"><a class="reference external" href="https://torchmetrics.readthedocs.io/en/stable/">Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model/build_model.html">Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/model_parallel.html">Model Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../extensions/plugins.html">Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="progress_bar.html">Progress bar</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/production_advanced.html">Production</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/production_basic.html">Predict</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tuning/profiler.html">Profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/pruning_quantization.html">Pruning and Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="remote_fs.html">Remote filesystem and FSSPEC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/strategy_registry.html">Strategy registry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../starter/style_guide.html">Style guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../clouds/run_intermediate.html">Sweep</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/training_tricks.html">SWA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../clouds/cluster_advanced.html">SLURM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/transfer_learning.html">Transfer learning</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../clouds/cluster_intermediate_2.html">Torch distributed</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Hands-on Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.youtube.com/playlist?list=PLaMu-SDt_RB5NUm67hU2pdE75j6KaIOv2">PyTorch Lightning 101 class</a></li>
<li class="toctree-l1"><a class="reference external" href="https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09">From PyTorch to PyTorch Lightning [Blog]</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.youtube.com/watch?v=QHww1JH7IDU">From PyTorch to PyTorch Lightning [Video]</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>Trainer</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/common/trainer.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <section id="trainer">
<span id="id1"></span><h1>Trainer<a class="headerlink" href="#trainer" title="Permalink to this headline">¶</a></h1>
<p>Once you’ve organized your PyTorch code into a LightningModule,
the Trainer automates everything else.</p>
<video width="100%" max-width="800px" controls autoplay
src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/pt_trainer_mov.m4v"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<p>This abstraction achieves the following:</p>
<ol class="arabic simple">
<li><p>You maintain control over all aspects via PyTorch code without an added abstraction.</p></li>
<li><p>The trainer uses best practices embedded by contributors and users
from top AI labs such as Facebook AI Research, NYU, MIT, Stanford, etc…</p></li>
<li><p>The trainer allows overriding any key part that you don’t want automated.</p></li>
</ol>
<div class="line-block">
<div class="line"><br /></div>
</div>
<hr class="docutils" />
<section id="basic-use">
<h2>Basic use<a class="headerlink" href="#basic-use" title="Permalink to this headline">¶</a></h2>
<p>This is the basic use of the trainer:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">MyLightningModule</span><span class="p">()</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">()</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">val_dataloader</span><span class="p">)</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="under-the-hood">
<h2>Under the hood<a class="headerlink" href="#under-the-hood" title="Permalink to this headline">¶</a></h2>
<p>Under the hood, the Lightning Trainer handles the training loop details for you, some examples include:</p>
<ul class="simple">
<li><p>Automatically enabling/disabling grads</p></li>
<li><p>Running the training, validation and test dataloaders</p></li>
<li><p>Calling the Callbacks at the appropriate times</p></li>
<li><p>Putting batches and computations on the correct devices</p></li>
</ul>
<p>Here’s the pseudocode for what the trainer does under the hood (showing the train loop only)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># put model in train mode</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_dataloader</span><span class="p">:</span>
    <span class="c1"># calls hooks like this one</span>
    <span class="n">on_train_batch_start</span><span class="p">()</span>

    <span class="c1"># train step</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">training_step</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

    <span class="c1"># clear gradients</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># backward</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># update parameters</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="trainer-in-python-scripts">
<h2>Trainer in Python scripts<a class="headerlink" href="#trainer-in-python-scripts" title="Permalink to this headline">¶</a></h2>
<p>In Python scripts, it’s recommended you use a main function to call the Trainer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">argparse</span> <span class="kn">import</span> <span class="n">ArgumentParser</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">hparams</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">LightningModule</span><span class="p">()</span>
    <span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="n">hparams</span><span class="o">.</span><span class="n">accelerator</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="n">hparams</span><span class="o">.</span><span class="n">devices</span><span class="p">)</span>
    <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">ArgumentParser</span><span class="p">()</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--accelerator&quot;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--devices&quot;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="n">main</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
</pre></div>
</div>
<p>So you can run it like so:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python main.py --accelerator <span class="s1">&#39;gpu&#39;</span> --devices <span class="m">2</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Pro-tip: You don’t need to define all flags manually. Lightning can add them automatically</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">argparse</span> <span class="kn">import</span> <span class="n">ArgumentParser</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">LightningModule</span><span class="p">()</span>
    <span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="o">.</span><span class="n">from_argparse_args</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
    <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">ArgumentParser</span><span class="p">()</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">Trainer</span><span class="o">.</span><span class="n">add_argparse_args</span><span class="p">(</span><span class="n">parser</span><span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="n">main</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
</pre></div>
</div>
<p>So you can run it like so:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python main.py --accelerator <span class="s1">&#39;gpu&#39;</span> --devices <span class="m">2</span> --max_steps <span class="m">10</span> --limit_train_batches <span class="m">10</span> --any_trainer_arg x
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to stop a training run early, you can press “Ctrl + C” on your keyboard.
The trainer will catch the <code class="docutils literal notranslate"><span class="pre">KeyboardInterrupt</span></code> and attempt a graceful shutdown, including
running accelerator callback <code class="docutils literal notranslate"><span class="pre">on_train_end</span></code> to clean up memory. The trainer object will also set
an attribute <code class="docutils literal notranslate"><span class="pre">interrupted</span></code> to <code class="docutils literal notranslate"><span class="pre">True</span></code> in such cases. If you have a callback which shuts down compute
resources, for example, you can conditionally run the shutdown logic for only uninterrupted runs.</p>
</div>
</section>
<hr class="docutils" />
<section id="validation">
<h2>Validation<a class="headerlink" href="#validation" title="Permalink to this headline">¶</a></h2>
<p>You can perform an evaluation epoch over the validation set, outside of the training loop,
using <code class="xref py py-meth docutils literal notranslate"><span class="pre">validate()</span></code>. This might be
useful if you want to collect new metrics from a model right at its initialization
or after it has already been trained.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">validate</span><span class="p">(</span><span class="n">dataloaders</span><span class="o">=</span><span class="n">val_dataloaders</span><span class="p">)</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="testing">
<h2>Testing<a class="headerlink" href="#testing" title="Permalink to this headline">¶</a></h2>
<p>Once you’re done training, feel free to run the test set!
(Only right before publishing your paper or pushing to production)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">dataloaders</span><span class="o">=</span><span class="n">test_dataloaders</span><span class="p">)</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="reproducibility">
<h2>Reproducibility<a class="headerlink" href="#reproducibility" title="Permalink to this headline">¶</a></h2>
<p>To ensure full reproducibility from run to run you need to set seeds for pseudo-random generators,
and set <code class="docutils literal notranslate"><span class="pre">deterministic</span></code> flag in <code class="docutils literal notranslate"><span class="pre">Trainer</span></code>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_lightning</span> <span class="kn">import</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">seed_everything</span>

<span class="n">seed_everything</span><span class="p">(</span><span class="mi">42</span><span class="p">,</span> <span class="n">workers</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># sets seeds for numpy, torch and python.random.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">deterministic</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>By setting <code class="docutils literal notranslate"><span class="pre">workers=True</span></code> in <code class="xref py py-func docutils literal notranslate"><span class="pre">seed_everything()</span></code>, Lightning derives
unique seeds across all dataloader workers and processes for <code class="xref py py-mod docutils literal notranslate"><span class="pre">torch</span></code>, <code class="xref py py-mod docutils literal notranslate"><span class="pre">numpy</span></code> and stdlib
<code class="xref py py-mod docutils literal notranslate"><span class="pre">random</span></code> number generators. When turned on, it ensures that e.g. data augmentations are not repeated across workers.</p>
<hr class="docutils" />
</section>
<section id="trainer-flags">
<span id="id2"></span><h2>Trainer flags<a class="headerlink" href="#trainer-flags" title="Permalink to this headline">¶</a></h2>
<section id="accelerator">
<h3>accelerator<a class="headerlink" href="#accelerator" title="Permalink to this headline">¶</a></h3>
<p>Supports passing different accelerator types (<code class="docutils literal notranslate"><span class="pre">&quot;cpu&quot;,</span> <span class="pre">&quot;gpu&quot;,</span> <span class="pre">&quot;tpu&quot;,</span> <span class="pre">&quot;ipu&quot;,</span> <span class="pre">&quot;auto&quot;</span></code>)
as well as custom accelerator instances.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># CPU accelerator</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

<span class="c1"># Training with GPU Accelerator using 2 GPUs</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">devices</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">)</span>

<span class="c1"># Training with TPU Accelerator using 8 tpu cores</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">devices</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;tpu&quot;</span><span class="p">)</span>

<span class="c1"># Training with GPU Accelerator using the DistributedDataParallel strategy</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">devices</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;ddp&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="docutils literal notranslate"><span class="pre">&quot;auto&quot;</span></code> option recognizes the machine you are on, and selects the respective <code class="docutils literal notranslate"><span class="pre">Accelerator</span></code>.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># If your machine has GPUs, it will use the GPU Accelerator for training</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">devices</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>You can also modify hardware behavior by subclassing an existing accelerator to adjust for your needs.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyOwnAcc</span><span class="p">(</span><span class="n">CPUAccelerator</span><span class="p">):</span>
    <span class="o">...</span>

<span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="n">MyOwnAcc</span><span class="p">())</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">devices</span></code> flag is not defined, it will assume <code class="docutils literal notranslate"><span class="pre">devices</span></code> to be <code class="docutils literal notranslate"><span class="pre">&quot;auto&quot;</span></code> and fetch the <code class="docutils literal notranslate"><span class="pre">auto_device_count</span></code>
from the accelerator.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># This is part of the built-in `GPUAccelerator`</span>
<span class="k">class</span> <span class="nc">GPUAccelerator</span><span class="p">(</span><span class="n">Accelerator</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Accelerator for GPU devices.&quot;&quot;&quot;</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">auto_device_count</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Get the devices when set to auto.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>


<span class="c1"># Training with GPU Accelerator using total number of gpus available on the system</span>
<span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Passing training strategies (e.g., <code class="docutils literal notranslate"><span class="pre">&quot;ddp&quot;</span></code>) to <code class="docutils literal notranslate"><span class="pre">accelerator</span></code> has been deprecated in v1.5.0
and will be removed in v1.7.0. Please use the <code class="docutils literal notranslate"><span class="pre">strategy</span></code> argument instead.</p>
</div>
</section>
<section id="accumulate-grad-batches">
<h3>accumulate_grad_batches<a class="headerlink" href="#accumulate-grad-batches" title="Permalink to this headline">¶</a></h3>
<video width="50%" max-width="400px" controls
poster="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/accumulate_grad_batches.jpg"
src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/accumulate_grad_batches.mp4"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<p>Accumulates grads every k batches or as set up in the dict.
Trainer also calls <code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code> for the last indivisible step number.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># default used by the Trainer (no accumulation)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">accumulate_grad_batches</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># accumulate every 4 batches (effective batch size is batch*4)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">accumulate_grad_batches</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="c1"># no accumulation for epochs 1-4. accumulate 3 for epochs 5-10. accumulate 20 after that</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">accumulate_grad_batches</span><span class="o">=</span><span class="p">{</span><span class="mi">5</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">:</span> <span class="mi">20</span><span class="p">})</span>
</pre></div>
</div>
</section>
<section id="amp-backend">
<h3>amp_backend<a class="headerlink" href="#amp-backend" title="Permalink to this headline">¶</a></h3>
<video width="50%" max-width="400px" controls
poster="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/amp_backend.jpg"
src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/amp_backend.mp4"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<p>Use PyTorch AMP (‘native’), or NVIDIA apex (‘apex’).</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># using PyTorch built-in AMP, default used by the Trainer</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">amp_backend</span><span class="o">=</span><span class="s2">&quot;native&quot;</span><span class="p">)</span>

<span class="c1"># using NVIDIA Apex</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">amp_backend</span><span class="o">=</span><span class="s2">&quot;apex&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="amp-level">
<h3>amp_level<a class="headerlink" href="#amp-level" title="Permalink to this headline">¶</a></h3>
<video width="50%" max-width="400px" controls
poster="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/amp_level.jpg"
src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/amp_level.mp4"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<p>The optimization level to use (O1, O2, etc…)
for 16-bit GPU precision (using NVIDIA apex under the hood).</p>
<p>Check <a class="reference external" href="https://nvidia.github.io/apex/amp.html#opt-levels">NVIDIA apex docs</a> for level</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># default used by the Trainer</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">amp_level</span><span class="o">=</span><span class="s1">&#39;O2&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="auto-scale-batch-size">
<h3>auto_scale_batch_size<a class="headerlink" href="#auto-scale-batch-size" title="Permalink to this headline">¶</a></h3>
<video width="50%" max-width="400px" controls
poster="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/auto_scale%E2%80%A8_batch_size.jpg"
src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/auto_scale_batch_size.mp4"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<p>Automatically tries to find the largest batch size that fits into memory,
before any training.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># default used by the Trainer (no scaling of batch size)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">auto_scale_batch_size</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="c1"># run batch size scaling, result overrides hparams.batch_size</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">auto_scale_batch_size</span><span class="o">=</span><span class="s2">&quot;binsearch&quot;</span><span class="p">)</span>

<span class="c1"># call tune to find the batch size</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">tune</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="auto-select-gpus">
<h3>auto_select_gpus<a class="headerlink" href="#auto-select-gpus" title="Permalink to this headline">¶</a></h3>
<video width="50%" max-width="400px" controls
poster="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/auto_select+_gpus.jpg"
src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/auto_select_gpus.mp4"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<p>If enabled and <code class="docutils literal notranslate"><span class="pre">devices</span></code> is an integer, pick available GPUs automatically.
This is especially useful when GPUs are configured to be in “exclusive mode”,
such that only one process at a time can access them.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># no auto selection (picks first 2 GPUs on system, may fail if other process is occupying)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">auto_select_gpus</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># enable auto selection (will find two available GPUs on system)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">auto_select_gpus</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># specifies all GPUs regardless of its availability</span>
<span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">auto_select_gpus</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># specifies all available GPUs (if only one GPU is not occupied, uses one gpu)</span>
<span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">auto_select_gpus</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="auto-lr-find">
<h3>auto_lr_find<a class="headerlink" href="#auto-lr-find" title="Permalink to this headline">¶</a></h3>
<video width="50%" max-width="400px" controls
poster="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/auto_lr_find.jpg"
src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/auto_lr_find.mp4"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<p>Runs a learning rate finder algorithm (see this <a class="reference external" href="https://arxiv.org/abs/1506.01186">paper</a>)
when calling trainer.tune(), to find optimal initial learning rate.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># default used by the Trainer (no learning rate finder)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">auto_lr_find</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># run learning rate finder, results override hparams.learning_rate</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">auto_lr_find</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># call tune to find the lr</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">tune</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># run learning rate finder, results override hparams.my_lr_arg</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">auto_lr_find</span><span class="o">=</span><span class="s1">&#39;my_lr_arg&#39;</span><span class="p">)</span>

<span class="c1"># call tune to find the lr</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">tune</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See the <a class="reference internal" href="../advanced/training_tricks.html#learning-rate-finder"><span class="std std-ref">learning rate finder guide</span></a>.</p>
</div>
</section>
<section id="benchmark">
<h3>benchmark<a class="headerlink" href="#benchmark" title="Permalink to this headline">¶</a></h3>
<video width="50%" max-width="400px" controls
poster="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/benchmark.jpg"
src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/benchmark.mp4"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<p>Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code> if <code class="xref py py-paramref docutils literal notranslate"><span class="pre">deterministic</span></code> is not set.
This flag sets the <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.benchmark</span></code> flag. You can read more about its impact
<a class="reference external" href="https://pytorch.org/docs/stable/notes/randomness.html#cuda-convolution-benchmarking">here</a></p>
<p>This is likely to increase the speed of your system if your input sizes don’t change. However, if they do, then it
might make your system slower. The CUDNN auto-tuner will try to find the best algorithm for the hardware when a new
input size is encountered. Read more about it <a class="reference external" href="https://discuss.pytorch.org/t/what-does-torch-backends-cudnn-benchmark-do/5936">here</a>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># defaults to True if not deterministic (which is False by default)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">()</span>

<span class="c1"># you can overwrite the value</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">benchmark</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="deterministic">
<h3>deterministic<a class="headerlink" href="#deterministic" title="Permalink to this headline">¶</a></h3>
<video width="50%" max-width="400px" controls
poster="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/deterministic.jpg"
src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/deterministic.mp4"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<p>This flag sets the <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span></code> flag.
Might make your system slower, but ensures reproducibility.
Also sets <code class="docutils literal notranslate"><span class="pre">$HOROVOD_FUSION_THRESHOLD=0</span></code>.</p>
<p>For more info check <a class="reference external" href="https://pytorch.org/docs/stable/notes/randomness.html">PyTorch docs</a>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># default used by the Trainer</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">deterministic</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="callbacks">
<h3>callbacks<a class="headerlink" href="#callbacks" title="Permalink to this headline">¶</a></h3>
<video width="50%" max-width="400px" controls
poster="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/callbacks.jpg"
src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/callbacks.mp4"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<p>Add a list of <code class="xref py py-class docutils literal notranslate"><span class="pre">Callback</span></code>. Callbacks run sequentially in the order defined here
with the exception of <code class="xref py py-class docutils literal notranslate"><span class="pre">ModelCheckpoint</span></code> callbacks which run
after all others to ensure all states are saved to the checkpoints.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># a list of callbacks</span>
<span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">PrintCallback</span><span class="p">()]</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span>
</pre></div>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_lightning.callbacks</span> <span class="kn">import</span> <span class="n">Callback</span>

<span class="k">class</span> <span class="nc">PrintCallback</span><span class="p">(</span><span class="n">Callback</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">on_train_start</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training is started!&quot;</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">on_train_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training is done.&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Model-specific callbacks can also be added inside the <code class="docutils literal notranslate"><span class="pre">LightningModule</span></code> through
<code class="xref py py-meth docutils literal notranslate"><span class="pre">configure_callbacks()</span></code>.
Callbacks returned in this hook will extend the list initially given to the <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> argument, and replace
the trainer callbacks should there be two or more of the same type.
<code class="xref py py-class docutils literal notranslate"><span class="pre">ModelCheckpoint</span></code> callbacks always run last.</p>
</section>
<section id="check-val-every-n-epoch">
<h3>check_val_every_n_epoch<a class="headerlink" href="#check-val-every-n-epoch" title="Permalink to this headline">¶</a></h3>
<video width="50%" max-width="400px" controls
poster="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/check_val_every_n_epoch.jpg"
src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/check_val_every_n_epoch.mp4"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<p>Check val every n train epochs.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># default used by the Trainer</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">check_val_every_n_epoch</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># run val loop every 10 training epochs</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">check_val_every_n_epoch</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="checkpoint-callback">
<h3>checkpoint_callback<a class="headerlink" href="#checkpoint-callback" title="Permalink to this headline">¶</a></h3>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><cite>checkpoint_callback</cite> has been deprecated in v1.5 and will be removed in v1.7.
To disable checkpointing, pass <code class="docutils literal notranslate"><span class="pre">enable_checkpointing</span> <span class="pre">=</span> <span class="pre">False</span></code> to the Trainer instead.</p>
</div>
</section>
<section id="default-root-dir">
<h3>default_root_dir<a class="headerlink" href="#default-root-dir" title="Permalink to this headline">¶</a></h3>
<video width="50%" max-width="400px" controls
poster="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/default%E2%80%A8_root_dir.jpg"
src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/default_root_dir.mp4"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<p>Default path for logs and weights when no logger or
<code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_lightning.callbacks.ModelCheckpoint</span></code> callback passed.  On
certain clusters you might want to separate where logs and checkpoints are
stored. If you don’t then use this argument for convenience. Paths can be local
paths or remote paths such as <cite>s3://bucket/path</cite> or ‘hdfs://path/’. Credentials
will need to be set up to use remote filepaths.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># default used by the Trainer</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">default_root_dir</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">())</span>
</pre></div>
</div>
</section>
<section id="devices">
<h3>devices<a class="headerlink" href="#devices" title="Permalink to this headline">¶</a></h3>
<p>Number of devices to train on (<code class="docutils literal notranslate"><span class="pre">int</span></code>), which devices to train on (<code class="docutils literal notranslate"><span class="pre">list</span></code> or <code class="docutils literal notranslate"><span class="pre">str</span></code>), or <code class="docutils literal notranslate"><span class="pre">&quot;auto&quot;</span></code>.
It will be mapped to either <code class="docutils literal notranslate"><span class="pre">gpus</span></code>, <code class="docutils literal notranslate"><span class="pre">tpu_cores</span></code>, <code class="docutils literal notranslate"><span class="pre">num_processes</span></code> or <code class="docutils literal notranslate"><span class="pre">ipus</span></code>,
based on the accelerator type (<code class="docutils literal notranslate"><span class="pre">&quot;cpu&quot;,</span> <span class="pre">&quot;gpu&quot;,</span> <span class="pre">&quot;tpu&quot;,</span> <span class="pre">&quot;ipu&quot;,</span> <span class="pre">&quot;auto&quot;</span></code>).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Training with CPU Accelerator using 2 processes</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">devices</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

<span class="c1"># Training with GPU Accelerator using GPUs 1 and 3</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">devices</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">)</span>

<span class="c1"># Training with TPU Accelerator using 8 tpu cores</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">devices</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;tpu&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The <code class="docutils literal notranslate"><span class="pre">&quot;auto&quot;</span></code> option recognizes the devices to train on, depending on the <code class="docutils literal notranslate"><span class="pre">Accelerator</span></code> being used.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># If your machine has GPUs, it will use all the available GPUs for training</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">devices</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>

<span class="c1"># Training with CPU Accelerator using 1 process</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">devices</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

<span class="c1"># Training with TPU Accelerator using 8 tpu cores</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">devices</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;tpu&quot;</span><span class="p">)</span>

<span class="c1"># Training with IPU Accelerator using 4 ipus</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">devices</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;ipu&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">devices</span></code> flag is not defined, it will assume <code class="docutils literal notranslate"><span class="pre">devices</span></code> to be <code class="docutils literal notranslate"><span class="pre">&quot;auto&quot;</span></code> and fetch the <code class="docutils literal notranslate"><span class="pre">auto_device_count</span></code>
from the accelerator.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># This is part of the built-in `GPUAccelerator`</span>
<span class="k">class</span> <span class="nc">GPUAccelerator</span><span class="p">(</span><span class="n">Accelerator</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Accelerator for GPU devices.&quot;&quot;&quot;</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">auto_device_count</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Get the devices when set to auto.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>


<span class="c1"># Training with GPU Accelerator using total number of gpus available on the system</span>
<span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
<section id="enable-checkpointing">
<h3>enable_checkpointing<a class="headerlink" href="#enable-checkpointing" title="Permalink to this headline">¶</a></h3>
<video width="50%" max-width="400px" controls
poster="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/checkpoint_callback.jpg"
src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/checkpoint_callback.mp4"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<p>By default Lightning saves a checkpoint for you in your current working directory, with the state of your last training epoch,
Checkpoints capture the exact value of all parameters used by a model.
To disable automatic checkpointing, set this to <cite>False</cite>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># default used by Trainer, saves the most recent model to a single checkpoint after each epoch</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">enable_checkpointing</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># turn off automatic checkpointing</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">enable_checkpointing</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>You can override the default behavior by initializing the <code class="xref py py-class docutils literal notranslate"><span class="pre">ModelCheckpoint</span></code>
callback, and adding it to the <code class="xref py py-paramref docutils literal notranslate"><span class="pre">callbacks</span></code> list.
See <a class="reference internal" href="checkpointing.html"><span class="doc">Saving and Loading Checkpoints</span></a> for how to customize checkpointing.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_lightning.callbacks</span> <span class="kn">import</span> <span class="n">ModelCheckpoint</span>

<span class="c1"># Init ModelCheckpoint callback, monitoring &#39;val_loss&#39;</span>
<span class="n">checkpoint_callback</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s2">&quot;val_loss&quot;</span><span class="p">)</span>

<span class="c1"># Add your callback to the callbacks list</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">checkpoint_callback</span><span class="p">])</span>
</pre></div>
</div>
</section>
<section id="fast-dev-run">
<h3>fast_dev_run<a class="headerlink" href="#fast-dev-run" title="Permalink to this headline">¶</a></h3>
<video width="50%" max-width="400px" controls
poster="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/fast_dev_run.jpg"
src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/fast_dev_run.mp4"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<p>Runs n if set to <code class="docutils literal notranslate"><span class="pre">n</span></code> (int) else 1 if set to <code class="docutils literal notranslate"><span class="pre">True</span></code> batch(es) of train, val and test
to find any bugs (ie: a sort of unit test).</p>
<p>Under the hood the pseudocode looks like this when running <em>fast_dev_run</em> with a single batch:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># loading</span>
<span class="fm">__init__</span><span class="p">()</span>
<span class="n">prepare_data</span>

<span class="c1"># test training step</span>
<span class="n">training_batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">)</span>
<span class="n">training_step</span><span class="p">(</span><span class="n">training_batch</span><span class="p">)</span>

<span class="c1"># test val step</span>
<span class="n">val_batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">val_dataloader</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">validation_step</span><span class="p">(</span><span class="n">val_batch</span><span class="p">)</span>
<span class="n">validation_epoch_end</span><span class="p">([</span><span class="n">out</span><span class="p">])</span>
</pre></div>
</div>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># default used by the Trainer</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">fast_dev_run</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># runs 1 train, val, test batch and program ends</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">fast_dev_run</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># runs 7 train, val, test batches and program ends</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">fast_dev_run</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This argument is a bit different from <code class="docutils literal notranslate"><span class="pre">limit_train/val/test_batches</span></code>. Setting this argument will
disable tuner, checkpoint callbacks, early stopping callbacks, loggers and logger callbacks like
<code class="docutils literal notranslate"><span class="pre">LearningRateLogger</span></code> and runs for only 1 epoch. This must be used only for debugging purposes.
<code class="docutils literal notranslate"><span class="pre">limit_train/val/test_batches</span></code> only limits the number of batches and won’t disable anything.</p>
</div>
</section>
<section id="flush-logs-every-n-steps">
<h3>flush_logs_every_n_steps<a class="headerlink" href="#flush-logs-every-n-steps" title="Permalink to this headline">¶</a></h3>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><code class="docutils literal notranslate"><span class="pre">flush_logs_every_n_steps</span></code> has been deprecated in v1.5 and will be removed in v1.7.
Please configure flushing directly in the logger instead.</p>
</div>
<video width="50%" max-width="400px" controls
poster="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/flush_logs%E2%80%A8_every_n_steps.jpg"
src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/flush_logs_every_n_steps.mp4"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<p>Writes logs to disk this often.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># default used by the Trainer</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">flush_logs_every_n_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
<dl class="simple">
<dt>See Also:</dt><dd><ul class="simple">
<li><p><a class="reference internal" href="../extensions/logging.html"><span class="doc">logging</span></a></p></li>
</ul>
</dd>
</dl>
</section>
<section id="gpus">
<span id="id3"></span><h3>gpus<a class="headerlink" href="#gpus" title="Permalink to this headline">¶</a></h3>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><code class="docutils literal notranslate"><span class="pre">gpus=x</span></code> has been deprecated in v1.7 and will be removed in v2.0.
Please use <code class="docutils literal notranslate"><span class="pre">accelerator='gpu'</span></code> and <code class="docutils literal notranslate"><span class="pre">devices=x</span></code> instead.</p>
</div>
<video width="50%" max-width="400px" controls
poster="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/gpus.jpg"
src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/gpus.mp4"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<ul class="simple">
<li><p>Number of GPUs to train on (int)</p></li>
<li><p>or which GPUs to train on (list)</p></li>
<li><p>can handle strings</p></li>
</ul>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># default used by the Trainer (ie: train on CPU)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">gpus</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="c1"># equivalent</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">gpus</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># int: train on 2 gpus</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">gpus</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># list: train on GPUs 1, 4 (by bus ordering)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">gpus</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">gpus</span><span class="o">=</span><span class="s1">&#39;1, 4&#39;</span><span class="p">)</span> <span class="c1"># equivalent</span>

<span class="c1"># -1: train on all gpus</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">gpus</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">gpus</span><span class="o">=</span><span class="s1">&#39;-1&#39;</span><span class="p">)</span> <span class="c1"># equivalent</span>

<span class="c1"># combine with num_nodes to train on multiple GPUs across nodes</span>
<span class="c1"># uses 8 gpus in total</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">gpus</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_nodes</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="c1"># train only on GPUs 1 and 4 across nodes</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">gpus</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">num_nodes</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
<dl class="simple">
<dt>See Also:</dt><dd><ul class="simple">
<li><p><a class="reference internal" href="../accelerators/gpu_basic.html#multi-gpu"><span class="std std-ref">Multi GPU Training</span></a></p></li>
</ul>
</dd>
</dl>
</section>
<section id="gradient-clip-val">
<h3>gradient_clip_val<a class="headerlink" href="#gradient-clip-val" title="Permalink to this headline">¶</a></h3>
<video width="50%" max-width="400px" controls
poster="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/gradient+_clip_val.jpg"
src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/gradient_clip_val.mp4"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<p>Gradient clipping value</p>
<ul class="simple">
<li><p>0 means don’t clip.</p></li>
</ul>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># default used by the Trainer</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">gradient_clip_val</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="limit-train-batches">
<h3>limit_train_batches<a class="headerlink" href="#limit-train-batches" title="Permalink to this headline">¶</a></h3>
<video width="50%" max-width="400px" controls
poster="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/limit_train_batches.jpg"
src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/limit_batches.mp4"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<p>How much of training dataset to check.
Useful when debugging or testing something that happens at the end of an epoch.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># default used by the Trainer</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">limit_train_batches</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
</pre></div>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># default used by the Trainer</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">limit_train_batches</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="c1"># run through only 25% of the training set each epoch</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">limit_train_batches</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>

<span class="c1"># run through only 10 batches of the training set each epoch</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">limit_train_batches</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="limit-test-batches">
<h3>limit_test_batches<a class="headerlink" href="#limit-test-batches" title="Permalink to this headline">¶</a></h3>
<video width="50%" max-width="400px" controls
poster="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/limit_test_batches.jpg"
src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/limit_batches.mp4"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<p>How much of test dataset to check.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># default used by the Trainer</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">limit_test_batches</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="c1"># run through only 25% of the test set each epoch</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">limit_test_batches</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>

<span class="c1"># run for only 10 batches</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">limit_test_batches</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
<p>In the case of multiple test dataloaders, the limit applies to each dataloader individually.</p>
</section>
<section id="limit-val-batches">
<h3>limit_val_batches<a class="headerlink" href="#limit-val-batches" title="Permalink to this headline">¶</a></h3>
<video width="50%" max-width="400px" controls
poster="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/limit_val_batches.jpg"
src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/limit_batches.mp4"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<p>How much of validation dataset to check.
Useful when debugging or testing something that happens at the end of an epoch.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># default used by the Trainer</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">limit_val_batches</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="c1"># run through only 25% of the validation set each epoch</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">limit_val_batches</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>

<span class="c1"># run for only 10 batches</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">limit_val_batches</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
<p>In the case of multiple validation dataloaders, the limit applies to each dataloader individually.</p>
</section>
<section id="log-every-n-steps">
<h3>log_every_n_steps<a class="headerlink" href="#log-every-n-steps" title="Permalink to this headline">¶</a></h3>
<video width="50%" max-width="400px" controls
poster="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/log_every_n_steps.jpg"
src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/log_every_n_steps.mp4"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<p>How often to add logging rows (does not write to disk)</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># default used by the Trainer</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">log_every_n_steps</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
</pre></div>
</div>
<dl class="simple">
<dt>See Also:</dt><dd><ul class="simple">
<li><p><a class="reference internal" href="../extensions/logging.html"><span class="doc">logging</span></a></p></li>
</ul>
</dd>
</dl>
</section>
<section id="logger">
<h3>logger<a class="headerlink" href="#logger" title="Permalink to this headline">¶</a></h3>
<video width="50%" max-width="400px" controls
poster="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/logger.jpg"
src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/logger.mp4"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<p><a class="reference internal" href="../visualize/loggers.html"><span class="doc">Logger</span></a> (or iterable collection of loggers) for experiment tracking. A <code class="docutils literal notranslate"><span class="pre">True</span></code> value uses the default <code class="docutils literal notranslate"><span class="pre">TensorBoardLogger</span></code> shown below. <code class="docutils literal notranslate"><span class="pre">False</span></code> will disable logging.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_lightning.loggers</span> <span class="kn">import</span> <span class="n">TensorBoardLogger</span>

<span class="c1"># default logger used by trainer</span>
<span class="n">logger</span> <span class="o">=</span> <span class="n">TensorBoardLogger</span><span class="p">(</span><span class="n">save_dir</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">(),</span> <span class="n">version</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;lightning_logs&quot;</span><span class="p">)</span>
<span class="n">Trainer</span><span class="p">(</span><span class="n">logger</span><span class="o">=</span><span class="n">logger</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="max-epochs">
<h3>max_epochs<a class="headerlink" href="#max-epochs" title="Permalink to this headline">¶</a></h3>
<video width="50%" max-width="400px" controls
poster="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/max_epochs.jpg"
src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/min_max_epochs.mp4"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<p>Stop training once this number of epochs is reached</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># default used by the Trainer</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">max_epochs</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
<p>If both <code class="docutils literal notranslate"><span class="pre">max_epochs</span></code> and <code class="docutils literal notranslate"><span class="pre">max_steps</span></code> aren’t specified, <code class="docutils literal notranslate"><span class="pre">max_epochs</span></code> will default to <code class="docutils literal notranslate"><span class="pre">1000</span></code>.
To enable infinite training, set <code class="docutils literal notranslate"><span class="pre">max_epochs</span> <span class="pre">=</span> <span class="pre">-1</span></code>.</p>
</section>
<section id="min-epochs">
<h3>min_epochs<a class="headerlink" href="#min-epochs" title="Permalink to this headline">¶</a></h3>
<video width="50%" max-width="400px" controls
poster="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/min_epochs.jpg"
src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/min_max_epochs.mp4"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<p>Force training for at least these many epochs</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># default used by the Trainer</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">min_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="max-steps">
<h3>max_steps<a class="headerlink" href="#max-steps" title="Permalink to this headline">¶</a></h3>
<video width="50%" max-width="400px" controls
poster="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/max_steps.jpg"
src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/min_max_steps.mp4"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<p>Stop training after this number of <a class="reference internal" href="#global-step"><span class="std std-ref">global steps</span></a>.
Training will stop if max_steps or max_epochs have reached (earliest).</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Default (disabled)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">max_steps</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="c1"># Stop after 100 steps</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">max_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
<p>If <code class="docutils literal notranslate"><span class="pre">max_steps</span></code> is not specified, <code class="docutils literal notranslate"><span class="pre">max_epochs</span></code> will be used instead (and <code class="docutils literal notranslate"><span class="pre">max_epochs</span></code> defaults to
<code class="docutils literal notranslate"><span class="pre">1000</span></code> if <code class="docutils literal notranslate"><span class="pre">max_epochs</span></code> is not specified). To disable this default, set <code class="docutils literal notranslate"><span class="pre">max_steps</span> <span class="pre">=</span> <span class="pre">-1</span></code>.</p>
</section>
<section id="min-steps">
<h3>min_steps<a class="headerlink" href="#min-steps" title="Permalink to this headline">¶</a></h3>
<video width="50%" max-width="400px" controls
poster="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/min_steps.jpg"
src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/min_max_steps.mp4"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<p>Force training for at least this number of <a class="reference internal" href="#global-step"><span class="std std-ref">global steps</span></a>.
Trainer will train model for at least min_steps or min_epochs (latest).</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Default (disabled)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">min_steps</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="c1"># Run at least for 100 steps (disable min_epochs)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">min_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">min_epochs</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="max-time">
<h3>max_time<a class="headerlink" href="#max-time" title="Permalink to this headline">¶</a></h3>
<p>Set the maximum amount of time for training. Training will get interrupted mid-epoch.
For customizable options use the <code class="xref py py-class docutils literal notranslate"><span class="pre">Timer</span></code> callback.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Default (disabled)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">max_time</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="c1"># Stop after 12 hours of training or when reaching 10 epochs (string)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">max_time</span><span class="o">=</span><span class="s2">&quot;00:12:00:00&quot;</span><span class="p">,</span> <span class="n">max_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="c1"># Stop after 1 day and 5 hours (dict)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">max_time</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;days&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;hours&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">})</span>
</pre></div>
</div>
<p>In case <code class="docutils literal notranslate"><span class="pre">max_time</span></code> is used together with <code class="docutils literal notranslate"><span class="pre">min_steps</span></code> or <code class="docutils literal notranslate"><span class="pre">min_epochs</span></code>, the <code class="docutils literal notranslate"><span class="pre">min_*</span></code> requirement
always has precedence.</p>
</section>
<section id="num-nodes">
<h3>num_nodes<a class="headerlink" href="#num-nodes" title="Permalink to this headline">¶</a></h3>
<video width="50%" max-width="400px" controls
poster="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/num_nodes.jpg"
src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/num_nodes.mp4"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<p>Number of GPU nodes for distributed training.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># default used by the Trainer</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">num_nodes</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># to train on 8 nodes</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">num_nodes</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="num-processes">
<h3>num_processes<a class="headerlink" href="#num-processes" title="Permalink to this headline">¶</a></h3>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><code class="docutils literal notranslate"><span class="pre">num_processes=x</span></code> has been deprecated in v1.7 and will be removed in v2.0.
Please use <code class="docutils literal notranslate"><span class="pre">accelerator='cpu'</span></code> and <code class="docutils literal notranslate"><span class="pre">devices=x</span></code> instead.</p>
</div>
<video width="50%" max-width="400px" controls
poster="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/num_processes.jpg"
src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/num_processes.mp4"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<p>Number of processes to train with. Automatically set to the number of GPUs
when using <code class="docutils literal notranslate"><span class="pre">strategy=&quot;ddp&quot;</span></code>. Set to a number greater than 1 when
using <code class="docutils literal notranslate"><span class="pre">accelerator=&quot;cpu&quot;</span></code> and <code class="docutils literal notranslate"><span class="pre">strategy=&quot;ddp&quot;</span></code> to mimic distributed training on a
machine without GPUs. This is useful for debugging, but <strong>will not</strong> provide
any speedup, since single-process Torch already makes efficient use of multiple
CPUs. While it would typically spawns subprocesses for training, setting
<code class="docutils literal notranslate"><span class="pre">num_nodes</span> <span class="pre">&gt;</span> <span class="pre">1</span></code> and keeping <code class="docutils literal notranslate"><span class="pre">num_processes</span> <span class="pre">=</span> <span class="pre">1</span></code> runs training in the main
process.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Simulate DDP for debugging on your GPU-less laptop</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;ddp&quot;</span><span class="p">,</span> <span class="n">num_processes</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="num-sanity-val-steps">
<h3>num_sanity_val_steps<a class="headerlink" href="#num-sanity-val-steps" title="Permalink to this headline">¶</a></h3>
<video width="50%" max-width="400px" controls
poster="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/num_sanity%E2%80%A8_val_steps.jpg"
src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/num_sanity_val_steps.mp4"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<p>Sanity check runs n batches of val before starting the training routine.
This catches any bugs in your validation without having to wait for the first validation check.
The Trainer uses 2 steps by default. Turn it off or modify it here.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># default used by the Trainer</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">num_sanity_val_steps</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># turn it off</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">num_sanity_val_steps</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># check all validation data</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">num_sanity_val_steps</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>This option will reset the validation dataloader unless <code class="docutils literal notranslate"><span class="pre">num_sanity_val_steps=0</span></code>.</p>
</section>
<section id="overfit-batches">
<h3>overfit_batches<a class="headerlink" href="#overfit-batches" title="Permalink to this headline">¶</a></h3>
<video width="50%" max-width="400px" controls
poster="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/overfit_batches.jpg"
src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/overfit_batches.mp4"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<p>Uses this much data of the training &amp; validation set.
If the training &amp; validation dataloaders have <code class="docutils literal notranslate"><span class="pre">shuffle=True</span></code>, Lightning will automatically disable it.</p>
<p>Useful for quickly debugging or trying to overfit on purpose.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># default used by the Trainer</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">overfit_batches</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>

<span class="c1"># use only 1% of the train &amp; val set</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">overfit_batches</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="c1"># overfit on 10 of the same batches</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">overfit_batches</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="plugins">
<h3>plugins<a class="headerlink" href="#plugins" title="Permalink to this headline">¶</a></h3>
<video width="50%" max-width="400px" controls
poster="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/cluster_environment.jpg"
src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/cluster_environment.mp4"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<p><a class="reference internal" href="../extensions/plugins.html#plugins"><span class="std std-ref">Plugins</span></a> allow you to connect arbitrary backends, precision libraries, clusters etc. For example:</p>
<ul class="simple">
<li><p><a class="reference internal" href="checkpointing_expert.html#checkpointing-expert"><span class="std std-ref">Checkpoint IO</span></a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/elastic/0.2.2/index.html">TorchElastic</a></p></li>
<li><p><a class="reference internal" href="precision_expert.html#precision-expert"><span class="std std-ref">Precision Plugins</span></a></p></li>
</ul>
<p>To define your own behavior, subclass the relevant class and pass it in. Here’s an example linking up your own
<code class="xref py py-class docutils literal notranslate"><span class="pre">ClusterEnvironment</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_lightning.plugins.environments</span> <span class="kn">import</span> <span class="n">ClusterEnvironment</span>


<span class="k">class</span> <span class="nc">MyCluster</span><span class="p">(</span><span class="n">ClusterEnvironment</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">main_address</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">your_main_address</span>

    <span class="k">def</span> <span class="nf">main_port</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">your_main_port</span>

    <span class="k">def</span> <span class="nf">world_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">the_world_size</span>


<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">plugins</span><span class="o">=</span><span class="p">[</span><span class="n">MyCluster</span><span class="p">()],</span> <span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="precision">
<h3>precision<a class="headerlink" href="#precision" title="Permalink to this headline">¶</a></h3>
<video width="50%" max-width="400px" controls
poster="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/precision.jpg"
src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/precision.mp4"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<p>Lightning supports either double (64), float (32), bfloat16 (bf16), or half (16) precision training.</p>
<p>Half precision, or mixed precision, is the combined use of 32 and 16 bit floating points to reduce memory footprint during model training. This can result in improved performance, achieving +3X speedups on modern GPUs.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># default used by the Trainer</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>

<span class="c1"># 16-bit precision</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># works only on CUDA</span>

<span class="c1"># bfloat16 precision</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="s2">&quot;bf16&quot;</span><span class="p">)</span>

<span class="c1"># 64-bit precision</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When running on TPUs, torch.bfloat16 will be used but tensor printing will still show torch.float32.</p>
</div>
<div class="dropdown admonition">
<p class="admonition-title">If you are interested in using Apex 16-bit training:</p>
<blockquote>
<div><p>NVIDIA Apex and DDP have instability problems. We recommend using the native AMP for 16-bit precision with multiple GPUs.
To use Apex 16-bit training:</p>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://github.com/NVIDIA/apex#quick-start">Install apex.</a></p></li>
<li><p>Set the <code class="docutils literal notranslate"><span class="pre">precision</span></code> trainer flag to 16. You can customize the <a class="reference external" href="https://nvidia.github.io/apex/amp.html#opt-levels">Apex optimization level</a> by setting the <cite>amp_level</cite> flag.</p></li>
</ol>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># turn on 16-bit</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">amp_backend</span><span class="o">=</span><span class="s2">&quot;apex&quot;</span><span class="p">,</span> <span class="n">amp_level</span><span class="o">=</span><span class="s2">&quot;O2&quot;</span><span class="p">,</span> <span class="n">precision</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
</div>
</section>
<section id="process-position">
<h3>process_position<a class="headerlink" href="#process-position" title="Permalink to this headline">¶</a></h3>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><code class="docutils literal notranslate"><span class="pre">process_position</span></code> has been deprecated in v1.5 and will be removed in v1.7.
Please pass <code class="xref py py-class docutils literal notranslate"><span class="pre">TQDMProgressBar</span></code> with <code class="docutils literal notranslate"><span class="pre">process_position</span></code>
directly to the Trainer’s <code class="docutils literal notranslate"><span class="pre">callbacks</span></code> argument instead.</p>
</div>
<video width="50%" max-width="400px" controls
poster="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/process_position.jpg"
src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/process_position.mp4"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<p>Orders the progress bar. Useful when running multiple trainers on the same node.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># default used by the Trainer</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">process_position</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This argument is ignored if a custom callback is passed to <code class="xref py py-paramref docutils literal notranslate"><span class="pre">callbacks</span></code>.</p>
</div>
</section>
<section id="profiler">
<h3>profiler<a class="headerlink" href="#profiler" title="Permalink to this headline">¶</a></h3>
<video width="50%" max-width="400px" controls
poster="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/profiler.jpg"
src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/profiler.mp4"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<p>To profile individual steps during training and assist in identifying bottlenecks.</p>
<p>See the <a class="reference internal" href="../tuning/profiler.html"><span class="doc">profiler documentation</span></a>. for more details.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_lightning.profiler</span> <span class="kn">import</span> <span class="n">SimpleProfiler</span><span class="p">,</span> <span class="n">AdvancedProfiler</span>

<span class="c1"># default used by the Trainer</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">profiler</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="c1"># to profile standard training events, equivalent to `profiler=SimpleProfiler()`</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">profiler</span><span class="o">=</span><span class="s2">&quot;simple&quot;</span><span class="p">)</span>

<span class="c1"># advanced profiler for function-level stats, equivalent to `profiler=AdvancedProfiler()`</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">profiler</span><span class="o">=</span><span class="s2">&quot;advanced&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="enable-progress-bar">
<h3>enable_progress_bar<a class="headerlink" href="#enable-progress-bar" title="Permalink to this headline">¶</a></h3>
<p>Whether to enable or disable the progress bar. Defaults to True.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># default used by the Trainer</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">enable_progress_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># disable progress bar</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">enable_progress_bar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="reload-dataloaders-every-n-epochs">
<h3>reload_dataloaders_every_n_epochs<a class="headerlink" href="#reload-dataloaders-every-n-epochs" title="Permalink to this headline">¶</a></h3>
<video width="50%" max-width="400px" controls
poster="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/reload_%E2%80%A8dataloaders_%E2%80%A8every_epoch.jpg"
src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/reload_dataloaders_every_epoch.mp4"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<p>Set to a positive integer to reload dataloaders every n epochs.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># if 0 (default)</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">train_dataloader</span><span class="p">()</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">epochs</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="o">...</span>

<span class="c1"># if a positive integer</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">epochs</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">epoch</span> <span class="o">%</span> <span class="n">reload_dataloaders_every_n_epochs</span><span class="p">:</span>
        <span class="n">train_loader</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">train_dataloader</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="o">...</span>
</pre></div>
</div>
</section>
<section id="replace-sampler-ddp">
<span id="id4"></span><h3>replace_sampler_ddp<a class="headerlink" href="#replace-sampler-ddp" title="Permalink to this headline">¶</a></h3>
<video width="50%" max-width="400px" controls
poster="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/replace_sampler_ddp.jpg"
src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/replace_sampler_ddp.mp4"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<p>Enables auto adding of <code class="xref py py-class docutils literal notranslate"><span class="pre">DistributedSampler</span></code>. In PyTorch, you must use it in
distributed settings such as TPUs or multi-node. The sampler makes sure each GPU sees the appropriate part of your data.
By default it will add <code class="docutils literal notranslate"><span class="pre">shuffle=True</span></code> for train sampler and <code class="docutils literal notranslate"><span class="pre">shuffle=False</span></code> for val/test sampler.
If you want to customize it, you can set <code class="docutils literal notranslate"><span class="pre">replace_sampler_ddp=False</span></code> and add your own distributed sampler.
If <code class="docutils literal notranslate"><span class="pre">replace_sampler_ddp=True</span></code> and a distributed sampler was already added,
Lightning will not replace the existing one.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># default used by the Trainer</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">replace_sampler_ddp</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>By setting to False, you have to add your own distributed sampler:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># in your LightningModule or LightningDataModule</span>
<span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># default used by the Trainer</span>
    <span class="n">sampler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">DistributedSampler</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">sampler</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dataloader</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For iterable datasets, we don’t do this automatically.</p>
</div>
</section>
<section id="resume-from-checkpoint">
<h3>resume_from_checkpoint<a class="headerlink" href="#resume-from-checkpoint" title="Permalink to this headline">¶</a></h3>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><code class="docutils literal notranslate"><span class="pre">resume_from_checkpoint</span></code> is deprecated in v1.5 and will be removed in v2.0.
Please pass <code class="docutils literal notranslate"><span class="pre">trainer.fit(ckpt_path=&quot;some/path/to/my_checkpoint.ckpt&quot;)</span></code> instead.</p>
</div>
<video width="50%" max-width="400px" controls
poster="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/resume_from_checkpoint.jpg"
src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/resume_from_checkpoint.mp4"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<p>To resume training from a specific checkpoint pass in the path here. If resuming from a mid-epoch
checkpoint, training will start from the beginning of the next epoch.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># default used by the Trainer</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">resume_from_checkpoint</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="c1"># resume from a specific checkpoint</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">resume_from_checkpoint</span><span class="o">=</span><span class="s2">&quot;some/path/to/my_checkpoint.ckpt&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="strategy">
<h3>strategy<a class="headerlink" href="#strategy" title="Permalink to this headline">¶</a></h3>
<p>Supports passing different training strategies with aliases (ddp, ddp_spawn, etc) as well as custom strategies.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Training with the DistributedDataParallel strategy on 4 GPUs</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;ddp&quot;</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="c1"># Training with the DDP Spawn strategy using 4 cpu processes</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;ddp_spawn&quot;</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Additionally, you can pass your custom strategy to the <code class="docutils literal notranslate"><span class="pre">strategy</span></code> argument.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_lightning.strategies</span> <span class="kn">import</span> <span class="n">DDPStrategy</span>


<span class="k">class</span> <span class="nc">CustomDDPStrategy</span><span class="p">(</span><span class="n">DDPStrategy</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">configure_ddp</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_model</span> <span class="o">=</span> <span class="n">MyCustomDistributedDataParallel</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span>
            <span class="n">device_ids</span><span class="o">=...</span><span class="p">,</span>
        <span class="p">)</span>


<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="n">CustomDDPStrategy</span><span class="p">(),</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<dl class="simple">
<dt>See Also:</dt><dd><ul class="simple">
<li><p><a class="reference internal" href="../accelerators/gpu_basic.html#multi-gpu"><span class="std std-ref">Multi GPU Training</span></a>.</p></li>
<li><p><a class="reference internal" href="../advanced/model_parallel.html"><span class="doc">Model Parallel GPU training guide</span></a>.</p></li>
<li><p><a class="reference internal" href="../accelerators/tpu.html"><span class="doc">TPU training guide</span></a>.</p></li>
</ul>
</dd>
</dl>
</section>
<section id="sync-batchnorm">
<h3>sync_batchnorm<a class="headerlink" href="#sync-batchnorm" title="Permalink to this headline">¶</a></h3>
<video width="50%" max-width="400px" controls
poster="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/sync_batchnorm.jpg"
src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/sync_batchnorm.mp4"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<p>Enable synchronization between batchnorm layers across all GPUs.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">sync_batchnorm</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="track-grad-norm">
<h3>track_grad_norm<a class="headerlink" href="#track-grad-norm" title="Permalink to this headline">¶</a></h3>
<video width="50%" max-width="400px" controls
poster="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/track_grad_norm.jpg"
src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/track_grad_norm.mp4"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<ul class="simple">
<li><p>no tracking (-1)</p></li>
<li><p>Otherwise tracks that norm (2 for 2-norm)</p></li>
</ul>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># default used by the Trainer</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">track_grad_norm</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># track the 2-norm</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">track_grad_norm</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="tpu-cores">
<span id="id5"></span><h3>tpu_cores<a class="headerlink" href="#tpu-cores" title="Permalink to this headline">¶</a></h3>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><code class="docutils literal notranslate"><span class="pre">tpu_cores=x</span></code> has been deprecated in v1.7 and will be removed in v2.0.
Please use <code class="docutils literal notranslate"><span class="pre">accelerator='tpu'</span></code> and <code class="docutils literal notranslate"><span class="pre">devices=x</span></code> instead.</p>
</div>
<video width="50%" max-width="400px" controls
poster="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/tpu_cores.jpg"
src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/tpu_cores.mp4"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<ul class="simple">
<li><p>How many TPU cores to train on (1 or 8).</p></li>
<li><p>Which TPU core to train on [1-8]</p></li>
</ul>
<p>A single TPU v2 or v3 has 8 cores. A TPU pod has
up to 2048 cores. A slice of a POD means you get as many cores
as you request.</p>
<p>Your effective batch size is batch_size * total tpu cores.</p>
<p>This parameter can be either 1 or 8.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># your_trainer_file.py</span>

<span class="c1"># default used by the Trainer (ie: train on CPU)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">tpu_cores</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="c1"># int: train on a single core</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">tpu_cores</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># list: train on a single selected core</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">tpu_cores</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>

<span class="c1"># int: train on all cores few cores</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">tpu_cores</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>

<span class="c1"># for 8+ cores must submit via xla script with</span>
<span class="c1"># a max of 8 cores specified. The XLA script</span>
<span class="c1"># will duplicate script onto each TPU in the POD</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">tpu_cores</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
</pre></div>
</div>
<p>To train on more than 8 cores (ie: a POD),
submit this script using the xla_dist script.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>python -m torch_xla.distributed.xla_dist
--tpu=$TPU_POD_NAME
--conda-env=torch-xla-nightly
--env=XLA_USE_BF16=1
-- python your_trainer_file.py
</pre></div>
</div>
</section>
<section id="val-check-interval">
<h3>val_check_interval<a class="headerlink" href="#val-check-interval" title="Permalink to this headline">¶</a></h3>
<video width="50%" max-width="400px" controls
poster="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/val_check_interval.jpg"
src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/val_check_interval.mp4"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<p>How often within one training epoch to check the validation set.
Can specify as float or int.</p>
<ul class="simple">
<li><p>pass a <code class="docutils literal notranslate"><span class="pre">float</span></code> in the range [0.0, 1.0] to check after a fraction of the training epoch.</p></li>
<li><p>pass an <code class="docutils literal notranslate"><span class="pre">int</span></code> to check after a fixed number of training batches.</p></li>
</ul>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># default used by the Trainer</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">val_check_interval</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="c1"># check validation set 4 times during a training epoch</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">val_check_interval</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>

<span class="c1"># check validation set every 1000 training batches</span>
<span class="c1"># use this when using iterableDataset and your dataset has no length</span>
<span class="c1"># (ie: production cases with streaming data)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">val_check_interval</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Here is the computation to estimate the total number of batches seen within an epoch.</span>

<span class="c1"># Find the total number of train batches</span>
<span class="n">total_train_batches</span> <span class="o">=</span> <span class="n">total_train_samples</span> <span class="o">//</span> <span class="p">(</span><span class="n">train_batch_size</span> <span class="o">*</span> <span class="n">world_size</span><span class="p">)</span>

<span class="c1"># Compute how many times we will call validation during the training loop</span>
<span class="n">val_check_batch</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">total_train_batches</span> <span class="o">*</span> <span class="n">val_check_interval</span><span class="p">))</span>
<span class="n">val_checks_per_epoch</span> <span class="o">=</span> <span class="n">total_train_batches</span> <span class="o">/</span> <span class="n">val_check_batch</span>

<span class="c1"># Find the total number of validation batches</span>
<span class="n">total_val_batches</span> <span class="o">=</span> <span class="n">total_val_samples</span> <span class="o">//</span> <span class="p">(</span><span class="n">val_batch_size</span> <span class="o">*</span> <span class="n">world_size</span><span class="p">)</span>

<span class="c1"># Total number of batches run</span>
<span class="n">total_fit_batches</span> <span class="o">=</span> <span class="n">total_train_batches</span> <span class="o">+</span> <span class="n">total_val_batches</span>
</pre></div>
</div>
</section>
<section id="weights-save-path">
<h3>weights_save_path<a class="headerlink" href="#weights-save-path" title="Permalink to this headline">¶</a></h3>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><cite>weights_save_path</cite> has been deprecated in v1.6 and will be removed in v1.8. Please pass
<code class="docutils literal notranslate"><span class="pre">dirpath</span></code> directly to the <code class="xref py py-class docutils literal notranslate"><span class="pre">ModelCheckpoint</span></code>
callback.</p>
</div>
<video width="50%" max-width="400px" controls
poster="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/weights_save_path.jpg"
src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/weights_save_path.mp4"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<p>Directory of where to save weights if specified.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># default used by the Trainer</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">weights_save_path</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">())</span>

<span class="c1"># save to your custom path</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">weights_save_path</span><span class="o">=</span><span class="s2">&quot;my/path&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># if checkpoint callback used, then overrides the weights path</span>
<span class="c1"># **NOTE: this saves weights to some/path NOT my/path</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">dirpath</span><span class="o">=</span><span class="s1">&#39;some/path&#39;</span><span class="p">)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">checkpoint</span><span class="p">],</span>
    <span class="n">weights_save_path</span><span class="o">=</span><span class="s1">&#39;my/path&#39;</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="weights-summary">
<h3>weights_summary<a class="headerlink" href="#weights-summary" title="Permalink to this headline">¶</a></h3>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><cite>weights_summary</cite> is deprecated in v1.5 and will be removed in v1.7. Please pass <code class="xref py py-class docutils literal notranslate"><span class="pre">ModelSummary</span></code>
directly to the Trainer’s <code class="docutils literal notranslate"><span class="pre">callbacks</span></code> argument instead. To disable the model summary,
pass <code class="docutils literal notranslate"><span class="pre">enable_model_summary</span> <span class="pre">=</span> <span class="pre">False</span></code> to the Trainer.</p>
</div>
<video width="50%" max-width="400px" controls
poster="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/thumb/weights_summary.jpg"
src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/trainer_flags/weights_summary.mp4"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<p>Prints a summary of the weights when training begins.
Options: ‘full’, ‘top’, None.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># default used by the Trainer (ie: print summary of top level modules)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">weights_summary</span><span class="o">=</span><span class="s2">&quot;top&quot;</span><span class="p">)</span>

<span class="c1"># print full summary of all modules and submodules</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">weights_summary</span><span class="o">=</span><span class="s2">&quot;full&quot;</span><span class="p">)</span>

<span class="c1"># don&#39;t print a summary</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">weights_summary</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="enable-model-summary">
<h3>enable_model_summary<a class="headerlink" href="#enable-model-summary" title="Permalink to this headline">¶</a></h3>
<p>Whether to enable or disable the model summarization. Defaults to True.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># default used by the Trainer</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">enable_model_summary</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># disable summarization</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">enable_model_summary</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># enable custom summarization</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.callbacks</span> <span class="kn">import</span> <span class="n">ModelSummary</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">enable_model_summary</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">ModelSummary</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=-</span><span class="mi">1</span><span class="p">)])</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="trainer-class-api">
<h2>Trainer class API<a class="headerlink" href="#trainer-class-api" title="Permalink to this headline">¶</a></h2>
<section id="methods">
<h3>Methods<a class="headerlink" href="#methods" title="Permalink to this headline">¶</a></h3>
<section id="init">
<h4>init<a class="headerlink" href="#init" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">Trainer.</span></span><span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">logger</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">checkpoint_callback</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_checkpointing</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">callbacks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">default_root_dir</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_clip_val</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_clip_algorithm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">process_position</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_nodes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_processes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">devices</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gpus</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">auto_select_gpus</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tpu_cores</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ipus</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log_gpu_memory</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">progress_bar_refresh_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_progress_bar</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">overfit_batches</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">track_grad_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">check_val_every_n_epoch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fast_dev_run</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">accumulate_grad_batches</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_steps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_steps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_time</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">limit_train_batches</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">limit_val_batches</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">limit_test_batches</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">limit_predict_batches</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">val_check_interval</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">flush_logs_every_n_steps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log_every_n_steps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">50</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">accelerator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_batchnorm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">precision</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_model_summary</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights_summary</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'top'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights_save_path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_sanity_val_steps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">resume_from_checkpoint</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">profiler</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">benchmark</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">deterministic</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reload_dataloaders_every_n_epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">auto_lr_find</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">replace_sampler_ddp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">detect_anomaly</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">auto_scale_batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prepare_data_per_node</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">plugins</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">amp_backend</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'native'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">amp_level</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">move_metrics_to_cpu</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multiple_trainloader_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'max_size_cycle'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stochastic_weight_avg</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">terminate_on_nan</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_lightning/trainer/trainer.html#Trainer.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Customize every aspect of training via flags.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.accelerator"></span><strong>accelerator</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.accelerator">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Accelerator</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – <p>Supports passing different accelerator types (“cpu”, “gpu”, “tpu”, “ipu”, “hpu”, “auto”)
as well as custom accelerator instances.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span>Passing training strategies (e.g., ‘ddp’) to <code class="docutils literal notranslate"><span class="pre">accelerator</span></code> has been deprecated in v1.5.0
and will be removed in v1.7.0. Please use the <code class="docutils literal notranslate"><span class="pre">strategy</span></code> argument instead.</p>
</div>
</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.accumulate_grad_batches"></span><strong>accumulate_grad_batches</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.accumulate_grad_batches">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – Accumulates grads every k batches or as set up in the dict.
Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.amp_backend"></span><strong>amp_backend</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.amp_backend">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The mixed precision backend to use (“native” or “apex”).
Default: <code class="docutils literal notranslate"><span class="pre">'native''</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.amp_level"></span><strong>amp_level</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.amp_level">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – The optimization level to use (O1, O2, etc…). By default it will be set to “O2”
if <code class="docutils literal notranslate"><span class="pre">amp_backend</span></code> is set to “apex”.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.auto_lr_find"></span><strong>auto_lr_find</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.auto_lr_find">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – If set to True, will make trainer.tune() run a learning rate finder,
trying to optimize initial learning for faster convergence. trainer.tune() method will
set the suggested learning rate in self.lr or self.learning_rate in the LightningModule.
To use a different key set a string instead of True with the key name.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.auto_scale_batch_size"></span><strong>auto_scale_batch_size</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.auto_scale_batch_size">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>]) – If set to True, will <cite>initially</cite> run a batch size
finder trying to find the largest batch size that fits into memory.
The result will be stored in self.batch_size in the LightningModule.
Additionally, can be set to either <cite>power</cite> that estimates the batch size through
a power search or <cite>binsearch</cite> that estimates the batch size through a binary search.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.auto_select_gpus"></span><strong>auto_select_gpus</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.auto_select_gpus">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – If enabled and <code class="docutils literal notranslate"><span class="pre">gpus</span></code> or <code class="docutils literal notranslate"><span class="pre">devices</span></code> is an integer, pick available
gpus automatically. This is especially useful when
GPUs are configured to be in “exclusive mode”, such
that only one process at a time can access them.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.benchmark"></span><strong>benchmark</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.benchmark">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>]) – Sets <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.benchmark</span></code>.
Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code> if <code class="xref py py-paramref docutils literal notranslate"><span class="pre">deterministic</span></code>
is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Overwrite to manually set a different value. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.callbacks"></span><strong>callbacks</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.callbacks">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Callback</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Callback</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – Add a callback or list of callbacks.
Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.checkpoint_callback"></span><strong>checkpoint_callback</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.checkpoint_callback">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>]) – <p>If <code class="docutils literal notranslate"><span class="pre">True</span></code>, enable checkpointing.
Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span><code class="docutils literal notranslate"><span class="pre">checkpoint_callback</span></code> has been deprecated in v1.5 and will be removed in v1.7.
Please consider using <code class="docutils literal notranslate"><span class="pre">enable_checkpointing</span></code> instead.</p>
</div>
</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.enable_checkpointing"></span><strong>enable_checkpointing</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.enable_checkpointing">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, enable checkpointing.
It will configure a default ModelCheckpoint callback if there is no user-defined ModelCheckpoint in
<code class="xref py py-paramref docutils literal notranslate"><span class="pre">callbacks</span></code>.
Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.check_val_every_n_epoch"></span><strong>check_val_every_n_epoch</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.check_val_every_n_epoch">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Check val every n train epochs.
Default: <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.default_root_dir"></span><strong>default_root_dir</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.default_root_dir">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Default path for logs and weights when no logger/ckpt_callback passed.
Default: <code class="docutils literal notranslate"><span class="pre">os.getcwd()</span></code>.
Can be remote file paths such as <cite>s3://mybucket/path</cite> or ‘hdfs://path/’</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.detect_anomaly"></span><strong>detect_anomaly</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.detect_anomaly">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – Enable anomaly detection for the autograd engine.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.deterministic"></span><strong>deterministic</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.deterministic">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, sets whether PyTorch operations must use deterministic algorithms.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.devices"></span><strong>devices</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.devices">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – Will be mapped to either <cite>gpus</cite>, <cite>tpu_cores</cite>, <cite>num_processes</cite> or <cite>ipus</cite>,
based on the accelerator type.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.fast_dev_run"></span><strong>fast_dev_run</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.fast_dev_run">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>]) – Runs n if set to <code class="docutils literal notranslate"><span class="pre">n</span></code> (int) else 1 if set to <code class="docutils literal notranslate"><span class="pre">True</span></code> batch(es)
of train, val and test to find any bugs (ie: a sort of unit test).
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.flush_logs_every_n_steps"></span><strong>flush_logs_every_n_steps</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.flush_logs_every_n_steps">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – <p>How often to flush logs to disk (defaults to every 100 steps).</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span><code class="docutils literal notranslate"><span class="pre">flush_logs_every_n_steps</span></code> has been deprecated in v1.5 and will be removed in v1.7.
Please configure flushing directly in the logger instead.</p>
</div>
</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.gpus"></span><strong>gpus</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.gpus">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – Number of GPUs to train on (int) or which GPUs to train on (list or str) applied per node
Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.gradient_clip_val"></span><strong>gradient_clip_val</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.gradient_clip_val">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – The value at which to clip gradients. Passing <code class="docutils literal notranslate"><span class="pre">gradient_clip_val=None</span></code> disables
gradient clipping. If using Automatic Mixed Precision (AMP), the gradients will be unscaled before.
Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.gradient_clip_algorithm"></span><strong>gradient_clip_algorithm</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.gradient_clip_algorithm">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – The gradient clipping algorithm to use. Pass <code class="docutils literal notranslate"><span class="pre">gradient_clip_algorithm=&quot;value&quot;</span></code>
to clip by value, and <code class="docutils literal notranslate"><span class="pre">gradient_clip_algorithm=&quot;norm&quot;</span></code> to clip by norm. By default it will
be set to <code class="docutils literal notranslate"><span class="pre">&quot;norm&quot;</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.limit_train_batches"></span><strong>limit_train_batches</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.limit_train_batches">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – How much of training dataset to check (float = fraction, int = num_batches).
Default: <code class="docutils literal notranslate"><span class="pre">1.0</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.limit_val_batches"></span><strong>limit_val_batches</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.limit_val_batches">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – How much of validation dataset to check (float = fraction, int = num_batches).
Default: <code class="docutils literal notranslate"><span class="pre">1.0</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.limit_test_batches"></span><strong>limit_test_batches</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.limit_test_batches">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – How much of test dataset to check (float = fraction, int = num_batches).
Default: <code class="docutils literal notranslate"><span class="pre">1.0</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.limit_predict_batches"></span><strong>limit_predict_batches</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.limit_predict_batches">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – How much of prediction dataset to check (float = fraction, int = num_batches).
Default: <code class="docutils literal notranslate"><span class="pre">1.0</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.logger"></span><strong>logger</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.logger">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningLoggerBase</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Iterable</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningLoggerBase</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>]) – Logger (or iterable collection of loggers) for experiment tracking. A <code class="docutils literal notranslate"><span class="pre">True</span></code> value uses
the default <code class="docutils literal notranslate"><span class="pre">TensorBoardLogger</span></code>. <code class="docutils literal notranslate"><span class="pre">False</span></code> will disable logging. If multiple loggers are
provided and the <cite>save_dir</cite> property of that logger is not set, local files (checkpoints,
profiler traces, etc.) are saved in <code class="docutils literal notranslate"><span class="pre">default_root_dir</span></code> rather than in the <code class="docutils literal notranslate"><span class="pre">log_dir</span></code> of any
of the individual loggers.
Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.log_gpu_memory"></span><strong>log_gpu_memory</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.log_gpu_memory">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – <p>None, ‘min_max’, ‘all’. Might slow performance.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span>Deprecated in v1.5.0 and will be removed in v1.7.0
Please use the <code class="docutils literal notranslate"><span class="pre">DeviceStatsMonitor</span></code> callback directly instead.</p>
</div>
</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.log_every_n_steps"></span><strong>log_every_n_steps</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.log_every_n_steps">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – How often to log within steps.
Default: <code class="docutils literal notranslate"><span class="pre">50</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.prepare_data_per_node"></span><strong>prepare_data_per_node</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.prepare_data_per_node">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>]) – <p>If True, each LOCAL_RANK=0 will call prepare data.
Otherwise only NODE_RANK=0, LOCAL_RANK=0 will prepare data</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span>Deprecated in v1.5.0 and will be removed in v1.7.0
Please set <code class="docutils literal notranslate"><span class="pre">prepare_data_per_node</span></code> in <code class="docutils literal notranslate"><span class="pre">LightningDataModule</span></code> and/or
<code class="docutils literal notranslate"><span class="pre">LightningModule</span></code> directly instead.</p>
</div>
</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.process_position"></span><strong>process_position</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.process_position">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – <p>Orders the progress bar when running multiple models on same machine.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span><code class="docutils literal notranslate"><span class="pre">process_position</span></code> has been deprecated in v1.5 and will be removed in v1.7.
Please pass <code class="xref py py-class docutils literal notranslate"><span class="pre">TQDMProgressBar</span></code> with <code class="docutils literal notranslate"><span class="pre">process_position</span></code>
directly to the Trainer’s <code class="docutils literal notranslate"><span class="pre">callbacks</span></code> argument instead.</p>
</div>
</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.progress_bar_refresh_rate"></span><strong>progress_bar_refresh_rate</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.progress_bar_refresh_rate">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – <p>How often to refresh progress bar (in steps). Value <code class="docutils literal notranslate"><span class="pre">0</span></code> disables progress bar.
Ignored when a custom progress bar is passed to <code class="xref py py-paramref docutils literal notranslate"><span class="pre">callbacks</span></code>. Default: None, means
a suitable value will be chosen based on the environment (terminal, Google COLAB, etc.).</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span><code class="docutils literal notranslate"><span class="pre">progress_bar_refresh_rate</span></code> has been deprecated in v1.5 and will be removed in v1.7.
Please pass <code class="xref py py-class docutils literal notranslate"><span class="pre">TQDMProgressBar</span></code> with <code class="docutils literal notranslate"><span class="pre">refresh_rate</span></code>
directly to the Trainer’s <code class="docutils literal notranslate"><span class="pre">callbacks</span></code> argument instead. To disable the progress bar,
pass <code class="docutils literal notranslate"><span class="pre">enable_progress_bar</span> <span class="pre">=</span> <span class="pre">False</span></code> to the Trainer.</p>
</div>
</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.enable_progress_bar"></span><strong>enable_progress_bar</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.enable_progress_bar">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – Whether to enable to progress bar by default.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.profiler"></span><strong>profiler</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.profiler">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">BaseProfiler</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – To profile individual steps during training and assist in identifying bottlenecks.
Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.overfit_batches"></span><strong>overfit_batches</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.overfit_batches">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>]) – Overfit a fraction of training data (float) or a set number of batches (int).
Default: <code class="docutils literal notranslate"><span class="pre">0.0</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.plugins"></span><strong>plugins</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.plugins">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Strategy</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">PrecisionPlugin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">ClusterEnvironment</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">CheckpointIO</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">LayerSync</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Strategy</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">PrecisionPlugin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">ClusterEnvironment</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">CheckpointIO</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">LayerSync</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – Plugins allow modification of core behavior like ddp and amp, and enable custom lightning plugins.
Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.precision"></span><strong>precision</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.precision">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Double precision (64), full precision (32), half precision (16) or bfloat16 precision (bf16).
Can be used on CPU, GPU, TPUs, HPUs or IPUs.
Default: <code class="docutils literal notranslate"><span class="pre">32</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.max_epochs"></span><strong>max_epochs</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.max_epochs">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – Stop training once this number of epochs is reached. Disabled by default (None).
If both max_epochs and max_steps are not specified, defaults to <code class="docutils literal notranslate"><span class="pre">max_epochs</span> <span class="pre">=</span> <span class="pre">1000</span></code>.
To enable infinite training, set <code class="docutils literal notranslate"><span class="pre">max_epochs</span> <span class="pre">=</span> <span class="pre">-1</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.min_epochs"></span><strong>min_epochs</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.min_epochs">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – Force training for at least these many epochs. Disabled by default (None).</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.max_steps"></span><strong>max_steps</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.max_steps">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Stop training after this number of steps. Disabled by default (-1). If <code class="docutils literal notranslate"><span class="pre">max_steps</span> <span class="pre">=</span> <span class="pre">-1</span></code>
and <code class="docutils literal notranslate"><span class="pre">max_epochs</span> <span class="pre">=</span> <span class="pre">None</span></code>, will default to <code class="docutils literal notranslate"><span class="pre">max_epochs</span> <span class="pre">=</span> <span class="pre">1000</span></code>. To enable infinite training, set
<code class="docutils literal notranslate"><span class="pre">max_epochs</span></code> to <code class="docutils literal notranslate"><span class="pre">-1</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.min_steps"></span><strong>min_steps</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.min_steps">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – Force training for at least these number of steps. Disabled by default (<code class="docutils literal notranslate"><span class="pre">None</span></code>).</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.max_time"></span><strong>max_time</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.max_time">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">timedelta</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – Stop training after this amount of time has passed. Disabled by default (<code class="docutils literal notranslate"><span class="pre">None</span></code>).
The time duration can be specified in the format DD:HH:MM:SS (days, hours, minutes seconds), as a
<code class="xref py py-class docutils literal notranslate"><span class="pre">datetime.timedelta</span></code>, or a dictionary with keys that will be passed to
<code class="xref py py-class docutils literal notranslate"><span class="pre">datetime.timedelta</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.num_nodes"></span><strong>num_nodes</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.num_nodes">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Number of GPU nodes for distributed training.
Default: <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.num_processes"></span><strong>num_processes</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.num_processes">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – Number of processes for distributed training with <code class="docutils literal notranslate"><span class="pre">accelerator=&quot;cpu&quot;</span></code>.
Default: <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.num_sanity_val_steps"></span><strong>num_sanity_val_steps</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.num_sanity_val_steps">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Sanity check runs n validation batches before starting the training routine.
Set it to <cite>-1</cite> to run all batches in all validation dataloaders.
Default: <code class="docutils literal notranslate"><span class="pre">2</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.reload_dataloaders_every_n_epochs"></span><strong>reload_dataloaders_every_n_epochs</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.reload_dataloaders_every_n_epochs">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Set to a non-negative integer to reload dataloaders every n epochs.
Default: <code class="docutils literal notranslate"><span class="pre">0</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.replace_sampler_ddp"></span><strong>replace_sampler_ddp</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.replace_sampler_ddp">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – Explicitly enables or disables sampler replacement. If not specified this
will toggled automatically when DDP is used. By default it will add <code class="docutils literal notranslate"><span class="pre">shuffle=True</span></code> for
train sampler and <code class="docutils literal notranslate"><span class="pre">shuffle=False</span></code> for val/test sampler. If you want to customize it,
you can set <code class="docutils literal notranslate"><span class="pre">replace_sampler_ddp=False</span></code> and add your own distributed sampler.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.resume_from_checkpoint"></span><strong>resume_from_checkpoint</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.resume_from_checkpoint">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Path</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – <p>Path/URL of the checkpoint from which training is resumed. If there is
no checkpoint file at the path, an exception is raised. If resuming from mid-epoch checkpoint,
training will start from the beginning of the next epoch.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span><code class="docutils literal notranslate"><span class="pre">resume_from_checkpoint</span></code> is deprecated in v1.5 and will be removed in v2.0.
Please pass the path to <code class="docutils literal notranslate"><span class="pre">Trainer.fit(...,</span> <span class="pre">ckpt_path=...)</span></code> instead.</p>
</div>
</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.strategy"></span><strong>strategy</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.strategy">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Strategy</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – Supports different training strategies with aliases
as well custom strategies.
Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.sync_batchnorm"></span><strong>sync_batchnorm</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.sync_batchnorm">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – Synchronize batch norm layers between process groups/whole world.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.terminate_on_nan"></span><strong>terminate_on_nan</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.terminate_on_nan">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>]) – <p>If set to True, will terminate training (by raising a <cite>ValueError</cite>) at the
end of each training batch, if any of the parameters or the loss are NaN or +/-inf.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span>Trainer argument <code class="docutils literal notranslate"><span class="pre">terminate_on_nan</span></code> was deprecated in v1.5 and will be removed in 1.7.
Please use <code class="docutils literal notranslate"><span class="pre">detect_anomaly</span></code> instead.</p>
</div>
</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.detect_anomaly"></span><strong>detect_anomaly</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.detect_anomaly">¶</a> – Enable anomaly detection for the autograd engine.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.tpu_cores"></span><strong>tpu_cores</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.tpu_cores">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – How many TPU cores to train on (1 or 8) / Single TPU to train on (1)
Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.ipus"></span><strong>ipus</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.ipus">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – How many IPUs to train on.
Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.track_grad_norm"></span><strong>track_grad_norm</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.track_grad_norm">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – -1 no tracking. Otherwise tracks that p-norm. May be set to ‘inf’ infinity-norm. If using
Automatic Mixed Precision (AMP), the gradients will be unscaled before logging them.
Default: <code class="docutils literal notranslate"><span class="pre">-1</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.val_check_interval"></span><strong>val_check_interval</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.val_check_interval">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – How often to check the validation set. Pass a <code class="docutils literal notranslate"><span class="pre">float</span></code> in the range [0.0, 1.0] to check
after a fraction of the training epoch. Pass an <code class="docutils literal notranslate"><span class="pre">int</span></code> to check after a fixed number of training
batches.
Default: <code class="docutils literal notranslate"><span class="pre">1.0</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.enable_model_summary"></span><strong>enable_model_summary</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.enable_model_summary">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – Whether to enable model summarization by default.
Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.weights_summary"></span><strong>weights_summary</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.weights_summary">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – <p>Prints a summary of the weights when training begins.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span><code class="docutils literal notranslate"><span class="pre">weights_summary</span></code> has been deprecated in v1.5 and will be removed in v1.7.
To disable the summary, pass <code class="docutils literal notranslate"><span class="pre">enable_model_summary</span> <span class="pre">=</span> <span class="pre">False</span></code> to the Trainer.
To customize the summary, pass <code class="xref py py-class docutils literal notranslate"><span class="pre">ModelSummary</span></code>
directly to the Trainer’s <code class="docutils literal notranslate"><span class="pre">callbacks</span></code> argument.</p>
</div>
</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.weights_save_path"></span><strong>weights_save_path</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.weights_save_path">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – <p>Where to save weights if specified. Will override default_root_dir
for checkpoints only. Use this if for whatever reason you need the checkpoints
stored in a different place than the logs written in <cite>default_root_dir</cite>.
Can be remote file paths such as <cite>s3://mybucket/path</cite> or ‘hdfs://path/’
Defaults to <cite>default_root_dir</cite>.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.6: </span><code class="docutils literal notranslate"><span class="pre">weights_save_path</span></code> has been deprecated in v1.6 and will be removed in v1.8. Please pass
<code class="docutils literal notranslate"><span class="pre">dirpath</span></code> directly to the <code class="xref py py-class docutils literal notranslate"><span class="pre">ModelCheckpoint</span></code>
callback.</p>
</div>
</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.move_metrics_to_cpu"></span><strong>move_metrics_to_cpu</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.move_metrics_to_cpu">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – Whether to force internal logged metrics to be moved to cpu.
This can save some gpu memory, but can make training slower. Use with attention.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.multiple_trainloader_mode"></span><strong>multiple_trainloader_mode</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.multiple_trainloader_mode">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – How to loop over the datasets when there are multiple train loaders.
In ‘max_size_cycle’ mode, the trainer ends one epoch when the largest dataset is traversed,
and smaller datasets reload when running out of their data. In ‘min_size’ mode, all the datasets
reload when reaching the minimum length of datasets.
Default: <code class="docutils literal notranslate"><span class="pre">&quot;max_size_cycle&quot;</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.params.stochastic_weight_avg"></span><strong>stochastic_weight_avg</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.params.stochastic_weight_avg">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – <p>Whether to use <a class="reference external" href="https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/">Stochastic Weight Averaging (SWA)</a>.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span><code class="docutils literal notranslate"><span class="pre">stochastic_weight_avg</span></code> has been deprecated in v1.5 and will be removed in v1.7.
Please pass <code class="xref py py-class docutils literal notranslate"><span class="pre">StochasticWeightAveraging</span></code>
directly to the Trainer’s <code class="docutils literal notranslate"><span class="pre">callbacks</span></code> argument instead.</p>
</div>
</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="fit">
<h4>fit<a class="headerlink" href="#fit" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">Trainer.</span></span><span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_dataloaders</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">val_dataloaders</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">datamodule</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ckpt_path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_lightning/trainer/trainer.html#Trainer.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Runs the full optimization routine.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.fit.params.model"></span><strong>model</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.fit.params.model">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code>) – Model to fit.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.fit.params.train_dataloaders"></span><strong>train_dataloaders</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.fit.params.train_dataloaders">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]], <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]], <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]], <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]], <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningDataModule</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – A collection of <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> or a
<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningDataModule</span></code> specifying training samples.
In the case of multiple dataloaders, please see this <a class="reference internal" href="../guides/data.html#multiple-dataloaders"><span class="std std-ref">section</span></a>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.fit.params.val_dataloaders"></span><strong>val_dataloaders</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.fit.params.val_dataloaders">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – A <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> or a sequence of them specifying validation samples.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.fit.params.ckpt_path"></span><strong>ckpt_path</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.fit.params.ckpt_path">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Path/URL of the checkpoint from which training is resumed. If there is
no checkpoint file at the path, an exception is raised. If resuming from mid-epoch checkpoint,
training will start from the beginning of the next epoch.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.fit.params.datamodule"></span><strong>datamodule</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.fit.params.datamodule">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningDataModule</span></code>]) – An instance of <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningDataModule</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="validate">
<h4>validate<a class="headerlink" href="#validate" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">Trainer.</span></span><span class="sig-name descname"><span class="pre">validate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloaders</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ckpt_path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">datamodule</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_lightning/trainer/trainer.html#Trainer.validate"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Perform one evaluation epoch over the validation set.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.validate.params.model"></span><strong>model</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.validate.params.model">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code>]) – The model to validate.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.validate.params.dataloaders"></span><strong>dataloaders</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.validate.params.dataloaders">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningDataModule</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – A <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> or a sequence of them,
or a <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningDataModule</span></code> specifying validation samples.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.validate.params.ckpt_path"></span><strong>ckpt_path</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.validate.params.ckpt_path">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Either <code class="docutils literal notranslate"><span class="pre">best</span></code> or path to the checkpoint you wish to validate.
If <code class="docutils literal notranslate"><span class="pre">None</span></code> and the model instance was passed, use the current weights.
Otherwise, the best model checkpoint from the previous <code class="docutils literal notranslate"><span class="pre">trainer.fit</span></code> call will be loaded
if a checkpoint callback is configured.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.validate.params.verbose"></span><strong>verbose</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.validate.params.verbose">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – If True, prints the validation results.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.validate.params.datamodule"></span><strong>datamodule</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.validate.params.datamodule">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningDataModule</span></code>]) – An instance of <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningDataModule</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>]]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>List of dictionaries with metrics logged during the validation phase, e.g., in model- or callback hooks
like <code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code>,
<code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_epoch_end()</span></code>, etc.
The length of the list corresponds to the number of validation dataloaders used.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="test">
<h4>test<a class="headerlink" href="#test" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">Trainer.</span></span><span class="sig-name descname"><span class="pre">test</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloaders</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ckpt_path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">datamodule</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_lightning/trainer/trainer.html#Trainer.test"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Perform one evaluation epoch over the test set.
It’s separated from fit to make sure you never run on your test set until you want to.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.test.params.model"></span><strong>model</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.test.params.model">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code>]) – The model to test.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.test.params.dataloaders"></span><strong>dataloaders</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.test.params.dataloaders">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningDataModule</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – A <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> or a sequence of them,
or a <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningDataModule</span></code> specifying test samples.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.test.params.ckpt_path"></span><strong>ckpt_path</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.test.params.ckpt_path">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Either <code class="docutils literal notranslate"><span class="pre">best</span></code> or path to the checkpoint you wish to test.
If <code class="docutils literal notranslate"><span class="pre">None</span></code> and the model instance was passed, use the current weights.
Otherwise, the best model checkpoint from the previous <code class="docutils literal notranslate"><span class="pre">trainer.fit</span></code> call will be loaded
if a checkpoint callback is configured.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.test.params.verbose"></span><strong>verbose</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.test.params.verbose">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – If True, prints the test results.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.test.params.datamodule"></span><strong>datamodule</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.test.params.datamodule">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningDataModule</span></code>]) – An instance of <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningDataModule</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>]]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>List of dictionaries with metrics logged during the test phase, e.g., in model- or callback hooks
like <code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code>,
<code class="xref py py-meth docutils literal notranslate"><span class="pre">test_epoch_end()</span></code>, etc.
The length of the list corresponds to the number of test dataloaders used.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="predict">
<h4>predict<a class="headerlink" href="#predict" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">Trainer.</span></span><span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloaders</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">datamodule</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_predictions</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ckpt_path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_lightning/trainer/trainer.html#Trainer.predict"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Run inference on your data.
This will call the model forward function to compute predictions. Useful to perform distributed
and batched predictions. Logging is disabled in the predict hooks.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.predict.params.model"></span><strong>model</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.predict.params.model">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code>]) – The model to predict with.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.predict.params.dataloaders"></span><strong>dataloaders</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.predict.params.dataloaders">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningDataModule</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – A <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> or a sequence of them,
or a <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningDataModule</span></code> specifying prediction samples.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.predict.params.datamodule"></span><strong>datamodule</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.predict.params.datamodule">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningDataModule</span></code>]) – The datamodule with a predict_dataloader method that returns one or more dataloaders.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.predict.params.return_predictions"></span><strong>return_predictions</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.predict.params.return_predictions">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>]) – Whether to return predictions.
<code class="docutils literal notranslate"><span class="pre">True</span></code> by default except when an accelerator that spawns processes is used (not supported).</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.predict.params.ckpt_path"></span><strong>ckpt_path</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.predict.params.ckpt_path">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Either <code class="docutils literal notranslate"><span class="pre">best</span></code> or path to the checkpoint you wish to predict.
If <code class="docutils literal notranslate"><span class="pre">None</span></code> and the model instance was passed, use the current weights.
Otherwise, the best model checkpoint from the previous <code class="docutils literal notranslate"><span class="pre">trainer.fit</span></code> call will be loaded
if a checkpoint callback is configured.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Returns a list of dictionaries, one for each provided dataloader containing their respective predictions.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="tune">
<h4>tune<a class="headerlink" href="#tune" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">Trainer.</span></span><span class="sig-name descname"><span class="pre">tune</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_dataloaders</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">val_dataloaders</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">datamodule</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_batch_size_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr_find_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_lightning/trainer/trainer.html#Trainer.tune"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Runs routines to tune hyperparameters before training.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.tune.params.model"></span><strong>model</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.tune.params.model">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code>) – Model to tune.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.tune.params.train_dataloaders"></span><strong>train_dataloaders</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.tune.params.train_dataloaders">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]], <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]], <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]], <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]], <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningDataModule</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – A collection of <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> or a
<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningDataModule</span></code> specifying training samples.
In the case of multiple dataloaders, please see this <a class="reference internal" href="../guides/data.html#multiple-dataloaders"><span class="std std-ref">section</span></a>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.tune.params.val_dataloaders"></span><strong>val_dataloaders</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.tune.params.val_dataloaders">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – A <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> or a sequence of them specifying validation samples.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.tune.params.datamodule"></span><strong>datamodule</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.tune.params.datamodule">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningDataModule</span></code>]) – An instance of <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningDataModule</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.tune.params.scale_batch_size_kwargs"></span><strong>scale_batch_size_kwargs</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.tune.params.scale_batch_size_kwargs">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – Arguments for <code class="xref py py-func docutils literal notranslate"><span class="pre">scale_batch_size()</span></code></p></li>
<li><p><span class="target" id="pytorch_lightning.trainer.Trainer.tune.params.lr_find_kwargs"></span><strong>lr_find_kwargs</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.trainer.Trainer.tune.params.lr_find_kwargs">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – Arguments for <code class="xref py py-func docutils literal notranslate"><span class="pre">lr_find()</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">_LRFinder</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]]</p>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="properties">
<h3>Properties<a class="headerlink" href="#properties" title="Permalink to this headline">¶</a></h3>
<section id="callback-metrics">
<h4>callback_metrics<a class="headerlink" href="#callback-metrics" title="Permalink to this headline">¶</a></h4>
<p>The metrics available to callbacks. These are automatically set when you log via <cite>self.log</cite></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;a_val&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>


<span class="n">callback_metrics</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">callback_metrics</span>
<span class="k">assert</span> <span class="n">callback_metrics</span><span class="p">[</span><span class="s2">&quot;a_val&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">2</span>
</pre></div>
</div>
</section>
<section id="current-epoch">
<h4>current_epoch<a class="headerlink" href="#current-epoch" title="Permalink to this headline">¶</a></h4>
<p>The number of epochs run.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">trainer</span><span class="o">.</span><span class="n">current_epoch</span> <span class="o">&gt;=</span> <span class="mi">10</span><span class="p">:</span>
    <span class="o">...</span>
</pre></div>
</div>
</section>
<section id="global-step">
<h4>global_step<a class="headerlink" href="#global-step" title="Permalink to this headline">¶</a></h4>
<p>The number of optimizer steps taken (does not reset each epoch).
This includes multiple optimizers and TBPTT steps (if enabled).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span> <span class="o">&gt;=</span> <span class="mi">100</span><span class="p">:</span>
    <span class="o">...</span>
</pre></div>
</div>
</section>
<section id="id6">
<h4>logger<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h4>
<p>The current logger being used. Here’s an example using tensorboard</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">logger</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">logger</span>
<span class="n">tensorboard</span> <span class="o">=</span> <span class="n">logger</span><span class="o">.</span><span class="n">experiment</span>
</pre></div>
</div>
</section>
<section id="loggers">
<h4>loggers<a class="headerlink" href="#loggers" title="Permalink to this headline">¶</a></h4>
<p>The list of loggers currently being used by the Trainer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># List of Logger objects</span>
<span class="n">loggers</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">loggers</span>
<span class="k">for</span> <span class="n">logger</span> <span class="ow">in</span> <span class="n">loggers</span><span class="p">:</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">log_metrics</span><span class="p">({</span><span class="s2">&quot;foo&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">})</span>
</pre></div>
</div>
</section>
<section id="logged-metrics">
<h4>logged_metrics<a class="headerlink" href="#logged-metrics" title="Permalink to this headline">¶</a></h4>
<p>The metrics sent to the logger (visualizer).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;a_val&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">logger</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="n">logged_metrics</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">logged_metrics</span>
<span class="k">assert</span> <span class="n">logged_metrics</span><span class="p">[</span><span class="s2">&quot;a_val&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">2</span>
</pre></div>
</div>
</section>
<section id="log-dir">
<h4>log_dir<a class="headerlink" href="#log-dir" title="Permalink to this headline">¶</a></h4>
<p>The directory for the current experiment. Use this to save images to, etc…</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">img</span> <span class="o">=</span> <span class="o">...</span>
    <span class="n">save_img</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">log_dir</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="is-global-zero">
<h4>is_global_zero<a class="headerlink" href="#is-global-zero" title="Permalink to this headline">¶</a></h4>
<p>Whether this process is the global zero in multi-node training</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">is_global_zero</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;in node 0, accelerator 0&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="progress-bar-metrics">
<h4>progress_bar_metrics<a class="headerlink" href="#progress-bar-metrics" title="Permalink to this headline">¶</a></h4>
<p>The metrics sent to the progress bar.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;a_val&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">prog_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="n">progress_bar_metrics</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">progress_bar_metrics</span>
<span class="k">assert</span> <span class="n">progress_bar_metrics</span><span class="p">[</span><span class="s2">&quot;a_val&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">2</span>
</pre></div>
</div>
</section>
<section id="estimated-stepping-batches">
<h4>estimated_stepping_batches<a class="headerlink" href="#estimated-stepping-batches" title="Permalink to this headline">¶</a></h4>
<p>Check out <code class="xref py py-meth docutils literal notranslate"><span class="pre">estimated_stepping_batches()</span></code>.</p>
</section>
<section id="state">
<h4>state<a class="headerlink" href="#state" title="Permalink to this headline">¶</a></h4>
<p>The current state of the Trainer, including the current function that is running, the stage of
execution within that function, and the status of the Trainer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># fn in (&quot;fit&quot;, &quot;validate&quot;, &quot;test&quot;, &quot;predict&quot;, &quot;tune&quot;)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">fn</span>
<span class="c1"># status in (&quot;initializing&quot;, &quot;running&quot;, &quot;finished&quot;, &quot;interrupted&quot;)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">status</span>
<span class="c1"># stage in (&quot;train&quot;, &quot;sanity_check&quot;, &quot;validate&quot;, &quot;test&quot;, &quot;predict&quot;, &quot;tune&quot;)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">stage</span>
</pre></div>
</div>
</section>
</section>
</section>
</section>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="evaluation.html" class="btn btn-neutral float-right" title="Add validation and test datasets" accesskey="n" rel="next">Next <img src="../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="lightning_module.html" class="btn btn-neutral" title="LightningModule" accesskey="p" rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright Copyright (c) 2018-2022, William Falcon et al...

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Trainer</a><ul>
<li><a class="reference internal" href="#basic-use">Basic use</a></li>
<li><a class="reference internal" href="#under-the-hood">Under the hood</a></li>
<li><a class="reference internal" href="#trainer-in-python-scripts">Trainer in Python scripts</a></li>
<li><a class="reference internal" href="#validation">Validation</a></li>
<li><a class="reference internal" href="#testing">Testing</a></li>
<li><a class="reference internal" href="#reproducibility">Reproducibility</a></li>
<li><a class="reference internal" href="#trainer-flags">Trainer flags</a><ul>
<li><a class="reference internal" href="#accelerator">accelerator</a></li>
<li><a class="reference internal" href="#accumulate-grad-batches">accumulate_grad_batches</a></li>
<li><a class="reference internal" href="#amp-backend">amp_backend</a></li>
<li><a class="reference internal" href="#amp-level">amp_level</a></li>
<li><a class="reference internal" href="#auto-scale-batch-size">auto_scale_batch_size</a></li>
<li><a class="reference internal" href="#auto-select-gpus">auto_select_gpus</a></li>
<li><a class="reference internal" href="#auto-lr-find">auto_lr_find</a></li>
<li><a class="reference internal" href="#benchmark">benchmark</a></li>
<li><a class="reference internal" href="#deterministic">deterministic</a></li>
<li><a class="reference internal" href="#callbacks">callbacks</a></li>
<li><a class="reference internal" href="#check-val-every-n-epoch">check_val_every_n_epoch</a></li>
<li><a class="reference internal" href="#checkpoint-callback">checkpoint_callback</a></li>
<li><a class="reference internal" href="#default-root-dir">default_root_dir</a></li>
<li><a class="reference internal" href="#devices">devices</a></li>
<li><a class="reference internal" href="#enable-checkpointing">enable_checkpointing</a></li>
<li><a class="reference internal" href="#fast-dev-run">fast_dev_run</a></li>
<li><a class="reference internal" href="#flush-logs-every-n-steps">flush_logs_every_n_steps</a></li>
<li><a class="reference internal" href="#gpus">gpus</a></li>
<li><a class="reference internal" href="#gradient-clip-val">gradient_clip_val</a></li>
<li><a class="reference internal" href="#limit-train-batches">limit_train_batches</a></li>
<li><a class="reference internal" href="#limit-test-batches">limit_test_batches</a></li>
<li><a class="reference internal" href="#limit-val-batches">limit_val_batches</a></li>
<li><a class="reference internal" href="#log-every-n-steps">log_every_n_steps</a></li>
<li><a class="reference internal" href="#logger">logger</a></li>
<li><a class="reference internal" href="#max-epochs">max_epochs</a></li>
<li><a class="reference internal" href="#min-epochs">min_epochs</a></li>
<li><a class="reference internal" href="#max-steps">max_steps</a></li>
<li><a class="reference internal" href="#min-steps">min_steps</a></li>
<li><a class="reference internal" href="#max-time">max_time</a></li>
<li><a class="reference internal" href="#num-nodes">num_nodes</a></li>
<li><a class="reference internal" href="#num-processes">num_processes</a></li>
<li><a class="reference internal" href="#num-sanity-val-steps">num_sanity_val_steps</a></li>
<li><a class="reference internal" href="#overfit-batches">overfit_batches</a></li>
<li><a class="reference internal" href="#plugins">plugins</a></li>
<li><a class="reference internal" href="#precision">precision</a></li>
<li><a class="reference internal" href="#process-position">process_position</a></li>
<li><a class="reference internal" href="#profiler">profiler</a></li>
<li><a class="reference internal" href="#enable-progress-bar">enable_progress_bar</a></li>
<li><a class="reference internal" href="#reload-dataloaders-every-n-epochs">reload_dataloaders_every_n_epochs</a></li>
<li><a class="reference internal" href="#replace-sampler-ddp">replace_sampler_ddp</a></li>
<li><a class="reference internal" href="#resume-from-checkpoint">resume_from_checkpoint</a></li>
<li><a class="reference internal" href="#strategy">strategy</a></li>
<li><a class="reference internal" href="#sync-batchnorm">sync_batchnorm</a></li>
<li><a class="reference internal" href="#track-grad-norm">track_grad_norm</a></li>
<li><a class="reference internal" href="#tpu-cores">tpu_cores</a></li>
<li><a class="reference internal" href="#val-check-interval">val_check_interval</a></li>
<li><a class="reference internal" href="#weights-save-path">weights_save_path</a></li>
<li><a class="reference internal" href="#weights-summary">weights_summary</a></li>
<li><a class="reference internal" href="#enable-model-summary">enable_model_summary</a></li>
</ul>
</li>
<li><a class="reference internal" href="#trainer-class-api">Trainer class API</a><ul>
<li><a class="reference internal" href="#methods">Methods</a><ul>
<li><a class="reference internal" href="#init">init</a></li>
<li><a class="reference internal" href="#fit">fit</a></li>
<li><a class="reference internal" href="#validate">validate</a></li>
<li><a class="reference internal" href="#test">test</a></li>
<li><a class="reference internal" href="#predict">predict</a></li>
<li><a class="reference internal" href="#tune">tune</a></li>
</ul>
</li>
<li><a class="reference internal" href="#properties">Properties</a><ul>
<li><a class="reference internal" href="#callback-metrics">callback_metrics</a></li>
<li><a class="reference internal" href="#current-epoch">current_epoch</a></li>
<li><a class="reference internal" href="#global-step">global_step</a></li>
<li><a class="reference internal" href="#id6">logger</a></li>
<li><a class="reference internal" href="#loggers">loggers</a></li>
<li><a class="reference internal" href="#logged-metrics">logged_metrics</a></li>
<li><a class="reference internal" href="#log-dir">log_dir</a></li>
<li><a class="reference internal" href="#is-global-zero">is_global_zero</a></li>
<li><a class="reference internal" href="#progress-bar-metrics">progress_bar_metrics</a></li>
<li><a class="reference internal" href="#estimated-stepping-batches">estimated_stepping_batches</a></li>
<li><a class="reference internal" href="#state">state</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
         <script src="../_static/jquery.js"></script>
         <script src="../_static/underscore.js"></script>
         <script src="../_static/doctools.js"></script>
         <script src="../_static/clipboard.min.js"></script>
         <script src="../_static/copybutton.js"></script>
         <script src="../_static/copybutton.js"></script>
         <script>let toggleHintShow = 'Click to show';</script>
         <script>let toggleHintHide = 'Click to hide';</script>
         <script>let toggleOpenOnPrint = 'true';</script>
         <script src="../_static/togglebutton.js"></script>
         <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Best practices', 'Optional Extensions', 'Tutorials', 'API References', 'Bolts', 'Examples', 'Partner Domain Frameworks', 'Community'];
</script>



  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <!--
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch-lightning.rtfd.io/en/latest">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch-lightning.readthedocs.io/en/latest/#tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch-lightning.readthedocs.io/en/latest/#community-examples">View Resources</a>
        </div>
        -->
      </div>
    </div>
  </div>

  <!--
  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch-lightning.rtfd.io/en/latest/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch-lightning.rtfd.io/en/latest/">PyTorch</a></li>
            <li><a href="https://pytorch-lightning.readthedocs.io/en/latest/starter/introduction.html">Get Started</a></li>
            <li><a href="https://pytorch-lightning.rtfd.io/en/latest/">Features</a></li>
            <li><a href="">Ecosystem</a></li>
            <li><a href="https://www.pytorchlightning.ai/blog">Blog</a></li>
            <li><a href="https://github.com/PyTorchLightning/pytorch-lightning/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch-lightning.readthedocs.io/en/latest/#community-examples">Resources</a></li>
            <li><a href="https://pytorch-lightning.readthedocs.io/en/latest/#tutorials">Tutorials</a></li>
            <li><a href="https://pytorch-lightning.rtfd.io/en/latest">Docs</a></li>
            <li><a href="https://www.pytorchlightning.ai/community" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/PyTorchLightning/pytorch-lightning/issues" target="_blank">Github Issues</a></li>
            <li><a href="" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Stay Connected</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/PyTorchLightnin" target="_blank" class="twitter"></a>
            <a href="" target="_blank" class="youtube"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>
  -->

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. Read PyTorch Lightning's <a href="https://pytorchlightning.ai/privacy-policy">Privacy Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch-lightning.rtfd.io/en/latest/" aria-label="PyTorch Lightning"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <!-- <li>
            <a href="https://pytorch-lightning.readthedocs.io/en/latest/starter/introduction.html">Get Started</a>
          </li>

          <li>
            <a href="https://www.pytorchlightning.ai/blog">Blog</a>
          </li>

          <li class="resources-mobile-menu-title">
            Ecosystem
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch-lightning.readthedocs.io/en/stable/">PyTorch Lightning</a>
            </li>

            <li>
              <a href="https://torchmetrics.readthedocs.io/en/stable/">TorchMetrics</a>
            </li>

            <li>
              <a href="https://lightning-flash.readthedocs.io/en/stable/">Lightning Flash</a>
            </li>

            <li>
              <a href="https://lightning-transformers.readthedocs.io/en/stable/">Lightning Transformers</a>
            </li>

            <li>
              <a href="https://lightning-bolts.readthedocs.io/en/stable/">Lightning Bolts</a>
            </li>
          </ul> -->

          <!--<li class="resources-mobile-menu-title">
            Resources
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch-lightning.readthedocs.io/en/latest/#community-examples">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch-lightning.rtfd.io/en/latest/">About</a>
            </li>

            <li>
              <a href="">Models (Beta)</a>
            </li>

            <li>
              <a href="https://www.pytorchlightning.ai/community">Community</a>
            </li>

            <li>
              <a href="https://github.com/PyTorchLightning/pytorch-lightning/discussions">Forums</a>
            </li>
          </ul>-->

          <!-- <li>
            <a href="https://github.com/PyTorchLightning/pytorch-lightning">Github</a>
          </li> -->

          <!-- <li>
            <a href="https://www.grid.ai/">Grid.ai</a>
          </li> -->
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>

 </body>
</html>