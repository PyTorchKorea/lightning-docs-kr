


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>

  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="google-site-verification" content="okUst94cAlWSsUsGZTB4xSS4UKTtRV8Nu5XZ9pdd3Aw" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>LightningModule &mdash; PyTorch Lightning 1.7.0dev documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/icon.svg"/>
  
  
  
    <link rel="canonical" href="https://pytorch-lightning.readthedocs.io/en/stable//common/lightning_module.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../_static/togglebutton.css" type="text/css" />
  <link rel="stylesheet" href="../_static/main.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sphinx_paramlinks.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Trainer" href="trainer.html" />
    <link rel="prev" title="Expert skills" href="../levels/expert.html" />
  <!-- Google Analytics -->
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-82W25RV60Q"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-82W25RV60Q');
    </script>
  
  <!-- End Google Analytics -->
  

  
  <script src="../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/UCity/UCity-Light.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/UCity/UCity-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/UCity/UCity-Semibold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/Inconsolata/Inconsolata.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <script defer src="https://use.fontawesome.com/releases/v6.1.1/js/all.js" integrity="sha384-xBXmu0dk1bEoiwd71wOonQLyH+VpgR1XcDH3rtxrLww5ajNTuMvBdL5SOiFZnNdp" crossorigin="anonymous"></script>
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch-lightning.rtfd.io/en/latest/" aria-label="PyTorch Lightning">
      <!--  <img class="call-to-action-img" src="../_static/images/logo-lightning-icon.png"/> -->
      </a>

      <div class="main-menu">
        <ul>
          <!-- <li>
            <a href="https://pytorch-lightning.readthedocs.io/en/latest/starter/introduction.html">Get Started</a>
          </li> -->

          <!-- <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch-lightning.readthedocs.io/en/stable/">
                  <span class="dropdown-title">PyTorch Lightning</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://torchmetrics.readthedocs.io/en/stable/">
                  <span class="dropdown-title">TorchMetrics</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning-flash.readthedocs.io/en/stable/">
                  <span class="dropdown-title">Lightning Flash</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning-transformers.readthedocs.io/en/stable/">
                  <span class="dropdown-title">Lightning Transformers</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning-bolts.readthedocs.io/en/stable/">
                  <span class="dropdown-title">Lightning Bolts</span>
                </a>
            </div>
          </li> -->

          <!--<li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://www.pytorchlightning.ai/community">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch-lightning.readthedocs.io/en/latest/#community-examples">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://github.com/PyTorchLightning/pytorch-lightning/discussions" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>-->

          <!-- <li>
            <a href="https://github.com/PyTorchLightning/pytorch-lightning">GitHub</a>
          </li>

          <li>
            <a href="https://www.grid.ai/">Train on the cloud</a>
          </li> -->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  1.7.0dev
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../starter/introduction.html">Lightning in 15 minutes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../starter/converting.html">Organize existing PyTorch into Lightning</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Level Up</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../levels/core_skills.html">Basic skills</a></li>
<li class="toctree-l1"><a class="reference internal" href="../levels/intermediate.html">Intermediate skills</a></li>
<li class="toctree-l1"><a class="reference internal" href="../levels/advanced.html">Advanced skills</a></li>
<li class="toctree-l1"><a class="reference internal" href="../levels/expert.html">Expert skills</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Core API</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">LightningModule</a></li>
<li class="toctree-l1"><a class="reference internal" href="trainer.html">Trainer</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Common Workflows</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="evaluation.html">Avoid overfitting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model/build_model.html">Build a Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="hyperparameters.html">Configure hyperparameters from the CLI</a></li>
<li class="toctree-l1"><a class="reference internal" href="progress_bar.html">Customize the progress bar</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/production.html">Deploy models into production</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/training_tricks.html">Effective Training Techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cli/lightning_cli.html">Eliminate config boilerplate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tuning/profiler.html">Find bottlenecks in your code</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/transfer_learning.html">Finetune a model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../visualize/logging_intermediate.html">Manage experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../clouds/cluster.html">Run on an on-prem cluster</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/model_parallel.html">Train 1 trillion+ parameter models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../clouds/cloud_training.html">Train on the cloud</a></li>
<li class="toctree-l1"><a class="reference internal" href="checkpointing.html">Save and load model progress</a></li>
<li class="toctree-l1"><a class="reference internal" href="precision.html">Save memory with half-precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accelerators/gpu.html">Train on single or multiple GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accelerators/hpu.html">Train on single or multiple HPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accelerators/ipu.html">Train on single or multiple IPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accelerators/tpu.html">Train on single or multiple TPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model/own_your_loop.html">Use a pure PyTorch training loop</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Glossary</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../extensions/accelerator.html">Accelerators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../extensions/callbacks.html">Callback</a></li>
<li class="toctree-l1"><a class="reference internal" href="checkpointing.html">Checkpointing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../clouds/cluster.html">Cluster</a></li>
<li class="toctree-l1"><a class="reference internal" href="checkpointing_advanced.html">Cloud checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="console_logs.html">Console Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug/debugging.html">Debugging</a></li>
<li class="toctree-l1"><a class="reference internal" href="early_stopping.html">Early stopping</a></li>
<li class="toctree-l1"><a class="reference internal" href="../visualize/experiment_managers.html">Experiment manager (Logger)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../clouds/fault_tolerant_training.html">Fault tolerant training</a></li>
<li class="toctree-l1"><a class="reference external" href="https://lightning-flash.readthedocs.io/en/stable/">Flash</a></li>
<li class="toctree-l1"><a class="reference internal" href="../clouds/cloud_training.html">Grid AI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accelerators/gpu.html">GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="precision.html">Half precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accelerators/hpu.html">HPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/production_intermediate.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accelerators/ipu.html">IPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cli/lightning_cli.html">Lightning CLI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model/build_model_expert.html">Raw PyTorch loop (expert)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model/build_model_expert.html#lightninglite-stepping-stone-to-lightning">LightningLite (Stepping Stone to Lightning)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data/datamodule.html">LightningDataModule</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">LightningModule</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch-lightning.readthedocs.io/en/stable/ecosystem/transformers.html">Lightning Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../visualize/loggers.html">Log</a></li>
<li class="toctree-l1"><a class="reference internal" href="../extensions/loops.html">Loops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accelerators/tpu.html">TPU</a></li>
<li class="toctree-l1"><a class="reference external" href="https://torchmetrics.readthedocs.io/en/stable/">Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model/build_model.html">Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/model_parallel.html">Model Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../extensions/plugins.html">Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="progress_bar.html">Progress bar</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/production_advanced.html">Production</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/production_basic.html">Predict</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tuning/profiler.html">Profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/pruning_quantization.html">Pruning and Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="remote_fs.html">Remote filesystem and FSSPEC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/strategy_registry.html">Strategy registry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../starter/style_guide.html">Style guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../clouds/run_intermediate.html">Sweep</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/training_tricks.html">SWA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../clouds/cluster_advanced.html">SLURM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/transfer_learning.html">Transfer learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="trainer.html">Trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../clouds/cluster_intermediate_2.html">Torch distributed</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Hands-on Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.youtube.com/playlist?list=PLaMu-SDt_RB5NUm67hU2pdE75j6KaIOv2">PyTorch Lightning 101 class</a></li>
<li class="toctree-l1"><a class="reference external" href="https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09">From PyTorch to PyTorch Lightning [Blog]</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.youtube.com/watch?v=QHww1JH7IDU">From PyTorch to PyTorch Lightning [Video]</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>LightningModule</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/common/lightning_module.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <section id="lightningmodule">
<span id="lightning-module"></span><h1>LightningModule<a class="headerlink" href="#lightningmodule" title="Permalink to this headline">¶</a></h1>
<p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code> organizes your PyTorch code into 6 sections:</p>
<ul class="simple">
<li><p>Computations (init).</p></li>
<li><p>Train Loop (training_step)</p></li>
<li><p>Validation Loop (validation_step)</p></li>
<li><p>Test Loop (test_step)</p></li>
<li><p>Prediction Loop (predict_step)</p></li>
<li><p>Optimizers and LR Schedulers (configure_optimizers)</p></li>
</ul>
<div class="line-block">
<div class="line"><br /></div>
</div>
<video width="100%" max-width="400px" controls autoplay muted playsinline src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/pl_mod_vid.m4v"></video><div class="line-block">
<div class="line"><br /></div>
</div>
<p>Notice a few things.</p>
<ol class="arabic simple">
<li><p>It is the SAME code.</p></li>
<li><p>The PyTorch code IS NOT abstracted - just organized.</p></li>
<li><p>All the other code that’s not in the <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code>
has been automated for you by the Trainer.</p></li>
</ol>
<div class="line-block">
<div class="line"><br /></div>
</div>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">()</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<ol class="arabic simple" start="4">
<li><p>There are no <code class="docutils literal notranslate"><span class="pre">.cuda()</span></code> or <code class="docutils literal notranslate"><span class="pre">.to(device)</span></code> calls required. Lightning does these for you.</p></li>
</ol>
<div class="line-block">
<div class="line"><br /></div>
</div>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># don&#39;t do in Lightning</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># do this instead</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span>  <span class="c1"># leave it alone!</span>

<span class="c1"># or to init a new tensor</span>
<span class="n">new_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">new_x</span> <span class="o">=</span> <span class="n">new_x</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<ol class="arabic simple" start="5">
<li><p>When running under a distributed strategy, Lightning handles the distributed sampler for you by default.</p></li>
</ol>
<div class="line-block">
<div class="line"><br /></div>
</div>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Don&#39;t do in Lightning...</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">sampler</span> <span class="o">=</span> <span class="n">DistributedSampler</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">DataLoader</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">sampler</span><span class="p">)</span>

<span class="c1"># do this instead</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">DataLoader</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<ol class="arabic simple" start="6">
<li><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code> is a <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code> but with added functionality. Use it as such!</p></li>
</ol>
<div class="line-block">
<div class="line"><br /></div>
</div>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span><span class="n">PATH</span><span class="p">)</span>
<span class="n">net</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<p>Thus, to use Lightning, you just need to organize your code which takes about 30 minutes,
(and let’s be real, you probably should do anyway).</p>
<hr class="docutils" />
<section id="starter-example">
<h2>Starter Example<a class="headerlink" href="#starter-example" title="Permalink to this headline">¶</a></h2>
<p>Here are the only required methods.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pytorch_lightning</span> <span class="k">as</span> <span class="nn">pl</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>


<span class="k">class</span> <span class="nc">LitModel</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
</pre></div>
</div>
<p>Which you can train by doing:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">MNIST</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">(),</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()))</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span><span class="n">max_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LitModel</span><span class="p">()</span>

<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_dataloaders</span><span class="o">=</span><span class="n">train_loader</span><span class="p">)</span>
</pre></div>
</div>
<p>The LightningModule has many convenience methods, but the core ones you need to know about are:</p>
<table class="colwidths-given docutils align-default">
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>init</p></td>
<td><p>Define computations here</p></td>
</tr>
<tr class="row-odd"><td><p>forward</p></td>
<td><p>Use for inference only (separate from training_step)</p></td>
</tr>
<tr class="row-even"><td><p>training_step</p></td>
<td><p>the complete training loop</p></td>
</tr>
<tr class="row-odd"><td><p>validation_step</p></td>
<td><p>the complete validation loop</p></td>
</tr>
<tr class="row-even"><td><p>test_step</p></td>
<td><p>the complete test loop</p></td>
</tr>
<tr class="row-odd"><td><p>predict_step</p></td>
<td><p>the complete prediction loop</p></td>
</tr>
<tr class="row-even"><td><p>configure_optimizers</p></td>
<td><p>define optimizers and LR schedulers</p></td>
</tr>
</tbody>
</table>
</section>
<hr class="docutils" />
<section id="training">
<h2>Training<a class="headerlink" href="#training" title="Permalink to this headline">¶</a></h2>
<section id="training-loop">
<h3>Training Loop<a class="headerlink" href="#training-loop" title="Permalink to this headline">¶</a></h3>
<p>To activate the training loop, override the <code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code> method.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LitClassifier</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>

    <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<p>Under the hood, Lightning does the following (pseudocode):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># put model in train mode and enable gradient calculation</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="n">outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">training_step</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>
    <span class="n">outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>

    <span class="c1"># clear gradients</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># backward</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># update parameters</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="train-epoch-level-metrics">
<h3>Train Epoch-level Metrics<a class="headerlink" href="#train-epoch-level-metrics" title="Permalink to this headline">¶</a></h3>
<p>If you want to calculate epoch-level metrics and log them, use <code class="xref py py-meth docutils literal notranslate"><span class="pre">log()</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># logs metrics for each training_step,</span>
    <span class="c1"># and the average across the epoch, to the progress bar and logger</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;train_loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">on_step</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">on_epoch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">prog_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">logger</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<p>The <code class="xref py py-meth docutils literal notranslate"><span class="pre">log()</span></code> object automatically reduces the
requested metrics across a complete epoch and devices. Here’s the pseudocode of what it does under the hood:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">):</span>
    <span class="c1"># forward</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">training_step</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>
    <span class="n">outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

    <span class="c1"># clear gradients</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># backward</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># update parameters</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="n">epoch_metric</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">outs</span><span class="p">]))</span>
</pre></div>
</div>
</section>
<section id="train-epoch-level-operations">
<h3>Train Epoch-level Operations<a class="headerlink" href="#train-epoch-level-operations" title="Permalink to this headline">¶</a></h3>
<p>If you need to do something with all the outputs of each <code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code>,
override the <code class="xref py py-meth docutils literal notranslate"><span class="pre">training_epoch_end()</span></code> method.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="o">...</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;loss&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s2">&quot;other_stuff&quot;</span><span class="p">:</span> <span class="n">preds</span><span class="p">}</span>


<span class="k">def</span> <span class="nf">training_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training_step_outputs</span><span class="p">):</span>
    <span class="n">all_preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">training_step_outputs</span><span class="p">)</span>
    <span class="o">...</span>
</pre></div>
</div>
<p>The matching pseudocode is:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">):</span>
    <span class="c1"># forward</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">training_step</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>
    <span class="n">outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

    <span class="c1"># clear gradients</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># backward</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># update parameters</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="n">training_epoch_end</span><span class="p">(</span><span class="n">outs</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="training-with-dataparallel">
<h3>Training with DataParallel<a class="headerlink" href="#training-with-dataparallel" title="Permalink to this headline">¶</a></h3>
<p>When training using a <code class="docutils literal notranslate"><span class="pre">strategy</span></code> that splits data from each batch across GPUs, sometimes you might
need to aggregate them on the main GPU for processing (DP, or DDP2).</p>
<p>In this case, implement the <code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step_end()</span></code>
method which will have outputs from all the devices and you can accumulate to get the effective results.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="o">...</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;loss&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s2">&quot;pred&quot;</span><span class="p">:</span> <span class="n">pred</span><span class="p">}</span>


<span class="k">def</span> <span class="nf">training_step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_parts</span><span class="p">):</span>
    <span class="c1"># predictions from each GPU</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">batch_parts</span><span class="p">[</span><span class="s2">&quot;pred&quot;</span><span class="p">]</span>
    <span class="c1"># losses from each GPU</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="n">batch_parts</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">]</span>

    <span class="n">gpu_0_prediction</span> <span class="o">=</span> <span class="n">predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">gpu_1_prediction</span> <span class="o">=</span> <span class="n">predictions</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># do something with both outputs</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">losses</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">losses</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">/</span> <span class="mi">2</span>


<span class="k">def</span> <span class="nf">training_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training_step_outputs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">training_step_outputs</span><span class="p">:</span>
        <span class="o">...</span>
</pre></div>
</div>
<p>Here is the Lightning training pseudo-code for DP:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">train_batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">):</span>
    <span class="n">batches</span> <span class="o">=</span> <span class="n">split_batch</span><span class="p">(</span><span class="n">train_batch</span><span class="p">)</span>
    <span class="n">dp_outs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">sub_batch</span> <span class="ow">in</span> <span class="n">batches</span><span class="p">:</span>
        <span class="c1"># 1</span>
        <span class="n">dp_out</span> <span class="o">=</span> <span class="n">training_step</span><span class="p">(</span><span class="n">sub_batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>
        <span class="n">dp_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dp_out</span><span class="p">)</span>

    <span class="c1"># 2</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">training_step_end</span><span class="p">(</span><span class="n">dp_outs</span><span class="p">)</span>
    <span class="n">outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

<span class="c1"># do something with the outputs for all batches</span>
<span class="c1"># 3</span>
<span class="n">training_epoch_end</span><span class="p">(</span><span class="n">outs</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="validation">
<h2>Validation<a class="headerlink" href="#validation" title="Permalink to this headline">¶</a></h2>
<section id="validation-loop">
<h3>Validation Loop<a class="headerlink" href="#validation-loop" title="Permalink to this headline">¶</a></h3>
<p>To activate the validation loop while training, override the <code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code> method.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LitModel</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;val_loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
<p>Under the hood, Lightning does the following (pseudocode):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># ...</span>
<span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">training_step</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="c1"># ...</span>

    <span class="k">if</span> <span class="n">validate_at_some_point</span><span class="p">:</span>
        <span class="c1"># disable grads + batchnorm + dropout</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

        <span class="c1"># ----------------- VAL LOOP ---------------</span>
        <span class="k">for</span> <span class="n">val_batch_idx</span><span class="p">,</span> <span class="n">val_batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">val_dataloader</span><span class="p">):</span>
            <span class="n">val_out</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">validation_step</span><span class="p">(</span><span class="n">val_batch</span><span class="p">,</span> <span class="n">val_batch_idx</span><span class="p">)</span>
        <span class="c1"># ----------------- VAL LOOP ---------------</span>

        <span class="c1"># enable grads + batchnorm + dropout</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
<p>You can also run just the validation loop on your validation dataloaders by overriding <code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code>
and calling <code class="xref py py-meth docutils literal notranslate"><span class="pre">validate()</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">()</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">validate</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is recommended to validate on single device to ensure each sample/batch gets evaluated exactly once.
This is helpful to make sure benchmarking for research papers is done the right way. Otherwise, in a
multi-device setting, samples could occur duplicated when <code class="xref py py-class docutils literal notranslate"><span class="pre">DistributedSampler</span></code>
is used, for eg. with <code class="docutils literal notranslate"><span class="pre">strategy=&quot;ddp&quot;</span></code>. It replicates some samples on some devices to make sure all devices have
same batch size in case of uneven inputs.</p>
</div>
</section>
<section id="validation-epoch-level-metrics">
<h3>Validation Epoch-level Metrics<a class="headerlink" href="#validation-epoch-level-metrics" title="Permalink to this headline">¶</a></h3>
<p>If you need to do something with all the outputs of each <code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code>,
override the <code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_epoch_end()</span></code> method. Note that this method is called before <code class="xref py py-meth docutils literal notranslate"><span class="pre">training_epoch_end()</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="o">...</span>
    <span class="k">return</span> <span class="n">pred</span>


<span class="k">def</span> <span class="nf">validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">validation_step_outputs</span><span class="p">):</span>
    <span class="n">all_preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">validation_step_outputs</span><span class="p">)</span>
    <span class="o">...</span>
</pre></div>
</div>
</section>
<section id="validating-with-dataparallel">
<h3>Validating with DataParallel<a class="headerlink" href="#validating-with-dataparallel" title="Permalink to this headline">¶</a></h3>
<p>When training using a <code class="docutils literal notranslate"><span class="pre">strategy</span></code> that splits data from each batch across GPUs, sometimes you might
need to aggregate them on the main GPU for processing (DP, or DDP2).</p>
<p>In this case, implement the <code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step_end()</span></code>
method which will have outputs from all the devices and you can accumulate to get the effective results.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="o">...</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;loss&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s2">&quot;pred&quot;</span><span class="p">:</span> <span class="n">pred</span><span class="p">}</span>


<span class="k">def</span> <span class="nf">validation_step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_parts</span><span class="p">):</span>
    <span class="c1"># predictions from each GPU</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">batch_parts</span><span class="p">[</span><span class="s2">&quot;pred&quot;</span><span class="p">]</span>
    <span class="c1"># losses from each GPU</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="n">batch_parts</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">]</span>

    <span class="n">gpu_0_prediction</span> <span class="o">=</span> <span class="n">predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">gpu_1_prediction</span> <span class="o">=</span> <span class="n">predictions</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># do something with both outputs</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">losses</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">losses</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">/</span> <span class="mi">2</span>


<span class="k">def</span> <span class="nf">validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">validation_step_outputs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">validation_step_outputs</span><span class="p">:</span>
        <span class="o">...</span>
</pre></div>
</div>
<p>Here is the Lightning validation pseudo-code for DP:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
    <span class="n">batches</span> <span class="o">=</span> <span class="n">split_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
    <span class="n">dp_outs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">sub_batch</span> <span class="ow">in</span> <span class="n">batches</span><span class="p">:</span>
        <span class="c1"># 1</span>
        <span class="n">dp_out</span> <span class="o">=</span> <span class="n">validation_step</span><span class="p">(</span><span class="n">sub_batch</span><span class="p">)</span>
        <span class="n">dp_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dp_out</span><span class="p">)</span>

    <span class="c1"># 2</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">validation_step_end</span><span class="p">(</span><span class="n">dp_outs</span><span class="p">)</span>
    <span class="n">outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

<span class="c1"># do something with the outputs for all batches</span>
<span class="c1"># 3</span>
<span class="n">validation_epoch_end</span><span class="p">(</span><span class="n">outs</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="testing">
<h2>Testing<a class="headerlink" href="#testing" title="Permalink to this headline">¶</a></h2>
<section id="test-loop">
<h3>Test Loop<a class="headerlink" href="#test-loop" title="Permalink to this headline">¶</a></h3>
<p>The process for enabling a test loop is the same as the process for enabling a validation loop. Please refer to
the section above for details. For this you need to override the <code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code> method.</p>
<p>The only difference is that the test loop is only called when <code class="xref py py-meth docutils literal notranslate"><span class="pre">test()</span></code> is used.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">()</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># automatically loads the best weights for you</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<p>There are two ways to call <code class="docutils literal notranslate"><span class="pre">test()</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># call after training</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">()</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># automatically auto-loads the best weights from the previous run</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">dataloaders</span><span class="o">=</span><span class="n">test_dataloader</span><span class="p">)</span>

<span class="c1"># or call with pretrained model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span><span class="n">PATH</span><span class="p">)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">()</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloaders</span><span class="o">=</span><span class="n">test_dataloader</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is recommended to validate on single device to ensure each sample/batch gets evaluated exactly once.
This is helpful to make sure benchmarking for research papers is done the right way. Otherwise, in a
multi-device setting, samples could occur duplicated when <code class="xref py py-class docutils literal notranslate"><span class="pre">DistributedSampler</span></code>
is used, for eg. with <code class="docutils literal notranslate"><span class="pre">strategy=&quot;ddp&quot;</span></code>. It replicates some samples on some devices to make sure all devices have
same batch size in case of uneven inputs.</p>
</div>
</section>
</section>
<hr class="docutils" />
<section id="inference">
<h2>Inference<a class="headerlink" href="#inference" title="Permalink to this headline">¶</a></h2>
<section id="prediction-loop">
<h3>Prediction Loop<a class="headerlink" href="#prediction-loop" title="Permalink to this headline">¶</a></h3>
<p>By default, the <code class="xref py py-meth docutils literal notranslate"><span class="pre">predict_step()</span></code> method runs the
<code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code> method. In order to customize this behaviour,
simply override the <code class="xref py py-meth docutils literal notranslate"><span class="pre">predict_step()</span></code> method.</p>
<p>For the example let’s override <code class="docutils literal notranslate"><span class="pre">predict_step</span></code> and try out <a class="reference external" href="https://arxiv.org/pdf/1506.02142.pdf">Monte Carlo Dropout</a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LitMCdropoutModel</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">mc_iteration</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mc_iteration</span> <span class="o">=</span> <span class="n">mc_iteration</span>

    <span class="k">def</span> <span class="nf">predict_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="c1"># enable Monte Carlo Dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

        <span class="c1"># take average of `self.mc_iteration` iterations</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mc_iteration</span><span class="p">)])</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pred</span>
</pre></div>
</div>
<p>Under the hood, Lightning does the following (pseudocode):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># disable grads + batchnorm + dropout</span>
<span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">all_preds</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">predict_dataloader</span><span class="p">):</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_step</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>
    <span class="n">all_preds</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span>
</pre></div>
</div>
<p>There are two ways to call <code class="docutils literal notranslate"><span class="pre">predict()</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># call after training</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">()</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># automatically auto-loads the best weights from the previous run</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">dataloaders</span><span class="o">=</span><span class="n">predict_dataloader</span><span class="p">)</span>

<span class="c1"># or call with pretrained model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span><span class="n">PATH</span><span class="p">)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">()</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloaders</span><span class="o">=</span><span class="n">test_dataloader</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="inference-in-research">
<h3>Inference in Research<a class="headerlink" href="#inference-in-research" title="Permalink to this headline">¶</a></h3>
<p>If you want to perform inference with the system, you can add a <code class="docutils literal notranslate"><span class="pre">forward</span></code> method to the LightningModule.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When using forward, you are responsible to call <code class="xref py py-func docutils literal notranslate"><span class="pre">eval()</span></code> and use the <code class="xref py py-func docutils literal notranslate"><span class="pre">no_grad()</span></code> context manager.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Autoencoder</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">Autoencoder</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">reconstruction</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span>
</pre></div>
</div>
<p>The advantage of adding a forward is that in complex systems, you can do a much more involved inference procedure,
such as text generation:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Seq2Seq</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">hidden_states</span><span class="p">:</span>
            <span class="c1"># decode</span>
            <span class="o">...</span>
        <span class="k">return</span> <span class="n">decoded</span>
</pre></div>
</div>
<p>In the case where you want to scale your inference, you should be using
<code class="xref py py-meth docutils literal notranslate"><span class="pre">predict_step()</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Autoencoder</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="c1"># this calls forward</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>


<span class="n">data_module</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Autoencoder</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">data_module</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="inference-in-production">
<h3>Inference in Production<a class="headerlink" href="#inference-in-production" title="Permalink to this headline">¶</a></h3>
<p>For cases like production, you might want to iterate different models inside a LightningModule.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchmetrics.functional</span> <span class="kn">import</span> <span class="n">accuracy</span>


<span class="k">class</span> <span class="nc">ClassificationTask</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>

    <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="n">loss</span><span class="p">,</span> <span class="n">acc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shared_eval_step</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>
        <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;val_acc&quot;</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span> <span class="s2">&quot;val_loss&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">metrics</span>

    <span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="n">loss</span><span class="p">,</span> <span class="n">acc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shared_eval_step</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>
        <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;test_acc&quot;</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span> <span class="s2">&quot;test_loss&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">metrics</span>

    <span class="k">def</span> <span class="nf">_shared_eval_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">acc</span> <span class="o">=</span> <span class="n">accuracy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">acc</span>

    <span class="k">def</span> <span class="nf">predict_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y_hat</span>

    <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
</pre></div>
</div>
<p>Then pass in any arbitrary model to be fit with this task</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="p">[</span><span class="n">resnet50</span><span class="p">(),</span> <span class="n">vgg16</span><span class="p">(),</span> <span class="n">BidirectionalRNN</span><span class="p">()]:</span>
    <span class="n">task</span> <span class="o">=</span> <span class="n">ClassificationTask</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

    <span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">task</span><span class="p">,</span> <span class="n">train_dataloaders</span><span class="o">=</span><span class="n">train_dataloader</span><span class="p">,</span> <span class="n">val_dataloaders</span><span class="o">=</span><span class="n">val_dataloader</span><span class="p">)</span>
</pre></div>
</div>
<p>Tasks can be arbitrarily complex such as implementing GAN training, self-supervised or even RL.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GANTask</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">generator</span><span class="p">,</span> <span class="n">discriminator</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">generator</span> <span class="o">=</span> <span class="n">generator</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">discriminator</span> <span class="o">=</span> <span class="n">discriminator</span>

    <span class="o">...</span>
</pre></div>
</div>
<p>When used like this, the model can be separated from the Task and thus used in production without needing to keep it in
a <code class="docutils literal notranslate"><span class="pre">LightningModule</span></code>.</p>
<p>The following example shows how you can run inference in the Python runtime:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">task</span> <span class="o">=</span> <span class="n">ClassificationTask</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">task</span><span class="p">,</span> <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">val_dataloader</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">(</span><span class="s2">&quot;best_model.ckpt&quot;</span><span class="p">)</span>

<span class="c1"># use model after training or load weights and drop into the production system</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ClassificationTask</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span><span class="s2">&quot;best_model.ckpt&quot;</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>Check out <a class="reference internal" href="../deploy/production.html#production-inference"><span class="std std-ref">Inference in Production</span></a> guide to learn about the possible ways to perform inference in production.</p>
</section>
</section>
<hr class="docutils" />
<section id="child-modules">
<h2>Child Modules<a class="headerlink" href="#child-modules" title="Permalink to this headline">¶</a></h2>
<p>Research projects tend to test different approaches to the same dataset.
This is very easy to do in Lightning with inheritance.</p>
<p>For example, imagine we now want to train an <code class="docutils literal notranslate"><span class="pre">AutoEncoder</span></code> to use as a feature extractor for images.
The only things that change in the <code class="docutils literal notranslate"><span class="pre">LitAutoEncoder</span></code> model are the init, forward, training, validation and test step.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="o">...</span>


<span class="k">class</span> <span class="nc">Decoder</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="o">...</span>


<span class="k">class</span> <span class="nc">AutoEncoder</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">Decoder</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>


<span class="k">class</span> <span class="nc">LitAutoEncoder</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">auto_encoder</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">auto_encoder</span> <span class="o">=</span> <span class="n">auto_encoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metric</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">auto_encoder</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="n">x_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">auto_encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">metric</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_hat</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_shared_eval</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="s2">&quot;val&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_shared_eval</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="s2">&quot;test&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_shared_eval</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">prefix</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="n">x_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">auto_encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">metric</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_hat</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">_loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
<p>and we can train this using the <code class="docutils literal notranslate"><span class="pre">Trainer</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">auto_encoder</span> <span class="o">=</span> <span class="n">AutoEncoder</span><span class="p">()</span>
<span class="n">lightning_module</span> <span class="o">=</span> <span class="n">LitAutoEncoder</span><span class="p">(</span><span class="n">auto_encoder</span><span class="p">)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">()</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">lightning_module</span><span class="p">,</span> <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">val_dataloader</span><span class="p">)</span>
</pre></div>
</div>
<p>And remember that the forward method should define the practical use of a <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code>.
In this case, we want to use the <code class="docutils literal notranslate"><span class="pre">LitAutoEncoder</span></code> to extract image representations:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">some_images</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
<span class="n">representations</span> <span class="o">=</span> <span class="n">lightning_module</span><span class="p">(</span><span class="n">some_images</span><span class="p">)</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="lightningmodule-api">
<h2>LightningModule API<a class="headerlink" href="#lightningmodule-api" title="Permalink to this headline">¶</a></h2>
<section id="methods">
<h3>Methods<a class="headerlink" href="#methods" title="Permalink to this headline">¶</a></h3>
<section id="all-gather">
<h4>all_gather<a class="headerlink" href="#all-gather" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">all_gather</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_grads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_lightning/core/lightning.html#LightningModule.all_gather"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Allows users to call <code class="docutils literal notranslate"><span class="pre">self.all_gather()</span></code> from the LightningModule, thus making the <code class="docutils literal notranslate"><span class="pre">all_gather</span></code> operation
accelerator agnostic. <code class="docutils literal notranslate"><span class="pre">all_gather</span></code> is a function provided by accelerators to gather a tensor from several
distributed processes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.all_gather.params.data"></span><strong>data</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.all_gather.params.data">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>]) – int, float, tensor of shape (batch, …), or a (possibly nested) collection thereof.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.all_gather.params.group"></span><strong>group</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.all_gather.params.group">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – the process group to gather results from. Defaults to all processes (world)</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.all_gather.params.sync_grads"></span><strong>sync_grads</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.all_gather.params.sync_grads">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – flag that allows users to synchronize gradients for the all_gather operation</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor of shape (world_size, batch, …), or if the input was a collection
the output will also be a collection with tensors of this shape.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="configure-callbacks">
<h4>configure_callbacks<a class="headerlink" href="#configure-callbacks" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">configure_callbacks</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_lightning/core/lightning.html#LightningModule.configure_callbacks"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Configure model-specific callbacks. When the model gets attached, e.g., when <code class="docutils literal notranslate"><span class="pre">.fit()</span></code> or <code class="docutils literal notranslate"><span class="pre">.test()</span></code>
gets called, the list or a callback returned here will be merged with the list of callbacks passed to the
Trainer’s <code class="docutils literal notranslate"><span class="pre">callbacks</span></code> argument. If a callback returned here has the same type as one or several callbacks
already present in the Trainer’s callbacks list, it will take priority and replace them. In addition,
Lightning will make sure <code class="xref py py-class docutils literal notranslate"><span class="pre">ModelCheckpoint</span></code> callbacks
run last.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Callback</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Callback</span></code>]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A callback or a list of callbacks which will extend the list of callbacks in the Trainer.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">configure_callbacks</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">early_stop</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s2">&quot;val_acc&quot;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;max&quot;</span><span class="p">)</span>
    <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s2">&quot;val_loss&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">early_stop</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Certain callback methods like <code class="xref py py-meth docutils literal notranslate"><span class="pre">on_init_start()</span></code>
will never be invoked on the new callbacks returned here.</p>
</div>
</dd></dl>

</section>
<section id="configure-optimizers">
<h4>configure_optimizers<a class="headerlink" href="#configure-optimizers" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">configure_optimizers</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_lightning/core/lightning.html#LightningModule.configure_optimizers"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Choose what optimizers and learning-rate schedulers to use in your optimization.
Normally you’d need one. But in the case of GANs or similar you might have multiple.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><p>Any of these 6 options.</p>
<ul class="simple">
<li><p><strong>Single optimizer</strong>.</p></li>
<li><p><strong>List or Tuple</strong> of optimizers.</p></li>
<li><p><strong>Two lists</strong> - The first list has multiple optimizers, and the second has multiple LR schedulers
(or multiple <code class="docutils literal notranslate"><span class="pre">lr_scheduler_config</span></code>).</p></li>
<li><p><strong>Dictionary</strong>, with an <code class="docutils literal notranslate"><span class="pre">&quot;optimizer&quot;</span></code> key, and (optionally) a <code class="docutils literal notranslate"><span class="pre">&quot;lr_scheduler&quot;</span></code>
key whose value is a single LR scheduler or <code class="docutils literal notranslate"><span class="pre">lr_scheduler_config</span></code>.</p></li>
<li><p><strong>Tuple of dictionaries</strong> as described above, with an optional <code class="docutils literal notranslate"><span class="pre">&quot;frequency&quot;</span></code> key.</p></li>
<li><p><strong>None</strong> - Fit will run without any optimizer.</p></li>
</ul>
</p>
</dd>
</dl>
<p>The <code class="docutils literal notranslate"><span class="pre">lr_scheduler_config</span></code> is a dictionary which contains the scheduler and its associated configuration.
The default configuration is shown below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">lr_scheduler_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="c1"># REQUIRED: The scheduler instance</span>
    <span class="s2">&quot;scheduler&quot;</span><span class="p">:</span> <span class="n">lr_scheduler</span><span class="p">,</span>
    <span class="c1"># The unit of the scheduler&#39;s step size, could also be &#39;step&#39;.</span>
    <span class="c1"># &#39;epoch&#39; updates the scheduler on epoch end whereas &#39;step&#39;</span>
    <span class="c1"># updates it after a optimizer update.</span>
    <span class="s2">&quot;interval&quot;</span><span class="p">:</span> <span class="s2">&quot;epoch&quot;</span><span class="p">,</span>
    <span class="c1"># How many epochs/steps should pass between calls to</span>
    <span class="c1"># `scheduler.step()`. 1 corresponds to updating the learning</span>
    <span class="c1"># rate after every epoch/step.</span>
    <span class="s2">&quot;frequency&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="c1"># Metric to to monitor for schedulers like `ReduceLROnPlateau`</span>
    <span class="s2">&quot;monitor&quot;</span><span class="p">:</span> <span class="s2">&quot;val_loss&quot;</span><span class="p">,</span>
    <span class="c1"># If set to `True`, will enforce that the value specified &#39;monitor&#39;</span>
    <span class="c1"># is available when the scheduler is updated, thus stopping</span>
    <span class="c1"># training if not found. If set to `False`, it will only produce a warning</span>
    <span class="s2">&quot;strict&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="c1"># If using the `LearningRateMonitor` callback to monitor the</span>
    <span class="c1"># learning rate progress, this keyword can be used to specify</span>
    <span class="c1"># a custom logged name</span>
    <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>When there are schedulers in which the <code class="docutils literal notranslate"><span class="pre">.step()</span></code> method is conditioned on a value, such as the
<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.lr_scheduler.ReduceLROnPlateau</span></code> scheduler, Lightning requires that the
<code class="docutils literal notranslate"><span class="pre">lr_scheduler_config</span></code> contains the keyword <code class="docutils literal notranslate"><span class="pre">&quot;monitor&quot;</span></code> set to the metric name that the scheduler
should be conditioned on.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># The ReduceLROnPlateau scheduler requires a monitor</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;optimizer&quot;</span><span class="p">:</span> <span class="n">optimizer</span><span class="p">,</span>
        <span class="s2">&quot;lr_scheduler&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;scheduler&quot;</span><span class="p">:</span> <span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="o">...</span><span class="p">),</span>
            <span class="s2">&quot;monitor&quot;</span><span class="p">:</span> <span class="s2">&quot;metric_to_track&quot;</span><span class="p">,</span>
            <span class="s2">&quot;frequency&quot;</span><span class="p">:</span> <span class="s2">&quot;indicates how often the metric is updated&quot;</span>
            <span class="c1"># If &quot;monitor&quot; references validation metrics, then &quot;frequency&quot; should be set to a</span>
            <span class="c1"># multiple of &quot;trainer.check_val_every_n_epoch&quot;.</span>
        <span class="p">},</span>
    <span class="p">}</span>


<span class="c1"># In the case of two optimizers, only one using the ReduceLROnPlateau scheduler</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">optimizer1</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">optimizer2</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">scheduler1</span> <span class="o">=</span> <span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">optimizer1</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
    <span class="n">scheduler2</span> <span class="o">=</span> <span class="n">LambdaLR</span><span class="p">(</span><span class="n">optimizer2</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="p">{</span>
            <span class="s2">&quot;optimizer&quot;</span><span class="p">:</span> <span class="n">optimizer1</span><span class="p">,</span>
            <span class="s2">&quot;lr_scheduler&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;scheduler&quot;</span><span class="p">:</span> <span class="n">scheduler1</span><span class="p">,</span>
                <span class="s2">&quot;monitor&quot;</span><span class="p">:</span> <span class="s2">&quot;metric_to_track&quot;</span><span class="p">,</span>
            <span class="p">},</span>
        <span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;optimizer&quot;</span><span class="p">:</span> <span class="n">optimizer2</span><span class="p">,</span> <span class="s2">&quot;lr_scheduler&quot;</span><span class="p">:</span> <span class="n">scheduler2</span><span class="p">},</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>Metrics can be made available to monitor by simply logging it using
<code class="docutils literal notranslate"><span class="pre">self.log('metric_to_track',</span> <span class="pre">metric_val)</span></code> in your <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="docutils literal notranslate"><span class="pre">frequency</span></code> value specified in a dict along with the <code class="docutils literal notranslate"><span class="pre">optimizer</span></code> key is an int corresponding
to the number of sequential batches optimized with the specific optimizer.
It should be given to none or to all of the optimizers.
There is a difference between passing multiple optimizers in a list,
and passing multiple optimizers in dictionaries with a frequency of 1:</p>
<blockquote>
<div><ul class="simple">
<li><p>In the former case, all optimizers will operate on the given batch in each optimization step.</p></li>
<li><p>In the latter, only one optimizer will operate on the given batch at every step.</p></li>
</ul>
</div></blockquote>
<p>This is different from the <code class="docutils literal notranslate"><span class="pre">frequency</span></code> value specified in the <code class="docutils literal notranslate"><span class="pre">lr_scheduler_config</span></code> mentioned above.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">optimizer_one</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">optimizer_two</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;optimizer&quot;</span><span class="p">:</span> <span class="n">optimizer_one</span><span class="p">,</span> <span class="s2">&quot;frequency&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;optimizer&quot;</span><span class="p">:</span> <span class="n">optimizer_two</span><span class="p">,</span> <span class="s2">&quot;frequency&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">},</span>
    <span class="p">]</span>
</pre></div>
</div>
<p>In this example, the first optimizer will be used for the first 5 steps,
the second optimizer for the next 10 steps and that cycle will continue.
If an LR scheduler is specified for an optimizer using the <code class="docutils literal notranslate"><span class="pre">lr_scheduler</span></code> key in the above dict,
the scheduler will only be updated when its optimizer is being used.</p>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># most cases. no learning rate scheduler</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>

<span class="c1"># multiple optimizer case (e.g.: GAN)</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">gen_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">dis_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_dis</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gen_opt</span><span class="p">,</span> <span class="n">dis_opt</span>

<span class="c1"># example with learning rate schedulers</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">gen_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">dis_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_dis</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="n">dis_sch</span> <span class="o">=</span> <span class="n">CosineAnnealing</span><span class="p">(</span><span class="n">dis_opt</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">gen_opt</span><span class="p">,</span> <span class="n">dis_opt</span><span class="p">],</span> <span class="p">[</span><span class="n">dis_sch</span><span class="p">]</span>

<span class="c1"># example with step-based learning rate schedulers</span>
<span class="c1"># each optimizer has its own scheduler</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">gen_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">dis_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_dis</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="n">gen_sch</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;scheduler&#39;</span><span class="p">:</span> <span class="n">ExponentialLR</span><span class="p">(</span><span class="n">gen_opt</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">),</span>
        <span class="s1">&#39;interval&#39;</span><span class="p">:</span> <span class="s1">&#39;step&#39;</span>  <span class="c1"># called after each training step</span>
    <span class="p">}</span>
    <span class="n">dis_sch</span> <span class="o">=</span> <span class="n">CosineAnnealing</span><span class="p">(</span><span class="n">dis_opt</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> <span class="c1"># called every epoch</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">gen_opt</span><span class="p">,</span> <span class="n">dis_opt</span><span class="p">],</span> <span class="p">[</span><span class="n">gen_sch</span><span class="p">,</span> <span class="n">dis_sch</span><span class="p">]</span>

<span class="c1"># example with optimizer frequencies</span>
<span class="c1"># see training procedure in `Improved Training of Wasserstein GANs`, Algorithm 1</span>
<span class="c1"># https://arxiv.org/abs/1704.00028</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">gen_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">dis_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_dis</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="n">n_critic</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="p">{</span><span class="s1">&#39;optimizer&#39;</span><span class="p">:</span> <span class="n">dis_opt</span><span class="p">,</span> <span class="s1">&#39;frequency&#39;</span><span class="p">:</span> <span class="n">n_critic</span><span class="p">},</span>
        <span class="p">{</span><span class="s1">&#39;optimizer&#39;</span><span class="p">:</span> <span class="n">gen_opt</span><span class="p">,</span> <span class="s1">&#39;frequency&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
    <span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Some things to know:</p>
<ul class="simple">
<li><p>Lightning calls <code class="docutils literal notranslate"><span class="pre">.backward()</span></code> and <code class="docutils literal notranslate"><span class="pre">.step()</span></code> on each optimizer and learning rate scheduler as needed.</p></li>
<li><p>If you use 16-bit precision (<code class="docutils literal notranslate"><span class="pre">precision=16</span></code>), Lightning will automatically handle the optimizers.</p></li>
<li><p>If you use multiple optimizers, <code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code> will have an additional <code class="docutils literal notranslate"><span class="pre">optimizer_idx</span></code> parameter.</p></li>
<li><p>If you use <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.LBFGS</span></code>, Lightning handles the closure function automatically for you.</p></li>
<li><p>If you use multiple optimizers, gradients will be calculated only for the parameters of current optimizer
at each training step.</p></li>
<li><p>If you need to control how often those optimizers step or override the default <code class="docutils literal notranslate"><span class="pre">.step()</span></code> schedule,
override the <code class="xref py py-meth docutils literal notranslate"><span class="pre">optimizer_step()</span></code> hook.</p></li>
</ul>
</div>
</dd></dl>

</section>
<section id="forward">
<h4>forward<a class="headerlink" href="#forward" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_lightning/core/lightning.html#LightningModule.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Same as <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.forward()</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.forward.params.*args"></span><strong>*args</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.forward.params.*args">¶</a> – Whatever you decide to pass into the forward method.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.forward.params.**kwargs"></span><strong>**kwargs</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.forward.params.**kwargs">¶</a> – Keyword arguments are also possible.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Your model’s output</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="freeze">
<h4>freeze<a class="headerlink" href="#freeze" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">freeze</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_lightning/core/lightning.html#LightningModule.freeze"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Freeze all params for inference.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">MyLightningModule</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="log">
<span id="lm-log"></span><h4>log<a class="headerlink" href="#log" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">log</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prog_bar</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logger</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_step</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_epoch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce_fx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_graph</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist_group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_dataloader_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric_attribute</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rank_zero_only</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_lightning/core/lightning.html#LightningModule.log"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Log a key, value pair.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s1">&#39;train_loss&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
<p>The default behavior per hook is documented here: <a class="reference internal" href="../extensions/logging.html#automatic-logging"><span class="std std-ref">Automatic Logging</span></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.log.params.name"></span><strong>name</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.log.params.name">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – key to log.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.log.params.value"></span><strong>value</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.log.params.value">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Metric</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Mapping</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Metric</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>]]]) – value to log. Can be a <code class="docutils literal notranslate"><span class="pre">float</span></code>, <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="docutils literal notranslate"><span class="pre">Metric</span></code>, or a dictionary of the former.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.log.params.prog_bar"></span><strong>prog_bar</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.log.params.prog_bar">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code> logs to the progress bar.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.log.params.logger"></span><strong>logger</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.log.params.logger">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code> logs to the logger.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.log.params.on_step"></span><strong>on_step</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.log.params.on_step">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>]) – if <code class="docutils literal notranslate"><span class="pre">True</span></code> logs at this step. The default value is determined by the hook.
See <a class="reference internal" href="../extensions/logging.html#automatic-logging"><span class="std std-ref">Automatic Logging</span></a> for details.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.log.params.on_epoch"></span><strong>on_epoch</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.log.params.on_epoch">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>]) – if <code class="docutils literal notranslate"><span class="pre">True</span></code> logs epoch accumulated metrics. The default value is determined by the hook.
See <a class="reference internal" href="../extensions/logging.html#automatic-logging"><span class="std std-ref">Automatic Logging</span></a> for details.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.log.params.reduce_fx"></span><strong>reduce_fx</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.log.params.reduce_fx">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>]) – reduction function over step values for end of epoch. <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.mean()</span></code> by default.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.log.params.enable_graph"></span><strong>enable_graph</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.log.params.enable_graph">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, will not auto detach the graph.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.log.params.sync_dist"></span><strong>sync_dist</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.log.params.sync_dist">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, reduces the metric across devices. Use with care as this may lead to a significant
communication overhead.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.log.params.sync_dist_group"></span><strong>sync_dist_group</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.log.params.sync_dist_group">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – the DDP group to sync across.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.log.params.add_dataloader_idx"></span><strong>add_dataloader_idx</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.log.params.add_dataloader_idx">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, appends the index of the current dataloader to
the name (when using multiple dataloaders). If False, user needs to give unique names for
each dataloader to not mix the values.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.log.params.batch_size"></span><strong>batch_size</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.log.params.batch_size">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – Current batch_size. This will be directly inferred from the loaded batch,
but for some data structures you might need to explicitly provide it.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.log.params.metric_attribute"></span><strong>metric_attribute</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.log.params.metric_attribute">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – To restore the metric state, Lightning requires the reference of the
<code class="xref py py-class docutils literal notranslate"><span class="pre">torchmetrics.Metric</span></code> in your model. This is found automatically if it is a model attribute.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.log.params.rank_zero_only"></span><strong>rank_zero_only</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.log.params.rank_zero_only">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – Whether the value will be logged only on rank 0. This will prevent synchronization which
would produce a deadlock as not all processes would perform this log call.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="log-dict">
<h4>log_dict<a class="headerlink" href="#log-dict" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">log_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dictionary</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prog_bar</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logger</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_step</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_epoch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce_fx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_graph</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist_group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_dataloader_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rank_zero_only</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_lightning/core/lightning.html#LightningModule.log_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Log a dictionary of values at once.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">values</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;acc&#39;</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="s1">&#39;metric_n&#39;</span><span class="p">:</span> <span class="n">metric_n</span><span class="p">}</span>
<span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.log_dict.params.dictionary"></span><strong>dictionary</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.log_dict.params.dictionary">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Mapping</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Metric</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Mapping</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Metric</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>]]]]) – key value pairs.
The values can be a <code class="docutils literal notranslate"><span class="pre">float</span></code>, <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="docutils literal notranslate"><span class="pre">Metric</span></code>, or a dictionary of the former.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.log_dict.params.prog_bar"></span><strong>prog_bar</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.log_dict.params.prog_bar">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code> logs to the progress base.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.log_dict.params.logger"></span><strong>logger</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.log_dict.params.logger">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code> logs to the logger.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.log_dict.params.on_step"></span><strong>on_step</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.log_dict.params.on_step">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>]) – if <code class="docutils literal notranslate"><span class="pre">True</span></code> logs at this step.
<code class="docutils literal notranslate"><span class="pre">None</span></code> auto-logs for training_step but not validation/test_step.
The default value is determined by the hook.
See <a class="reference internal" href="../extensions/logging.html#automatic-logging"><span class="std std-ref">Automatic Logging</span></a> for details.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.log_dict.params.on_epoch"></span><strong>on_epoch</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.log_dict.params.on_epoch">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>]) – if <code class="docutils literal notranslate"><span class="pre">True</span></code> logs epoch accumulated metrics.
<code class="docutils literal notranslate"><span class="pre">None</span></code> auto-logs for val/test step but not <code class="docutils literal notranslate"><span class="pre">training_step</span></code>.
The default value is determined by the hook.
See <a class="reference internal" href="../extensions/logging.html#automatic-logging"><span class="std std-ref">Automatic Logging</span></a> for details.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.log_dict.params.reduce_fx"></span><strong>reduce_fx</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.log_dict.params.reduce_fx">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>]) – reduction function over step values for end of epoch. <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.mean()</span></code> by default.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.log_dict.params.enable_graph"></span><strong>enable_graph</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.log_dict.params.enable_graph">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, will not auto-detach the graph</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.log_dict.params.sync_dist"></span><strong>sync_dist</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.log_dict.params.sync_dist">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, reduces the metric across GPUs/TPUs. Use with care as this may lead to a significant
communication overhead.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.log_dict.params.sync_dist_group"></span><strong>sync_dist_group</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.log_dict.params.sync_dist_group">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – the ddp group to sync across.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.log_dict.params.add_dataloader_idx"></span><strong>add_dataloader_idx</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.log_dict.params.add_dataloader_idx">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, appends the index of the current dataloader to
the name (when using multiple). If <code class="docutils literal notranslate"><span class="pre">False</span></code>, user needs to give unique names for
each dataloader to not mix values.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.log_dict.params.batch_size"></span><strong>batch_size</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.log_dict.params.batch_size">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – Current batch size. This will be directly inferred from the loaded batch,
but some data structures might need to explicitly provide it.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.log_dict.params.rank_zero_only"></span><strong>rank_zero_only</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.log_dict.params.rank_zero_only">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – Whether the value will be logged only on rank 0. This will prevent synchronization which
would produce a deadlock as not all processes would perform this log call.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="lr-schedulers">
<h4>lr_schedulers<a class="headerlink" href="#lr-schedulers" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">lr_schedulers</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_lightning/core/lightning.html#LightningModule.lr_schedulers"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Returns the learning rate scheduler(s) that are being used during training. Useful for manual
optimization.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">_LRScheduler</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">ReduceLROnPlateau</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">_LRScheduler</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">ReduceLROnPlateau</span></code>]], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A single scheduler, or a list of schedulers in case multiple ones are present, or <code class="docutils literal notranslate"><span class="pre">None</span></code> if no
schedulers were returned in <code class="xref py py-meth docutils literal notranslate"><span class="pre">configure_optimizers()</span></code>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="manual-backward">
<h4>manual_backward<a class="headerlink" href="#manual-backward" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">manual_backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_lightning/core/lightning.html#LightningModule.manual_backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Call this directly from your <code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code> when doing optimizations manually. By using this,
Lightning can ensure that all the proper scaling gets applied when using mixed precision.</p>
<p>See <a class="reference internal" href="optimization.html#id2"><span class="std std-ref">manual optimization</span></a> for more examples.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizers</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">...</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="c1"># automatically applies scaling, etc...</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">manual_backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.manual_backward.params.loss"></span><strong>loss</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.manual_backward.params.loss">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – The tensor on which to compute gradients. Must have a graph attached.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.manual_backward.params.*args"></span><strong>*args</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.manual_backward.params.*args">¶</a> – Additional positional arguments to be forwarded to <code class="xref py py-meth docutils literal notranslate"><span class="pre">backward()</span></code></p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.manual_backward.params.**kwargs"></span><strong>**kwargs</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.manual_backward.params.**kwargs">¶</a> – Additional keyword arguments to be forwarded to <code class="xref py py-meth docutils literal notranslate"><span class="pre">backward()</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="optimizers">
<h4>optimizers<a class="headerlink" href="#optimizers" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">optimizers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">use_pl_optimizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Literal</span><span class="p"><span class="pre">[</span></span><span class="k"><span class="pre">True</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">pytorch_lightning.core.optimizer.LightningOptimizer</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">pytorch_lightning.core.optimizer.LightningOptimizer</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/pytorch_lightning/core/lightning.html#LightningModule.optimizers"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">optimizers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">use_pl_optimizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Literal</span><span class="p"><span class="pre">[</span></span><span class="k"><span class="pre">False</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.optim.optimizer.Optimizer</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.optim.optimizer.Optimizer</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span></dt>
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">optimizers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">use_pl_optimizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.optim.optimizer.Optimizer</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">pytorch_lightning.core.optimizer.LightningOptimizer</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.optim.optimizer.Optimizer</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">pytorch_lightning.core.optimizer.LightningOptimizer</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span></dt>
<dd><p>Returns the optimizer(s) that are being used during training. Useful for manual optimization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.optimizers.params.use_pl_optimizer"></span><strong>use_pl_optimizer</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.optimizers.params.use_pl_optimizer">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, will wrap the optimizer(s) in a
<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code> for automatic handling of precision and
profiling.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code>]]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A single optimizer, or a list of optimizers in case multiple ones are present.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="print">
<h4>print<a class="headerlink" href="#print" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">print</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_lightning/core/lightning.html#LightningModule.print"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Prints only from process 0. Use this in any distributed mode to log only once.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.print.params.*args"></span><strong>*args</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.print.params.*args">¶</a> – The thing to print. The same as for Python’s built-in print function.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.print.params.**kwargs"></span><strong>**kwargs</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.print.params.**kwargs">¶</a> – The same as for Python’s built-in print function.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;in forward&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="predict-step">
<h4>predict_step<a class="headerlink" href="#predict-step" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">predict_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_lightning/core/lightning.html#LightningModule.predict_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Step function called during <code class="xref py py-meth docutils literal notranslate"><span class="pre">predict()</span></code>. By default, it
calls <code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code>. Override to add any processing
logic.</p>
<p>The <code class="xref py py-meth docutils literal notranslate"><span class="pre">predict_step()</span></code> is used
to scale inference on multi-devices.</p>
<p>To prevent an OOM error, it is possible to use <code class="xref py py-class docutils literal notranslate"><span class="pre">BasePredictionWriter</span></code>
callback to write the predictions to disk or database after each batch or on epoch end.</p>
<p>The <code class="xref py py-class docutils literal notranslate"><span class="pre">BasePredictionWriter</span></code> should be used while using a spawn
based accelerator. This happens for <code class="docutils literal notranslate"><span class="pre">Trainer(strategy=&quot;ddp_spawn&quot;)</span></code>
or training on 8 TPU cores with <code class="docutils literal notranslate"><span class="pre">Trainer(accelerator=&quot;tpu&quot;,</span> <span class="pre">devices=8)</span></code> as predictions won’t be returned.</p>
<p>Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyModel</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">predicts_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

<span class="n">dm</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dm</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.predict_step.params.batch"></span><strong>batch</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.predict_step.params.batch">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – Current batch.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.predict_step.params.batch_idx"></span><strong>batch_idx</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.predict_step.params.batch_idx">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Index of current batch.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.predict_step.params.dataloader_idx"></span><strong>dataloader_idx</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.predict_step.params.dataloader_idx">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Index of the current dataloader.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Predicted output</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="save-hyperparameters">
<h4>save_hyperparameters<a class="headerlink" href="#save-hyperparameters" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">save_hyperparameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">frame</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logger</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Save arguments to <code class="docutils literal notranslate"><span class="pre">hparams</span></code> attribute.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.save_hyperparameters.params.args"></span><strong>args</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.save_hyperparameters.params.args">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – single object of <cite>dict</cite>, <cite>NameSpace</cite> or <cite>OmegaConf</cite>
or string names or arguments from class <code class="docutils literal notranslate"><span class="pre">__init__</span></code></p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.save_hyperparameters.params.ignore"></span><strong>ignore</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.save_hyperparameters.params.ignore">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – an argument name or a list of argument names from
class <code class="docutils literal notranslate"><span class="pre">__init__</span></code> to be ignored</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.save_hyperparameters.params.frame"></span><strong>frame</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.save_hyperparameters.params.frame">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">frame</span></code>]) – a frame object. Default is None</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.save_hyperparameters.params.logger"></span><strong>logger</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.save_hyperparameters.params.logger">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – Whether to send the hyperparameters to the logger. Default: True</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">ManuallyArgsModel</span><span class="p">(</span><span class="n">HyperparametersMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="p">,</span> <span class="n">arg3</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="c1"># manually assign arguments</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">(</span><span class="s1">&#39;arg1&#39;</span><span class="p">,</span> <span class="s1">&#39;arg3&#39;</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="gp">... </span>        <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ManuallyArgsModel</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;abc&#39;</span><span class="p">,</span> <span class="mf">3.14</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">hparams</span>
<span class="go">&quot;arg1&quot;: 1</span>
<span class="go">&quot;arg3&quot;: 3.14</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">AutomaticArgsModel</span><span class="p">(</span><span class="n">HyperparametersMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="p">,</span> <span class="n">arg3</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="c1"># equivalent automatic</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">()</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="gp">... </span>        <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutomaticArgsModel</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;abc&#39;</span><span class="p">,</span> <span class="mf">3.14</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">hparams</span>
<span class="go">&quot;arg1&quot;: 1</span>
<span class="go">&quot;arg2&quot;: abc</span>
<span class="go">&quot;arg3&quot;: 3.14</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">SingleArgModel</span><span class="p">(</span><span class="n">HyperparametersMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="c1"># manually assign single argument</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="gp">... </span>        <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">SingleArgModel</span><span class="p">(</span><span class="n">Namespace</span><span class="p">(</span><span class="n">p1</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">p2</span><span class="o">=</span><span class="s1">&#39;abc&#39;</span><span class="p">,</span> <span class="n">p3</span><span class="o">=</span><span class="mf">3.14</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">hparams</span>
<span class="go">&quot;p1&quot;: 1</span>
<span class="go">&quot;p2&quot;: abc</span>
<span class="go">&quot;p3&quot;: 3.14</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">ManuallyArgsModel</span><span class="p">(</span><span class="n">HyperparametersMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="p">,</span> <span class="n">arg3</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="c1"># pass argument(s) to ignore as a string or in a list</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">(</span><span class="n">ignore</span><span class="o">=</span><span class="s1">&#39;arg2&#39;</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="gp">... </span>        <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ManuallyArgsModel</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;abc&#39;</span><span class="p">,</span> <span class="mf">3.14</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">hparams</span>
<span class="go">&quot;arg1&quot;: 1</span>
<span class="go">&quot;arg3&quot;: 3.14</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="toggle-optimizer">
<h4>toggle_optimizer<a class="headerlink" href="#toggle-optimizer" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">toggle_optimizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_lightning/core/lightning.html#LightningModule.toggle_optimizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Makes sure only the gradients of the current optimizer’s parameters are calculated in the training step
to prevent dangling gradients in multiple-optimizer setup.</p>
<p>This is only called automatically when automatic optimization is enabled and multiple optimizers are used.
It works with <code class="xref py py-meth docutils literal notranslate"><span class="pre">untoggle_optimizer()</span></code> to make sure <code class="docutils literal notranslate"><span class="pre">param_requires_grad_state</span></code> is properly reset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.toggle_optimizer.params.optimizer"></span><strong>optimizer</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.toggle_optimizer.params.optimizer">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code>]) – The optimizer to toggle.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.toggle_optimizer.params.optimizer_idx"></span><strong>optimizer_idx</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.toggle_optimizer.params.optimizer_idx">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The index of the optimizer to toggle.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="test-step">
<h4>test_step<a class="headerlink" href="#test-step" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">test_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_lightning/core/lightning.html#LightningModule.test_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Operates on a single batch of data from the test set.
In this step you’d normally generate examples or calculate anything of interest
such as accuracy.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">test_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">test_batch</span> <span class="ow">in</span> <span class="n">test_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">test_step</span><span class="p">(</span><span class="n">test_batch</span><span class="p">)</span>
    <span class="n">test_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">test_epoch_end</span><span class="p">(</span><span class="n">test_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.test_step.params.batch"></span><strong>batch</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.test_step.params.batch">¶</a> – The output of your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.test_step.params.batch_idx"></span><strong>batch_idx</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.test_step.params.batch_idx">¶</a> – The index of this batch.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.test_step.params.dataloader_id"></span><strong>dataloader_id</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.test_step.params.dataloader_id">¶</a> – The index of the dataloader that produced this batch.
(only if multiple test dataloaders used).</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><p>Any of.</p>
<blockquote>
<div><ul class="simple">
<li><p>Any object or value</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> - Testing will skip to the next batch</p></li>
</ul>
</div></blockquote>
</p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># if you have one test dataloader:</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="o">...</span>


<span class="c1"># if you have multiple test dataloaders:</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="o">...</span>
</pre></div>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 1: A single test dataset</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="c1"># implement your own</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># log 6 example images</span>
    <span class="c1"># or generated text... or whatever</span>
    <span class="n">sample_imgs</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="mi">6</span><span class="p">]</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">make_grid</span><span class="p">(</span><span class="n">sample_imgs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_image</span><span class="p">(</span><span class="s1">&#39;example_images&#39;</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># calculate acc</span>
    <span class="n">labels_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">test_acc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">labels_hat</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span><span class="p">)</span>

    <span class="c1"># log the outputs!</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">({</span><span class="s1">&#39;test_loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;test_acc&#39;</span><span class="p">:</span> <span class="n">test_acc</span><span class="p">})</span>
</pre></div>
</div>
<p>If you pass in multiple test dataloaders, <code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code> will have an additional argument. We recommend
setting the default value of 0 so that you can quickly switch between single and multiple dataloaders.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 2: multiple test dataloaders</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="c1"># dataloader_idx tells you which dataset this is.</span>
    <span class="o">...</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need to test you don’t need to implement this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When the <code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code> is called, the model has been put in eval mode and
PyTorch gradients have been disabled. At the end of the test epoch, the model goes back
to training mode and gradients are enabled.</p>
</div>
</dd></dl>

</section>
<section id="test-step-end">
<h4>test_step_end<a class="headerlink" href="#test-step-end" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">test_step_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_lightning/core/lightning.html#LightningModule.test_step_end"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Use this when testing with dp or ddp2 because <code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code> will operate on only part of the batch.
However, this is still optional and only needed for things like softmax or NCE loss.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you later switch to ddp or some other mode, this will still be called
so that you don’t have to change your code.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># pseudocode</span>
<span class="n">sub_batches</span> <span class="o">=</span> <span class="n">split_batches_for_dp</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
<span class="n">step_output</span> <span class="o">=</span> <span class="p">[</span><span class="n">test_step</span><span class="p">(</span><span class="n">sub_batch</span><span class="p">)</span> <span class="k">for</span> <span class="n">sub_batch</span> <span class="ow">in</span> <span class="n">sub_batches</span><span class="p">]</span>
<span class="n">test_step_end</span><span class="p">(</span><span class="n">step_output</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.test_step_end.params.step_output"></span><strong>step_output</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.test_step_end.params.step_output">¶</a> – What you return in <code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code> for each batch part.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None or anything</p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># WITHOUT test_step_end</span>
<span class="c1"># if used in DP or DDP2, this batch is 1/num_gpus large</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># batch is 1/num_gpus big</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;test_loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>


<span class="c1"># --------------</span>
<span class="c1"># with test_step_end to do softmax over the full batch</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># batch is 1/num_gpus big</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>


<span class="k">def</span> <span class="nf">test_step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_results</span><span class="p">):</span>
    <span class="c1"># this out is now the full size of the batch</span>
    <span class="n">all_test_step_outs</span> <span class="o">=</span> <span class="n">output_results</span><span class="o">.</span><span class="n">out</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">nce_loss</span><span class="p">(</span><span class="n">all_test_step_outs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;test_loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>See the <span class="xref std std-ref">accelerators/gpu:Multi GPU Training</span> guide for more details.</p>
</div>
</dd></dl>

</section>
<section id="test-epoch-end">
<h4>test_epoch_end<a class="headerlink" href="#test-epoch-end" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">test_epoch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_lightning/core/lightning.html#LightningModule.test_epoch_end"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Called at the end of a test epoch with the output of all test steps.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">test_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">test_batch</span> <span class="ow">in</span> <span class="n">test_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">test_step</span><span class="p">(</span><span class="n">test_batch</span><span class="p">)</span>
    <span class="n">test_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">test_epoch_end</span><span class="p">(</span><span class="n">test_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.test_epoch_end.params.outputs"></span><strong>outputs</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.test_epoch_end.params.outputs">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]], <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]]]]) – List of outputs you defined in <code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step_end()</span></code>, or if there
are multiple dataloaders, a list containing a list of outputs for each dataloader</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you didn’t define a <code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code>, this won’t be called.</p>
</div>
<p class="rubric">Examples</p>
<p>With a single dataloader:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="c1"># do something with the outputs of all test batches</span>
    <span class="n">all_test_preds</span> <span class="o">=</span> <span class="n">test_step_outputs</span><span class="o">.</span><span class="n">predictions</span>

    <span class="n">some_result</span> <span class="o">=</span> <span class="n">calc_all_results</span><span class="p">(</span><span class="n">all_test_preds</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">some_result</span><span class="p">)</span>
</pre></div>
</div>
<p>With multiple dataloaders, <cite>outputs</cite> will be a list of lists. The outer list contains
one entry per dataloader, while the inner list contains the individual outputs of
each test step for that dataloader.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="n">final_value</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">dataloader_outputs</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">test_step_out</span> <span class="ow">in</span> <span class="n">dataloader_outputs</span><span class="p">:</span>
            <span class="c1"># do something</span>
            <span class="n">final_value</span> <span class="o">+=</span> <span class="n">test_step_out</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;final_metric&quot;</span><span class="p">,</span> <span class="n">final_value</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="to-onnx">
<h4>to_onnx<a class="headerlink" href="#to-onnx" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">to_onnx</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_sample</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_lightning/core/lightning.html#LightningModule.to_onnx"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Saves the model in ONNX format.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.to_onnx.params.file_path"></span><strong>file_path</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.to_onnx.params.file_path">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Path</span></code>]) – The path of the file the onnx model should be saved to.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.to_onnx.params.input_sample"></span><strong>input_sample</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.to_onnx.params.input_sample">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – An input for tracing. Default: None (Use self.example_input_array)</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.to_onnx.params.**kwargs"></span><strong>**kwargs</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.to_onnx.params.**kwargs">¶</a> – Will be passed to torch.onnx.export function.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">SimpleModel</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">NamedTemporaryFile</span><span class="p">(</span><span class="n">suffix</span><span class="o">=</span><span class="s1">&#39;.onnx&#39;</span><span class="p">,</span> <span class="n">delete</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="k">as</span> <span class="n">tmpfile</span><span class="p">:</span>
<span class="gp">... </span>    <span class="n">model</span> <span class="o">=</span> <span class="n">SimpleModel</span><span class="p">()</span>
<span class="gp">... </span>    <span class="n">input_sample</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">))</span>
<span class="gp">... </span>    <span class="n">model</span><span class="o">.</span><span class="n">to_onnx</span><span class="p">(</span><span class="n">tmpfile</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">input_sample</span><span class="p">,</span> <span class="n">export_params</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">tmpfile</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="to-torchscript">
<h4>to_torchscript<a class="headerlink" href="#to-torchscript" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">to_torchscript</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'script'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">example_inputs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_lightning/core/lightning.html#LightningModule.to_torchscript"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>By default compiles the whole model to a <code class="xref py py-class docutils literal notranslate"><span class="pre">ScriptModule</span></code>. If you want to use tracing,
please provided the argument <code class="docutils literal notranslate"><span class="pre">method='trace'</span></code> and make sure that either the <cite>example_inputs</cite> argument is
provided, or the model has <code class="xref py py-attr docutils literal notranslate"><span class="pre">example_input_array</span></code> set. If you would like to customize the modules that
are scripted you should override this method. In case you want to return multiple modules, we recommend
using a dictionary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.to_torchscript.params.file_path"></span><strong>file_path</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.to_torchscript.params.file_path">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Path</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – Path where to save the torchscript. Default: None (no file saved).</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.to_torchscript.params.method"></span><strong>method</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.to_torchscript.params.method">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Whether to use TorchScript’s script or trace method. Default: ‘script’</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.to_torchscript.params.example_inputs"></span><strong>example_inputs</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.to_torchscript.params.example_inputs">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – An input to be used to do tracing when method is set to ‘trace’.
Default: None (uses <code class="xref py py-attr docutils literal notranslate"><span class="pre">example_input_array</span></code>)</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.to_torchscript.params.**kwargs"></span><strong>**kwargs</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.to_torchscript.params.**kwargs">¶</a> – Additional arguments that will be passed to the <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.jit.script()</span></code> or
<code class="xref py py-func docutils literal notranslate"><span class="pre">torch.jit.trace()</span></code> function.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>Requires the implementation of the
<code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code> method.</p></li>
<li><p>The exported script will be set to evaluation mode.</p></li>
<li><p>It is recommended that you install the latest supported version of PyTorch
to use this feature without limitations. See also the <code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.jit</span></code>
documentation for supported features.</p></li>
</ul>
</div>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">SimpleModel</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">SimpleModel</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">to_torchscript</span><span class="p">(</span><span class="n">file_path</span><span class="o">=</span><span class="s2">&quot;model.pt&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="s2">&quot;model.pt&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">to_torchscript</span><span class="p">(</span><span class="n">file_path</span><span class="o">=</span><span class="s2">&quot;model_trace.pt&quot;</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;trace&#39;</span><span class="p">,</span> 
<span class="gp">... </span>                                    <span class="n">example_inputs</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">)))</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="s2">&quot;model_trace.pt&quot;</span><span class="p">)</span>  
<span class="go">True</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">ScriptModule</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">ScriptModule</span></code>]]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This LightningModule as a torchscript, regardless of whether <cite>file_path</cite> is
defined or not.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="training-step">
<h4>training_step<a class="headerlink" href="#training-step" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">training_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_lightning/core/lightning.html#LightningModule.training_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Here you compute and return the training loss and some additional metrics for e.g.
the progress bar or logger.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.training_step.params.batch"></span><strong>batch</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.training_step.params.batch">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> | (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …) | [<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …]) – The output of your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>. A tensor, tuple or list.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.training_step.params.batch_idx"></span><strong>batch_idx</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.training_step.params.batch_idx">¶</a> (<code class="docutils literal notranslate"><span class="pre">int</span></code>) – Integer displaying index of this batch</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.training_step.params.optimizer_idx"></span><strong>optimizer_idx</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.training_step.params.optimizer_idx">¶</a> (<code class="docutils literal notranslate"><span class="pre">int</span></code>) – When using multiple optimizers, this argument will also be present.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.training_step.params.hiddens"></span><strong>hiddens</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.training_step.params.hiddens">¶</a> (<code class="docutils literal notranslate"><span class="pre">Any</span></code>) – Passed in if
<code class="xref py py-paramref docutils literal notranslate"><span class="pre">truncated_bptt_steps</span></code> &gt; 0.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><p>Any of.</p>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> - The loss tensor</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dict</span></code> - A dictionary. Can include any keys, but must include the key <code class="docutils literal notranslate"><span class="pre">'loss'</span></code></p></li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">None</span></code> - Training will skip to the next batch. This is only for automatic optimization.</dt><dd><p>This is not supported for multi-GPU, TPU, IPU, or DeepSpeed.</p>
</dd>
</dl>
</li>
</ul>
</p>
</dd>
</dl>
<p>In this step you’d normally do the forward pass and calculate the loss for a batch.
You can also do fancier things like multiple forward passes or something model specific.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<p>If you define multiple optimizers, this step will be called with an additional
<code class="docutils literal notranslate"><span class="pre">optimizer_idx</span></code> parameter.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Multiple optimizers (e.g.: GANs)</span>
<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># do training_step with encoder</span>
        <span class="o">...</span>
    <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># do training_step with decoder</span>
        <span class="o">...</span>
</pre></div>
</div>
<p>If you add truncated back propagation through time you will also get an additional
argument with the hidden states of the previous step.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Truncated back-propagation through time</span>
<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">hiddens</span><span class="p">):</span>
    <span class="c1"># hiddens are the hidden states from the previous truncated backprop step</span>
    <span class="n">out</span><span class="p">,</span> <span class="n">hiddens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">hiddens</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">...</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;loss&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s2">&quot;hiddens&quot;</span><span class="p">:</span> <span class="n">hiddens</span><span class="p">}</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The loss value shown in the progress bar is smoothed (averaged) over the last values,
so it differs from the actual loss returned in train/validation step.</p>
</div>
</dd></dl>

</section>
<section id="training-step-end">
<h4>training_step_end<a class="headerlink" href="#training-step-end" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">training_step_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">step_output</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_lightning/core/lightning.html#LightningModule.training_step_end"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Use this when training with dp or ddp2 because <code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code> will operate on only part of the
batch. However, this is still optional and only needed for things like softmax or NCE loss.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you later switch to ddp or some other mode, this will still be called
so that you don’t have to change your code</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># pseudocode</span>
<span class="n">sub_batches</span> <span class="o">=</span> <span class="n">split_batches_for_dp</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
<span class="n">step_output</span> <span class="o">=</span> <span class="p">[</span><span class="n">training_step</span><span class="p">(</span><span class="n">sub_batch</span><span class="p">)</span> <span class="k">for</span> <span class="n">sub_batch</span> <span class="ow">in</span> <span class="n">sub_batches</span><span class="p">]</span>
<span class="n">training_step_end</span><span class="p">(</span><span class="n">step_output</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.training_step_end.params.step_output"></span><strong>step_output</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.training_step_end.params.step_output">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – What you return in <cite>training_step</cite> for each batch part.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Anything</p>
</dd>
</dl>
<p>When using dp/ddp2 distributed backends, only a portion of the batch is inside the training_step:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># batch is 1/num_gpus big</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># softmax uses only a portion of the batch in the denominator</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">nce_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<p>If you wish to do something with all the parts of the batch, then use this method to do it:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># batch is 1/num_gpus big</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;pred&quot;</span><span class="p">:</span> <span class="n">out</span><span class="p">}</span>


<span class="k">def</span> <span class="nf">training_step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training_step_outputs</span><span class="p">):</span>
    <span class="n">gpu_0_pred</span> <span class="o">=</span> <span class="n">training_step_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;pred&quot;</span><span class="p">]</span>
    <span class="n">gpu_1_pred</span> <span class="o">=</span> <span class="n">training_step_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;pred&quot;</span><span class="p">]</span>
    <span class="n">gpu_n_pred</span> <span class="o">=</span> <span class="n">training_step_outputs</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="s2">&quot;pred&quot;</span><span class="p">]</span>

    <span class="c1"># this softmax now uses the full batch</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">nce_loss</span><span class="p">([</span><span class="n">gpu_0_pred</span><span class="p">,</span> <span class="n">gpu_1_pred</span><span class="p">,</span> <span class="n">gpu_n_pred</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>See the <span class="xref std std-ref">accelerators/gpu:Multi GPU Training</span> guide for more details.</p>
</div>
</dd></dl>

</section>
<section id="training-epoch-end">
<h4>training_epoch_end<a class="headerlink" href="#training-epoch-end" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">training_epoch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_lightning/core/lightning.html#LightningModule.training_epoch_end"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Called at the end of the training epoch with the outputs of all training steps. Use this in case you
need to do something with all the outputs returned by <code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">train_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">train_batch</span> <span class="ow">in</span> <span class="n">train_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">training_step</span><span class="p">(</span><span class="n">train_batch</span><span class="p">)</span>
    <span class="n">train_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">training_epoch_end</span><span class="p">(</span><span class="n">train_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.training_epoch_end.params.outputs"></span><strong>outputs</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.training_epoch_end.params.outputs">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]]) – List of outputs you defined in <code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code>. If there are multiple optimizers or when
using <code class="docutils literal notranslate"><span class="pre">truncated_bptt_steps</span> <span class="pre">&gt;</span> <span class="pre">0</span></code>, the lists have the dimensions
(n_batches, tbptt_steps, n_optimizers). Dimensions of length 1 are squeezed.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If this method is not overridden, this won’t be called.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training_step_outputs</span><span class="p">):</span>
    <span class="c1"># do something with all training_step outputs</span>
    <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">training_step_outputs</span><span class="p">:</span>
        <span class="o">...</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="unfreeze">
<h4>unfreeze<a class="headerlink" href="#unfreeze" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">unfreeze</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_lightning/core/lightning.html#LightningModule.unfreeze"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Unfreeze all parameters for training.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">MyLightningModule</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">unfreeze</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="untoggle-optimizer">
<h4>untoggle_optimizer<a class="headerlink" href="#untoggle-optimizer" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">untoggle_optimizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_lightning/core/lightning.html#LightningModule.untoggle_optimizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Resets the state of required gradients that were toggled with <code class="xref py py-meth docutils literal notranslate"><span class="pre">toggle_optimizer()</span></code>.</p>
<p>This is only called automatically when automatic optimization is enabled and multiple optimizers are used.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.untoggle_optimizer.params.optimizer_idx"></span><strong>optimizer_idx</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.untoggle_optimizer.params.optimizer_idx">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The index of the optimizer to untoggle.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="validation-step">
<h4>validation_step<a class="headerlink" href="#validation-step" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">validation_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_lightning/core/lightning.html#LightningModule.validation_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Operates on a single batch of data from the validation set.
In this step you’d might generate examples or calculate anything of interest like accuracy.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">val_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">val_batch</span> <span class="ow">in</span> <span class="n">val_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">validation_step</span><span class="p">(</span><span class="n">val_batch</span><span class="p">)</span>
    <span class="n">val_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">validation_epoch_end</span><span class="p">(</span><span class="n">val_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.validation_step.params.batch"></span><strong>batch</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.validation_step.params.batch">¶</a> – The output of your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.validation_step.params.batch_idx"></span><strong>batch_idx</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.validation_step.params.batch_idx">¶</a> – The index of this batch.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.validation_step.params.dataloader_idx"></span><strong>dataloader_idx</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.validation_step.params.dataloader_idx">¶</a> – The index of the dataloader that produced this batch.
(only if multiple val dataloaders used)</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><ul class="simple">
<li><p>Any object or value</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> - Validation will skip to the next batch</p></li>
</ul>
</p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># pseudocode of order</span>
<span class="n">val_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">val_batch</span> <span class="ow">in</span> <span class="n">val_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">validation_step</span><span class="p">(</span><span class="n">val_batch</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">defined</span><span class="p">(</span><span class="s2">&quot;validation_step_end&quot;</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">validation_step_end</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">val_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">val_outs</span> <span class="o">=</span> <span class="n">validation_epoch_end</span><span class="p">(</span><span class="n">val_outs</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># if you have one val dataloader:</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="o">...</span>


<span class="c1"># if you have multiple val dataloaders:</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="o">...</span>
</pre></div>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 1: A single validation dataset</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="c1"># implement your own</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># log 6 example images</span>
    <span class="c1"># or generated text... or whatever</span>
    <span class="n">sample_imgs</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="mi">6</span><span class="p">]</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">make_grid</span><span class="p">(</span><span class="n">sample_imgs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_image</span><span class="p">(</span><span class="s1">&#39;example_images&#39;</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># calculate acc</span>
    <span class="n">labels_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">val_acc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">labels_hat</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span><span class="p">)</span>

    <span class="c1"># log the outputs!</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">({</span><span class="s1">&#39;val_loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;val_acc&#39;</span><span class="p">:</span> <span class="n">val_acc</span><span class="p">})</span>
</pre></div>
</div>
<p>If you pass in multiple val dataloaders, <code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code> will have an additional argument. We recommend
setting the default value of 0 so that you can quickly switch between single and multiple dataloaders.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 2: multiple validation dataloaders</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="c1"># dataloader_idx tells you which dataset this is.</span>
    <span class="o">...</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need to validate you don’t need to implement this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When the <code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code> is called, the model has been put in eval mode
and PyTorch gradients have been disabled. At the end of validation,
the model goes back to training mode and gradients are enabled.</p>
</div>
</dd></dl>

</section>
<section id="validation-step-end">
<h4>validation_step_end<a class="headerlink" href="#validation-step-end" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">validation_step_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_lightning/core/lightning.html#LightningModule.validation_step_end"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Use this when validating with dp or ddp2 because <code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code> will operate on only part of
the batch. However, this is still optional and only needed for things like softmax or NCE loss.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you later switch to ddp or some other mode, this will still be called
so that you don’t have to change your code.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># pseudocode</span>
<span class="n">sub_batches</span> <span class="o">=</span> <span class="n">split_batches_for_dp</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
<span class="n">step_output</span> <span class="o">=</span> <span class="p">[</span><span class="n">validation_step</span><span class="p">(</span><span class="n">sub_batch</span><span class="p">)</span> <span class="k">for</span> <span class="n">sub_batch</span> <span class="ow">in</span> <span class="n">sub_batches</span><span class="p">]</span>
<span class="n">validation_step_end</span><span class="p">(</span><span class="n">step_output</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.validation_step_end.params.step_output"></span><strong>step_output</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.validation_step_end.params.step_output">¶</a> – What you return in <code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code> for each batch part.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None or anything</p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># WITHOUT validation_step_end</span>
<span class="c1"># if used in DP or DDP2, this batch is 1/num_gpus large</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># batch is 1/num_gpus big</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">nce_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;val_loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>


<span class="c1"># --------------</span>
<span class="c1"># with validation_step_end to do softmax over the full batch</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># batch is 1/num_gpus big</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>


<span class="k">def</span> <span class="nf">validation_step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">val_step_outputs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">val_step_outputs</span><span class="p">:</span>
        <span class="o">...</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>See the <span class="xref std std-ref">accelerators/gpu:Multi GPU Training</span> guide for more details.</p>
</div>
</dd></dl>

</section>
<section id="validation-epoch-end">
<h4>validation_epoch_end<a class="headerlink" href="#validation-epoch-end" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">validation_epoch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_lightning/core/lightning.html#LightningModule.validation_epoch_end"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Called at the end of the validation epoch with the outputs of all validation steps.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">val_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">val_batch</span> <span class="ow">in</span> <span class="n">val_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">validation_step</span><span class="p">(</span><span class="n">val_batch</span><span class="p">)</span>
    <span class="n">val_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">validation_epoch_end</span><span class="p">(</span><span class="n">val_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.validation_epoch_end.params.outputs"></span><strong>outputs</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.validation_epoch_end.params.outputs">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]], <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]]]]) – List of outputs you defined in <code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code>, or if there
are multiple dataloaders, a list containing a list of outputs for each dataloader.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you didn’t define a <code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code>, this won’t be called.</p>
</div>
<p class="rubric">Examples</p>
<p>With a single dataloader:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">val_step_outputs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">val_step_outputs</span><span class="p">:</span>
        <span class="o">...</span>
</pre></div>
</div>
<p>With multiple dataloaders, <cite>outputs</cite> will be a list of lists. The outer list contains
one entry per dataloader, while the inner list contains the individual outputs of
each validation step for that dataloader.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">dataloader_output_result</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
        <span class="n">dataloader_outs</span> <span class="o">=</span> <span class="n">dataloader_output_result</span><span class="o">.</span><span class="n">dataloader_i_outputs</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;final_metric&quot;</span><span class="p">,</span> <span class="n">final_value</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</section>
</section>
<hr class="docutils" />
<section id="properties">
<h3>Properties<a class="headerlink" href="#properties" title="Permalink to this headline">¶</a></h3>
<p>These are properties available in a LightningModule.</p>
<section id="current-epoch">
<h4>current_epoch<a class="headerlink" href="#current-epoch" title="Permalink to this headline">¶</a></h4>
<p>The number of epochs run.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">current_epoch</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="o">...</span>
</pre></div>
</div>
</section>
<section id="device">
<h4>device<a class="headerlink" href="#device" title="Permalink to this headline">¶</a></h4>
<p>The device the module is on. Use it to keep your code device agnostic.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="global-rank">
<h4>global_rank<a class="headerlink" href="#global-rank" title="Permalink to this headline">¶</a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">global_rank</span></code> is the index of the current process across all nodes and devices.
Lightning will perform some operations such as logging, weight checkpointing only when <code class="docutils literal notranslate"><span class="pre">global_rank=0</span></code>. You
usually do not need to use this property, but it is useful to know how to access it if needed.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># do something only once across all the nodes</span>
        <span class="o">...</span>
</pre></div>
</div>
</section>
<section id="global-step">
<h4>global_step<a class="headerlink" href="#global-step" title="Permalink to this headline">¶</a></h4>
<p>The number of optimizer steps taken (does not reset each epoch).
This includes multiple optimizers and TBPTT steps (if enabled).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">log_image</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">global_step</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="hparams">
<h4>hparams<a class="headerlink" href="#hparams" title="Permalink to this headline">¶</a></h4>
<p>The arguments passed through <code class="docutils literal notranslate"><span class="pre">LightningModule.__init__()</span></code> and saved by calling
<code class="xref py py-meth docutils literal notranslate"><span class="pre">save_hyperparameters()</span></code> could be accessed by the <code class="docutils literal notranslate"><span class="pre">hparams</span></code> attribute.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="logger">
<h4>logger<a class="headerlink" href="#logger" title="Permalink to this headline">¶</a></h4>
<p>The current logger being used (tensorboard or other supported logger)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># the generic logger (same no matter if tensorboard or other supported logger)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span>

    <span class="c1"># the particular logger</span>
    <span class="n">tensorboard_logger</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span>
</pre></div>
</div>
</section>
<section id="loggers">
<h4>loggers<a class="headerlink" href="#loggers" title="Permalink to this headline">¶</a></h4>
<p>The list of loggers currently being used by the Trainer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># List of Logger objects</span>
    <span class="n">loggers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loggers</span>
    <span class="k">for</span> <span class="n">logger</span> <span class="ow">in</span> <span class="n">loggers</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">log_metrics</span><span class="p">({</span><span class="s2">&quot;foo&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">})</span>
</pre></div>
</div>
</section>
<section id="local-rank">
<h4>local_rank<a class="headerlink" href="#local-rank" title="Permalink to this headline">¶</a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">local_rank</span></code> is the index of the current process across all the devices for the current node.
You usually do not need to use this property, but it is useful to know how to access it if needed.
For example, if using 10 machines (or nodes), the GPU at index 0 on each machine has local_rank = 0.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># do something only once across each node</span>
        <span class="o">...</span>
</pre></div>
</div>
</section>
<section id="precision">
<h4>precision<a class="headerlink" href="#precision" title="Permalink to this headline">¶</a></h4>
<p>The type of precision used:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">precision</span> <span class="o">==</span> <span class="mi">16</span><span class="p">:</span>
        <span class="o">...</span>
</pre></div>
</div>
</section>
<section id="trainer">
<h4>trainer<a class="headerlink" href="#trainer" title="Permalink to this headline">¶</a></h4>
<p>Pointer to the trainer</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">max_steps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">max_steps</span>
    <span class="n">any_flag</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">any_flag</span>
</pre></div>
</div>
</section>
<section id="prepare-data-per-node">
<h4>prepare_data_per_node<a class="headerlink" href="#prepare-data-per-node" title="Permalink to this headline">¶</a></h4>
<p>If set to <code class="docutils literal notranslate"><span class="pre">True</span></code> will call <code class="docutils literal notranslate"><span class="pre">prepare_data()</span></code> on LOCAL_RANK=0 for every node.
If set to <code class="docutils literal notranslate"><span class="pre">False</span></code> will only call from NODE_RANK=0, LOCAL_RANK=0.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LitModel</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prepare_data_per_node</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
</section>
<section id="automatic-optimization">
<h4>automatic_optimization<a class="headerlink" href="#automatic-optimization" title="Permalink to this headline">¶</a></h4>
<p>When set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, Lightning does not automate the optimization process. This means you are responsible for handling
your optimizers. However, we do take care of precision and any accelerators used.</p>
<p>See <a class="reference internal" href="optimization.html#id2"><span class="std std-ref">manual optimization</span></a> for details.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">automatic_optimization</span> <span class="o">=</span> <span class="kc">False</span>


<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizers</span><span class="p">(</span><span class="n">use_pl_optimizer</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="o">...</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">manual_backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<p>This is recommended only if using 2+ optimizers AND if you know how to perform the optimization procedure properly. Note
that automatic optimization can still be used with multiple optimizers by relying on the <code class="docutils literal notranslate"><span class="pre">optimizer_idx</span></code> parameter.
Manual optimization is most useful for research topics like reinforcement learning, sparse coding, and GAN research.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">automatic_optimization</span> <span class="o">=</span> <span class="kc">False</span>


<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># access your optimizers with use_pl_optimizer=False. Default is True</span>
    <span class="n">opt_a</span><span class="p">,</span> <span class="n">opt_b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizers</span><span class="p">(</span><span class="n">use_pl_optimizer</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">gen_loss</span> <span class="o">=</span> <span class="o">...</span>
    <span class="n">opt_a</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">manual_backward</span><span class="p">(</span><span class="n">gen_loss</span><span class="p">)</span>
    <span class="n">opt_a</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="n">disc_loss</span> <span class="o">=</span> <span class="o">...</span>
    <span class="n">opt_b</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">manual_backward</span><span class="p">(</span><span class="n">disc_loss</span><span class="p">)</span>
    <span class="n">opt_b</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="example-input-array">
<h4>example_input_array<a class="headerlink" href="#example-input-array" title="Permalink to this headline">¶</a></h4>
<p>Set and access example_input_array, which basically represents a single batch.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">example_input_array</span> <span class="o">=</span> <span class="o">...</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">generator</span> <span class="o">=</span> <span class="o">...</span>


<span class="k">def</span> <span class="nf">on_train_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># generate some images using the example_input_array</span>
    <span class="n">gen_images</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generator</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">example_input_array</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="truncated-bptt-steps">
<h4>truncated_bptt_steps<a class="headerlink" href="#truncated-bptt-steps" title="Permalink to this headline">¶</a></h4>
<p>Truncated Backpropagation Through Time (TBPTT) performs perform backpropogation every k steps of
a much longer sequence. This is made possible by passing training batches
split along the time-dimensions into splits of size k to the
<code class="docutils literal notranslate"><span class="pre">training_step</span></code>. In order to keep the same forward propagation behavior, all
hidden states should be kept in-between each time-dimension split.</p>
<p>If this is enabled, your batches will automatically get truncated
and the Trainer will apply Truncated Backprop to it.</p>
<p>(<a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.56.7941&amp;rep=rep1&amp;type=pdf">Williams et al. “An efficient gradient-based algorithm for on-line training of
recurrent network trajectories.”</a>)</p>
<p><a class="reference external" href="https://d2l.ai/chapter_recurrent-neural-networks/bptt.html">Tutorial</a></p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_lightning</span> <span class="kn">import</span> <span class="n">LightningModule</span>


<span class="k">class</span> <span class="nc">MyModel</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># batch_first has to be set to True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span>
            <span class="n">input_size</span><span class="o">=</span><span class="n">input_size</span><span class="p">,</span>
            <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">num_layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span>
            <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="o">...</span>

        <span class="c1"># Important: This property activates truncated backpropagation through time</span>
        <span class="c1"># Setting this value to 2 splits the batch into sequences of size 2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">truncated_bptt_steps</span> <span class="o">=</span> <span class="mi">2</span>

    <span class="c1"># Truncated back-propagation through time</span>
    <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">hiddens</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

        <span class="c1"># the training step must be updated to accept a ``hiddens`` argument</span>
        <span class="c1"># hiddens are the hiddens from the previous truncated backprop step</span>
        <span class="n">out</span><span class="p">,</span> <span class="n">hiddens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">hiddens</span><span class="p">)</span>

        <span class="o">...</span>

        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;loss&quot;</span><span class="p">:</span> <span class="o">...</span><span class="p">,</span> <span class="s2">&quot;hiddens&quot;</span><span class="p">:</span> <span class="n">hiddens</span><span class="p">}</span>
</pre></div>
</div>
<p>Lightning takes care of splitting your batch along the time-dimension. It is
assumed to be the second dimension of your batches. Therefore, in the
example above, we have set <code class="docutils literal notranslate"><span class="pre">batch_first=True</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># we use the second as the time dimension</span>
<span class="c1"># (batch, time, ...)</span>
<span class="n">sub_batch</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="n">t</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
</pre></div>
</div>
<p>To modify how the batch is split,
override the <code class="xref py py-meth docutils literal notranslate"><span class="pre">pytorch_lightning.core.lightning.LightningModule.tbptt_split_batch()</span></code> method:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LitMNIST</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">tbptt_split_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">split_size</span><span class="p">):</span>
        <span class="c1"># do your own splitting on the batch</span>
        <span class="k">return</span> <span class="n">splits</span>
</pre></div>
</div>
<hr class="docutils" />
</section>
</section>
<section id="hooks">
<span id="lightning-hooks"></span><h3>Hooks<a class="headerlink" href="#hooks" title="Permalink to this headline">¶</a></h3>
<p>This is the pseudocode to describe the structure of <code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code>.
The inputs and outputs of each function are not represented for simplicity. Please check each function’s API reference
for more information.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">global_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># prepare data is called on GLOBAL_ZERO only</span>
        <span class="n">prepare_data</span><span class="p">()</span>

    <span class="n">configure_callbacks</span><span class="p">()</span>

    <span class="k">with</span> <span class="n">parallel</span><span class="p">(</span><span class="n">devices</span><span class="p">):</span>
        <span class="c1"># devices can be GPUs, TPUs, ...</span>
        <span class="n">train_on_device</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">train_on_device</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="c1"># called PER DEVICE</span>
    <span class="n">on_fit_start</span><span class="p">()</span>
    <span class="n">setup</span><span class="p">(</span><span class="s2">&quot;fit&quot;</span><span class="p">)</span>
    <span class="n">configure_optimizers</span><span class="p">()</span>

    <span class="c1"># the sanity check runs here</span>

    <span class="n">on_train_start</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">epochs</span><span class="p">:</span>
        <span class="n">fit_loop</span><span class="p">()</span>
    <span class="n">on_train_end</span><span class="p">()</span>

    <span class="n">on_fit_end</span><span class="p">()</span>
    <span class="n">teardown</span><span class="p">(</span><span class="s2">&quot;fit&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">fit_loop</span><span class="p">():</span>
    <span class="n">on_train_epoch_start</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_dataloader</span><span class="p">():</span>
        <span class="n">on_train_batch_start</span><span class="p">()</span>

        <span class="n">on_before_batch_transfer</span><span class="p">()</span>
        <span class="n">transfer_batch_to_device</span><span class="p">()</span>
        <span class="n">on_after_batch_transfer</span><span class="p">()</span>

        <span class="n">training_step</span><span class="p">()</span>

        <span class="n">on_before_zero_grad</span><span class="p">()</span>
        <span class="n">optimizer_zero_grad</span><span class="p">()</span>

        <span class="n">on_before_backward</span><span class="p">()</span>
        <span class="n">backward</span><span class="p">()</span>
        <span class="n">on_after_backward</span><span class="p">()</span>

        <span class="n">on_before_optimizer_step</span><span class="p">()</span>
        <span class="n">configure_gradient_clipping</span><span class="p">()</span>
        <span class="n">optimizer_step</span><span class="p">()</span>

        <span class="n">on_train_batch_end</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">should_check_val</span><span class="p">:</span>
            <span class="n">val_loop</span><span class="p">()</span>
    <span class="c1"># end training epoch</span>
    <span class="n">training_epoch_end</span><span class="p">()</span>

    <span class="n">on_train_epoch_end</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">val_loop</span><span class="p">():</span>
    <span class="n">on_validation_model_eval</span><span class="p">()</span>  <span class="c1"># calls `model.eval()`</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">on_validation_start</span><span class="p">()</span>
    <span class="n">on_validation_epoch_start</span><span class="p">()</span>

    <span class="n">val_outs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">val_dataloader</span><span class="p">()):</span>
        <span class="n">on_validation_batch_start</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>

        <span class="n">batch</span> <span class="o">=</span> <span class="n">on_before_batch_transfer</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">transfer_batch_to_device</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">on_after_batch_transfer</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="n">validation_step</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>

        <span class="n">on_validation_batch_end</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>
        <span class="n">val_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

    <span class="n">validation_epoch_end</span><span class="p">(</span><span class="n">val_outs</span><span class="p">)</span>

    <span class="n">on_validation_epoch_end</span><span class="p">()</span>
    <span class="n">on_validation_end</span><span class="p">()</span>

    <span class="c1"># set up for train</span>
    <span class="n">on_validation_model_train</span><span class="p">()</span>  <span class="c1"># calls `model.train()`</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<section id="backward">
<h4>backward<a class="headerlink" href="#backward" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_lightning/core/lightning.html#LightningModule.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Called to perform backward on the loss returned in <code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code>. Override this hook with your
own implementation if you need to.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.backward.params.loss"></span><strong>loss</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.backward.params.loss">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – The loss tensor returned by <code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code>. If gradient accumulation is used, the loss here
holds the normalized value (scaled by 1 / accumulation steps).</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.backward.params.optimizer"></span><strong>optimizer</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.backward.params.optimizer">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>]) – Current optimizer being used. <code class="docutils literal notranslate"><span class="pre">None</span></code> if using manual optimization.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.backward.params.optimizer_idx"></span><strong>optimizer_idx</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.backward.params.optimizer_idx">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – Index of the current optimizer being used. <code class="docutils literal notranslate"><span class="pre">None</span></code> if using manual optimization.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">):</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-before-backward">
<h4>on_before_backward<a class="headerlink" href="#on-before-backward" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_before_backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Called before <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.on_before_backward.params.loss"></span><strong>loss</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.on_before_backward.params.loss">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – Loss divided by number of batches for gradient accumulation and scaled if using native AMP.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-after-backward">
<h4>on_after_backward<a class="headerlink" href="#on-after-backward" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_after_backward</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Called after <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code> and before optimizers are stepped.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If using native AMP, the gradients will not be unscaled at this point.
Use the <code class="docutils literal notranslate"><span class="pre">on_before_optimizer_step</span></code> if you need the unscaled gradients.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-before-zero-grad">
<h4>on_before_zero_grad<a class="headerlink" href="#on-before-zero-grad" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_before_zero_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Called after <code class="docutils literal notranslate"><span class="pre">training_step()</span></code> and before <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>.</p>
<p>Called in the training loop after taking an optimizer step and before zeroing grads.
Good place to inspect weight information with weights updated.</p>
<p>This is where it is called:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">optimizer</span> <span class="ow">in</span> <span class="n">optimizers</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">training_step</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

    <span class="n">model</span><span class="o">.</span><span class="n">on_before_zero_grad</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span> <span class="c1"># &lt; ---- called here</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.on_before_zero_grad.params.optimizer"></span><strong>optimizer</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.on_before_zero_grad.params.optimizer">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) – The optimizer for which grads should be zeroed.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-fit-start">
<h4>on_fit_start<a class="headerlink" href="#on-fit-start" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_fit_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Called at the very beginning of fit.</p>
<p>If on DDP it is called on every process</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-fit-end">
<h4>on_fit_end<a class="headerlink" href="#on-fit-end" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_fit_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Called at the very end of fit.</p>
<p>If on DDP it is called on every process</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-load-checkpoint">
<h4>on_load_checkpoint<a class="headerlink" href="#on-load-checkpoint" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_load_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Called by Lightning to restore your model.
If you saved something with <code class="xref py py-meth docutils literal notranslate"><span class="pre">on_save_checkpoint()</span></code> this is your chance to restore this.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.on_load_checkpoint.params.checkpoint"></span><strong>checkpoint</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.on_load_checkpoint.params.checkpoint">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – Loaded checkpoint</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_load_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">):</span>
    <span class="c1"># 99% of the time you don&#39;t need to implement this method</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">something_cool_i_want_to_save</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;something_cool_i_want_to_save&#39;</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning auto-restores global step, epoch, and train state including amp scaling.
There is no need for you to restore anything regarding training.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-save-checkpoint">
<h4>on_save_checkpoint<a class="headerlink" href="#on-save-checkpoint" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_save_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Called by Lightning when saving a checkpoint to give you a chance to store anything
else you might want to save.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.on_save_checkpoint.params.checkpoint"></span><strong>checkpoint</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.on_save_checkpoint.params.checkpoint">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – The full checkpoint dictionary before it gets dumped to a file.
Implementations of this hook can insert additional data into this dictionary.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">):</span>
    <span class="c1"># 99% of use cases you don&#39;t need to implement this method</span>
    <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;something_cool_i_want_to_save&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">my_cool_pickable_object</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning saves all aspects of training (epoch, global step, etc…)
including amp scaling.
There is no need for you to store anything about training.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="load-from-checkpoint">
<h4>load_from_checkpoint<a class="headerlink" href="#load-from-checkpoint" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">load_from_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">map_location</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hparams_file</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint
it stores the arguments passed to <code class="docutils literal notranslate"><span class="pre">__init__</span></code>  in the checkpoint under <code class="docutils literal notranslate"><span class="pre">&quot;hyper_parameters&quot;</span></code>.</p>
<p>Any arguments specified through **kwargs will override args stored in <code class="docutils literal notranslate"><span class="pre">&quot;hyper_parameters&quot;</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.load_from_checkpoint.params.checkpoint_path"></span><strong>checkpoint_path</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.load_from_checkpoint.params.checkpoint_path">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">IO</span></code>]) – Path to checkpoint. This can also be a URL, or file-like object</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.load_from_checkpoint.params.map_location"></span><strong>map_location</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.load_from_checkpoint.params.map_location">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">device</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – If your checkpoint saved a GPU model and you now load on CPUs
or a different number of GPUs, use this to map to the new setup.
The behaviour is the same as in <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.load()</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.load_from_checkpoint.params.hparams_file"></span><strong>hparams_file</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.load_from_checkpoint.params.hparams_file">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – <p>Optional path to a .yaml file with hierarchical structure
as in this example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">drop_prob</span><span class="p">:</span> <span class="mf">0.2</span>
<span class="n">dataloader</span><span class="p">:</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="mi">32</span>
</pre></div>
</div>
<p>You most likely won’t need this since Lightning will always save the hyperparameters
to the checkpoint.
However, if your checkpoint weights don’t have the hyperparameters saved,
use this method to pass in a .yaml file with the hparams you’d like to use.
These will be converted into a <code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code> and passed into your
<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code> for use.</p>
<p>If your model’s <code class="docutils literal notranslate"><span class="pre">hparams</span></code> argument is <code class="xref py py-class docutils literal notranslate"><span class="pre">Namespace</span></code>
and .yaml file has hierarchical structure, you need to refactor your model to treat
<code class="docutils literal notranslate"><span class="pre">hparams</span></code> as <code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code>.</p>
</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.load_from_checkpoint.params.strict"></span><strong>strict</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.load_from_checkpoint.params.strict">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – Whether to strictly enforce that the keys in <code class="xref py py-attr docutils literal notranslate"><span class="pre">checkpoint_path</span></code> match the keys
returned by this module’s state dict.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.load_from_checkpoint.params.kwargs"></span><strong>kwargs</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.load_from_checkpoint.params.kwargs">¶</a> – Any extra keyword args needed to init the model. Can also be used to override saved
hyperparameter values.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code> instance with loaded weights and hyperparameters (if available).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">load_from_checkpoint</span></code> is a <strong>class</strong> method. You should use your <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code>
<strong>class</strong> to call it instead of the <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code> instance.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># load weights without mapping ...</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span><span class="s1">&#39;path/to/checkpoint.ckpt&#39;</span><span class="p">)</span>

<span class="c1"># or load weights mapping all weights from GPU 1 to GPU 0 ...</span>
<span class="n">map_location</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">:</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">}</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span>
    <span class="s1">&#39;path/to/checkpoint.ckpt&#39;</span><span class="p">,</span>
    <span class="n">map_location</span><span class="o">=</span><span class="n">map_location</span>
<span class="p">)</span>

<span class="c1"># or load weights and hyperparameters from separate files.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span>
    <span class="s1">&#39;path/to/checkpoint.ckpt&#39;</span><span class="p">,</span>
    <span class="n">hparams_file</span><span class="o">=</span><span class="s1">&#39;/path/to/hparams_file.yaml&#39;</span>
<span class="p">)</span>

<span class="c1"># override some of the params with new values</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span>
    <span class="n">PATH</span><span class="p">,</span>
    <span class="n">num_layers</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">pretrained_ckpt_path</span><span class="o">=</span><span class="n">NEW_PATH</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># predict</span>
<span class="n">pretrained_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">pretrained_model</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">pretrained_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="on-hpc-save">
<h4>on_hpc_save<a class="headerlink" href="#on-hpc-save" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_hpc_save</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Hook to do whatever you need right before Slurm manager saves the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.on_hpc_save.params.checkpoint"></span><strong>checkpoint</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.on_hpc_save.params.checkpoint">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – A dictionary in which you can save variables to save in a checkpoint.
Contents need to be pickleable.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.6: </span>This method is deprecated in v1.6 and will be removed in v1.8.
Please use <code class="docutils literal notranslate"><span class="pre">LightningModule.on_save_checkpoint</span></code> instead.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-hpc-load">
<h4>on_hpc_load<a class="headerlink" href="#on-hpc-load" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_hpc_load</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Hook to do whatever you need right before Slurm manager loads the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.on_hpc_load.params.checkpoint"></span><strong>checkpoint</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.on_hpc_load.params.checkpoint">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – A dictionary with variables from the checkpoint.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.6: </span>This method is deprecated in v1.6 and will be removed in v1.8.
Please use <code class="docutils literal notranslate"><span class="pre">LightningModule.on_load_checkpoint</span></code> instead.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-train-start">
<h4>on_train_start<a class="headerlink" href="#on-train-start" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_train_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Called at the beginning of training after sanity check.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-train-end">
<h4>on_train_end<a class="headerlink" href="#on-train-end" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_train_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Called at the end of training before logger experiment is closed.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-validation-start">
<h4>on_validation_start<a class="headerlink" href="#on-validation-start" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_validation_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Called at the beginning of validation.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-validation-end">
<h4>on_validation_end<a class="headerlink" href="#on-validation-end" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_validation_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Called at the end of validation.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-test-batch-start">
<h4>on_test_batch_start<a class="headerlink" href="#on-test-batch-start" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_test_batch_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Called in the test loop before anything happens for that batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.on_test_batch_start.params.batch"></span><strong>batch</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.on_test_batch_start.params.batch">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the test DataLoader.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.on_test_batch_start.params.batch_idx"></span><strong>batch_idx</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.on_test_batch_start.params.batch_idx">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.on_test_batch_start.params.dataloader_idx"></span><strong>dataloader_idx</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.on_test_batch_start.params.dataloader_idx">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-test-batch-end">
<h4>on_test_batch_end<a class="headerlink" href="#on-test-batch-end" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_test_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Called in the test loop after the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.on_test_batch_end.params.outputs"></span><strong>outputs</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.on_test_batch_end.params.outputs">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – The outputs of test_step_end(test_step(x))</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.on_test_batch_end.params.batch"></span><strong>batch</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.on_test_batch_end.params.batch">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the test DataLoader.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.on_test_batch_end.params.batch_idx"></span><strong>batch_idx</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.on_test_batch_end.params.batch_idx">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.on_test_batch_end.params.dataloader_idx"></span><strong>dataloader_idx</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.on_test_batch_end.params.dataloader_idx">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-test-epoch-start">
<h4>on_test_epoch_start<a class="headerlink" href="#on-test-epoch-start" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_test_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Called in the test loop at the very beginning of the epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-test-epoch-end">
<h4>on_test_epoch_end<a class="headerlink" href="#on-test-epoch-end" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_test_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Called in the test loop at the very end of the epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-test-start">
<h4>on_test_start<a class="headerlink" href="#on-test-start" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_test_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Called at the beginning of testing.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-test-end">
<h4>on_test_end<a class="headerlink" href="#on-test-end" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_test_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Called at the end of testing.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-predict-batch-start">
<h4>on_predict_batch_start<a class="headerlink" href="#on-predict-batch-start" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_predict_batch_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Called in the predict loop before anything happens for that batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.on_predict_batch_start.params.batch"></span><strong>batch</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.on_predict_batch_start.params.batch">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the test DataLoader.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.on_predict_batch_start.params.batch_idx"></span><strong>batch_idx</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.on_predict_batch_start.params.batch_idx">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.on_predict_batch_start.params.dataloader_idx"></span><strong>dataloader_idx</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.on_predict_batch_start.params.dataloader_idx">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-predict-batch-end">
<h4>on_predict_batch_end<a class="headerlink" href="#on-predict-batch-end" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_predict_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Called in the predict loop after the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.on_predict_batch_end.params.outputs"></span><strong>outputs</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.on_predict_batch_end.params.outputs">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – The outputs of predict_step_end(test_step(x))</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.on_predict_batch_end.params.batch"></span><strong>batch</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.on_predict_batch_end.params.batch">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the test DataLoader.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.on_predict_batch_end.params.batch_idx"></span><strong>batch_idx</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.on_predict_batch_end.params.batch_idx">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.on_predict_batch_end.params.dataloader_idx"></span><strong>dataloader_idx</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.on_predict_batch_end.params.dataloader_idx">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-predict-epoch-start">
<h4>on_predict_epoch_start<a class="headerlink" href="#on-predict-epoch-start" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_predict_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Called at the beginning of predicting.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-predict-epoch-end">
<h4>on_predict_epoch_end<a class="headerlink" href="#on-predict-epoch-end" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_predict_epoch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">results</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Called at the end of predicting.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-predict-start">
<h4>on_predict_start<a class="headerlink" href="#on-predict-start" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_predict_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Called at the beginning of predicting.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-predict-end">
<h4>on_predict_end<a class="headerlink" href="#on-predict-end" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_predict_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Called at the end of predicting.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-train-batch-start">
<h4>on_train_batch_start<a class="headerlink" href="#on-train-batch-start" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_train_batch_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unused</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Called in the training loop before anything happens for that batch.</p>
<p>If you return -1 here, you will skip training for the rest of the current epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.on_train_batch_start.params.batch"></span><strong>batch</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.on_train_batch_start.params.batch">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the training DataLoader.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.on_train_batch_start.params.batch_idx"></span><strong>batch_idx</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.on_train_batch_start.params.batch_idx">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.on_train_batch_start.params.unused"></span><strong>unused</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.on_train_batch_start.params.unused">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Deprecated argument. Will be removed in v1.7.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-train-batch-end">
<h4>on_train_batch_end<a class="headerlink" href="#on-train-batch-end" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_train_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unused</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Called in the training loop after the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.on_train_batch_end.params.outputs"></span><strong>outputs</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.on_train_batch_end.params.outputs">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – The outputs of training_step_end(training_step(x))</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.on_train_batch_end.params.batch"></span><strong>batch</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.on_train_batch_end.params.batch">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the training DataLoader.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.on_train_batch_end.params.batch_idx"></span><strong>batch_idx</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.on_train_batch_end.params.batch_idx">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.on_train_batch_end.params.unused"></span><strong>unused</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.on_train_batch_end.params.unused">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Deprecated argument. Will be removed in v1.7.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-train-epoch-start">
<h4>on_train_epoch_start<a class="headerlink" href="#on-train-epoch-start" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_train_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Called in the training loop at the very beginning of the epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-train-epoch-end">
<h4>on_train_epoch_end<a class="headerlink" href="#on-train-epoch-end" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_train_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Called in the training loop at the very end of the epoch.</p>
<p>To access all batch outputs at the end of the epoch, either:</p>
<ol class="arabic simple">
<li><p>Implement <cite>training_epoch_end</cite> in the LightningModule OR</p></li>
<li><p>Cache data across steps on the attribute(s) of the <cite>LightningModule</cite> and access them in this hook</p></li>
</ol>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-validation-batch-start">
<h4>on_validation_batch_start<a class="headerlink" href="#on-validation-batch-start" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_validation_batch_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Called in the validation loop before anything happens for that batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.on_validation_batch_start.params.batch"></span><strong>batch</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.on_validation_batch_start.params.batch">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the validation DataLoader.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.on_validation_batch_start.params.batch_idx"></span><strong>batch_idx</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.on_validation_batch_start.params.batch_idx">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.on_validation_batch_start.params.dataloader_idx"></span><strong>dataloader_idx</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.on_validation_batch_start.params.dataloader_idx">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-validation-batch-end">
<h4>on_validation_batch_end<a class="headerlink" href="#on-validation-batch-end" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_validation_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Called in the validation loop after the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.on_validation_batch_end.params.outputs"></span><strong>outputs</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.on_validation_batch_end.params.outputs">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – The outputs of validation_step_end(validation_step(x))</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.on_validation_batch_end.params.batch"></span><strong>batch</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.on_validation_batch_end.params.batch">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the validation DataLoader.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.on_validation_batch_end.params.batch_idx"></span><strong>batch_idx</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.on_validation_batch_end.params.batch_idx">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.on_validation_batch_end.params.dataloader_idx"></span><strong>dataloader_idx</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.on_validation_batch_end.params.dataloader_idx">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-validation-epoch-start">
<h4>on_validation_epoch_start<a class="headerlink" href="#on-validation-epoch-start" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_validation_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Called in the validation loop at the very beginning of the epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-validation-epoch-end">
<h4>on_validation_epoch_end<a class="headerlink" href="#on-validation-epoch-end" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_validation_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Called in the validation loop at the very end of the epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-post-move-to-device">
<h4>on_post_move_to_device<a class="headerlink" href="#on-post-move-to-device" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_post_move_to_device</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Called in the <code class="docutils literal notranslate"><span class="pre">parameter_validation</span></code> decorator after
<code class="xref py py-meth docutils literal notranslate"><span class="pre">to()</span></code> is called. This is a good place to tie weights between
modules after moving them to a device. Can be used when training models with weight sharing properties on
TPU.</p>
<p>Addresses the handling of shared weights on TPU:
<a class="reference external" href="https://github.com/pytorch/xla/blob/master/TROUBLESHOOTING.md#xla-tensor-quirks">https://github.com/pytorch/xla/blob/master/TROUBLESHOOTING.md#xla-tensor-quirks</a></p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_post_move_to_device</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">weight</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="configure-sharded-model">
<h4>configure_sharded_model<a class="headerlink" href="#configure-sharded-model" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">configure_sharded_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Hook to create modules in a distributed aware context. This is useful for when using sharded plugins,
where we’d like to shard the model instantly, which is useful for extremely large models which can save
memory and initialization time.</p>
<p>This hook is called during each of fit/val/test/predict stages in the same process, so ensure that
implementation of this hook is idempotent.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-validation-model-eval">
<h4>on_validation_model_eval<a class="headerlink" href="#on-validation-model-eval" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_validation_model_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Sets the model to eval during the val loop.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-validation-model-train">
<h4>on_validation_model_train<a class="headerlink" href="#on-validation-model-train" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_validation_model_train</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Sets the model to train during the val loop.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-test-model-eval">
<h4>on_test_model_eval<a class="headerlink" href="#on-test-model-eval" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_test_model_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Sets the model to eval during the test loop.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-test-model-train">
<h4>on_test_model_train<a class="headerlink" href="#on-test-model-train" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_test_model_train</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Sets the model to train during the test loop.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-before-optimizer-step">
<h4>on_before_optimizer_step<a class="headerlink" href="#on-before-optimizer-step" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_before_optimizer_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Called before <code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code>.</p>
<p>If using gradient accumulation, the hook is called once the gradients have been accumulated.
See: <code class="xref py py-paramref docutils literal notranslate"><span class="pre">accumulate_grad_batches</span></code>.</p>
<p>If using native AMP, the loss will be unscaled before calling this hook.
See these <a class="reference external" href="https://pytorch.org/docs/stable/notes/amp_examples.html#working-with-unscaled-gradients">docs</a>
for more information on the scaling of gradients.</p>
<p>If clipping gradients, the gradients will not have been clipped yet.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.on_before_optimizer_step.params.optimizer"></span><strong>optimizer</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.on_before_optimizer_step.params.optimizer">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) – Current optimizer being used.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.on_before_optimizer_step.params.optimizer_idx"></span><strong>optimizer_idx</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.on_before_optimizer_step.params.optimizer_idx">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Index of the current optimizer being used.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_before_optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">):</span>
    <span class="c1"># example to inspect gradient information in tensorboard</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span> <span class="o">%</span> <span class="mi">25</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># don&#39;t make the tf file huge</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_histogram</span><span class="p">(</span>
                <span class="n">tag</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">v</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">global_step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span>
            <span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="configure-gradient-clipping">
<h4>configure_gradient_clipping<a class="headerlink" href="#configure-gradient-clipping" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">configure_gradient_clipping</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_clip_val</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_clip_algorithm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_lightning/core/lightning.html#LightningModule.configure_gradient_clipping"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Perform gradient clipping for the optimizer parameters. Called before <code class="xref py py-meth docutils literal notranslate"><span class="pre">optimizer_step()</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.configure_gradient_clipping.params.optimizer"></span><strong>optimizer</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.configure_gradient_clipping.params.optimizer">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) – Current optimizer being used.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.configure_gradient_clipping.params.optimizer_idx"></span><strong>optimizer_idx</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.configure_gradient_clipping.params.optimizer_idx">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Index of the current optimizer being used.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.configure_gradient_clipping.params.gradient_clip_val"></span><strong>gradient_clip_val</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.configure_gradient_clipping.params.gradient_clip_val">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – The value at which to clip gradients. By default value passed in Trainer
will be available here.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.configure_gradient_clipping.params.gradient_clip_algorithm"></span><strong>gradient_clip_algorithm</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.configure_gradient_clipping.params.gradient_clip_algorithm">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – The gradient clipping algorithm to use. By default value
passed in Trainer will be available here.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Perform gradient clipping on gradients associated with discriminator (optimizer_idx=1) in GAN</span>
<span class="k">def</span> <span class="nf">configure_gradient_clipping</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">,</span> <span class="n">gradient_clip_val</span><span class="p">,</span> <span class="n">gradient_clip_algorithm</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># Lightning will handle the gradient clipping</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">clip_gradients</span><span class="p">(</span>
            <span class="n">optimizer</span><span class="p">,</span>
            <span class="n">gradient_clip_val</span><span class="o">=</span><span class="n">gradient_clip_val</span><span class="p">,</span>
            <span class="n">gradient_clip_algorithm</span><span class="o">=</span><span class="n">gradient_clip_algorithm</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># implement your own custom logic to clip gradients for generator (optimizer_idx=0)</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="optimizer-step">
<h4>optimizer_step<a class="headerlink" href="#optimizer-step" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">optimizer_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_closure</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_tpu</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">using_native_amp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">using_lbfgs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_lightning/core/lightning.html#LightningModule.optimizer_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Override this method to adjust the default way the <code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code> calls
each optimizer.</p>
<p>By default, Lightning calls <code class="docutils literal notranslate"><span class="pre">step()</span></code> and <code class="docutils literal notranslate"><span class="pre">zero_grad()</span></code> as shown in the example once per optimizer.
This method (and <code class="docutils literal notranslate"><span class="pre">zero_grad()</span></code>) won’t be called during the accumulation phase when
<code class="docutils literal notranslate"><span class="pre">Trainer(accumulate_grad_batches</span> <span class="pre">!=</span> <span class="pre">1)</span></code>. Overriding this hook has no benefit with manual optimization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.optimizer_step.params.epoch"></span><strong>epoch</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.optimizer_step.params.epoch">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Current epoch</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.optimizer_step.params.batch_idx"></span><strong>batch_idx</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.optimizer_step.params.batch_idx">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Index of current batch</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.optimizer_step.params.optimizer"></span><strong>optimizer</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.optimizer_step.params.optimizer">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code>]) – A PyTorch optimizer</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.optimizer_step.params.optimizer_idx"></span><strong>optimizer_idx</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.optimizer_step.params.optimizer_idx">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – If you used multiple optimizers, this indexes into that list.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.optimizer_step.params.optimizer_closure"></span><strong>optimizer_closure</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.optimizer_step.params.optimizer_closure">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[], <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – The optimizer closure. This closure must be executed as it includes the
calls to <code class="docutils literal notranslate"><span class="pre">training_step()</span></code>, <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>, and <code class="docutils literal notranslate"><span class="pre">backward()</span></code>.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.optimizer_step.params.on_tpu"></span><strong>on_tpu</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.optimizer_step.params.on_tpu">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – <code class="docutils literal notranslate"><span class="pre">True</span></code> if TPU backward is required</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.optimizer_step.params.using_native_amp"></span><strong>using_native_amp</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.optimizer_step.params.using_native_amp">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – <code class="docutils literal notranslate"><span class="pre">True</span></code> if using native amp</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.optimizer_step.params.using_lbfgs"></span><strong>using_lbfgs</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.optimizer_step.params.using_lbfgs">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – True if the matching optimizer is <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.LBFGS</span></code></p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT</span>
<span class="k">def</span> <span class="nf">optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">,</span>
                   <span class="n">optimizer_closure</span><span class="p">,</span> <span class="n">on_tpu</span><span class="p">,</span> <span class="n">using_native_amp</span><span class="p">,</span> <span class="n">using_lbfgs</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="o">=</span><span class="n">optimizer_closure</span><span class="p">)</span>

<span class="c1"># Alternating schedule for optimizer steps (i.e.: GANs)</span>
<span class="k">def</span> <span class="nf">optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">,</span>
                   <span class="n">optimizer_closure</span><span class="p">,</span> <span class="n">on_tpu</span><span class="p">,</span> <span class="n">using_native_amp</span><span class="p">,</span> <span class="n">using_lbfgs</span><span class="p">):</span>
    <span class="c1"># update generator opt every step</span>
    <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="o">=</span><span class="n">optimizer_closure</span><span class="p">)</span>

    <span class="c1"># update discriminator opt every 2 steps</span>
    <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">batch_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span> <span class="p">:</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="o">=</span><span class="n">optimizer_closure</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># call the closure by itself to run `training_step` + `backward` without an optimizer step</span>
            <span class="n">optimizer_closure</span><span class="p">()</span>

    <span class="c1"># ...</span>
    <span class="c1"># add as many optimizers as you want</span>
</pre></div>
</div>
<p>Here’s another example showing how to use this for more advanced things such as
learning rate warm-up:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># learning rate warm-up</span>
<span class="k">def</span> <span class="nf">optimizer_step</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">epoch</span><span class="p">,</span>
    <span class="n">batch_idx</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">,</span>
    <span class="n">optimizer_idx</span><span class="p">,</span>
    <span class="n">optimizer_closure</span><span class="p">,</span>
    <span class="n">on_tpu</span><span class="p">,</span>
    <span class="n">using_native_amp</span><span class="p">,</span>
    <span class="n">using_lbfgs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="c1"># update params</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="o">=</span><span class="n">optimizer_closure</span><span class="p">)</span>

    <span class="c1"># manually warm up lr without a scheduler</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span> <span class="o">&lt;</span> <span class="mi">500</span><span class="p">:</span>
        <span class="n">lr_scale</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mf">500.0</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">pg</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="n">pg</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr_scale</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="optimizer-zero-grad">
<h4>optimizer_zero_grad<a class="headerlink" href="#optimizer-zero-grad" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">optimizer_zero_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_lightning/core/lightning.html#LightningModule.optimizer_zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Override this method to change the default behaviour of <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.optimizer_zero_grad.params.epoch"></span><strong>epoch</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.optimizer_zero_grad.params.epoch">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Current epoch</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.optimizer_zero_grad.params.batch_idx"></span><strong>batch_idx</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.optimizer_zero_grad.params.batch_idx">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Index of current batch</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.optimizer_zero_grad.params.optimizer"></span><strong>optimizer</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.optimizer_zero_grad.params.optimizer">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) – A PyTorch optimizer</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.optimizer_zero_grad.params.optimizer_idx"></span><strong>optimizer_idx</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.optimizer_zero_grad.params.optimizer_idx">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – If you used multiple optimizers this indexes into that list.</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT</span>
<span class="k">def</span> <span class="nf">optimizer_zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

<span class="c1"># Set gradients to `None` instead of zero to improve performance.</span>
<span class="k">def</span> <span class="nf">optimizer_zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>See <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.optim.Optimizer.zero_grad()</span></code> for the explanation of the above example.</p>
</dd></dl>

</section>
<section id="prepare-data">
<h4>prepare_data<a class="headerlink" href="#prepare-data" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">prepare_data</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Use this to download and prepare data. Downloading and saving data with multiple processes (distributed
settings) will result in corrupted data. Lightning ensures this method is called only within a single
process, so you can safely add your downloading logic within.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>DO NOT set state to the model (use <code class="docutils literal notranslate"><span class="pre">setup</span></code> instead)
since this is NOT called on every device</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">prepare_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># good</span>
    <span class="n">download_data</span><span class="p">()</span>
    <span class="n">tokenize</span><span class="p">()</span>
    <span class="n">etc</span><span class="p">()</span>

    <span class="c1"># bad</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">split</span> <span class="o">=</span> <span class="n">data_split</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">some_state</span> <span class="o">=</span> <span class="n">some_other_state</span><span class="p">()</span>
</pre></div>
</div>
<p>In DDP <code class="docutils literal notranslate"><span class="pre">prepare_data</span></code> can be called in two ways (using Trainer(prepare_data_per_node)):</p>
<ol class="arabic simple">
<li><p>Once per node. This is the default and is only called on LOCAL_RANK=0.</p></li>
<li><p>Once in total. Only called on GLOBAL_RANK=0.</p></li>
</ol>
<p>See <a class="reference internal" href="#prepare-data-per-node"><span class="std std-ref">prepare_data_per_node</span></a>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT</span>
<span class="c1"># called once per node on LOCAL_RANK=0 of that node</span>
<span class="n">Trainer</span><span class="p">(</span><span class="n">prepare_data_per_node</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># call on GLOBAL_RANK=0 (great for shared file systems)</span>
<span class="n">Trainer</span><span class="p">(</span><span class="n">prepare_data_per_node</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>This is called before requesting the dataloaders:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">prepare_data</span><span class="p">()</span>
<span class="n">initialize_distributed</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">stage</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train_dataloader</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">val_dataloader</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">test_dataloader</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="setup">
<h4>setup<a class="headerlink" href="#setup" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">setup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Called at the beginning of fit (train + validate), validate, test, or predict. This is a good hook when
you need to build models dynamically or adjust something about them. This hook is called on every process
when using DDP.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.setup.params.stage"></span><strong>stage</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.setup.params.stage">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – either <code class="docutils literal notranslate"><span class="pre">'fit'</span></code>, <code class="docutils literal notranslate"><span class="pre">'validate'</span></code>, <code class="docutils literal notranslate"><span class="pre">'test'</span></code>, or <code class="docutils literal notranslate"><span class="pre">'predict'</span></code></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LitModel</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">prepare_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">download_data</span><span class="p">()</span>
        <span class="n">tokenize</span><span class="p">()</span>

        <span class="c1"># don&#39;t do this</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">something</span> <span class="o">=</span> <span class="k">else</span>

    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stage</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">num_classes</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="tbptt-split-batch">
<h4>tbptt_split_batch<a class="headerlink" href="#tbptt-split-batch" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">tbptt_split_batch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">split_size</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_lightning/core/lightning.html#LightningModule.tbptt_split_batch"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>When using truncated backpropagation through time, each batch must be split along the
time dimension. Lightning handles this by default, but for custom behavior override
this function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.tbptt_split_batch.params.batch"></span><strong>batch</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.tbptt_split_batch.params.batch">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – Current batch</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.tbptt_split_batch.params.split_size"></span><strong>split_size</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.tbptt_split_batch.params.split_size">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The size of the split</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>List of batch splits. Each split will be passed to <code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code> to enable truncated
back propagation through time. The default implementation splits root level Tensors and
Sequences at dim=1 (i.e. time dim). It assumes that each time dim is the same length.</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">tbptt_split_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">split_size</span><span class="p">):</span>
    <span class="n">splits</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">time_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">split_size</span><span class="p">):</span>
        <span class="n">batch_split</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="n">split_x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="n">t</span><span class="p">:</span><span class="n">t</span> <span class="o">+</span> <span class="n">split_size</span><span class="p">]</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">Sequence</span><span class="p">):</span>
                <span class="n">split_x</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">batch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
                  <span class="n">split_x</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">][</span><span class="n">t</span><span class="p">:</span><span class="n">t</span> <span class="o">+</span> <span class="n">split_size</span><span class="p">]</span>
            <span class="n">batch_split</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">split_x</span><span class="p">)</span>
        <span class="n">splits</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch_split</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">splits</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Called in the training loop after
<code class="xref py py-meth docutils literal notranslate"><span class="pre">on_train_batch_start()</span></code>
if <code class="xref py py-paramref docutils literal notranslate"><span class="pre">truncated_bptt_steps</span></code> &gt; 0.
Each returned batch split is passed separately to <code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code>.</p>
</div>
</dd></dl>

</section>
<section id="teardown">
<h4>teardown<a class="headerlink" href="#teardown" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">teardown</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Called at the end of fit (train + validate), validate, test, or predict.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.teardown.params.stage"></span><strong>stage</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.teardown.params.stage">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – either <code class="docutils literal notranslate"><span class="pre">'fit'</span></code>, <code class="docutils literal notranslate"><span class="pre">'validate'</span></code>, <code class="docutils literal notranslate"><span class="pre">'test'</span></code>, or <code class="docutils literal notranslate"><span class="pre">'predict'</span></code></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="train-dataloader">
<h4>train_dataloader<a class="headerlink" href="#train-dataloader" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">train_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Implement one or more PyTorch DataLoaders for training.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]], <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]], <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]], <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]]]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A collection of <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> specifying training samples.
In the case of multiple dataloaders, please see this <a class="reference internal" href="../guides/data.html#multiple-dataloaders"><span class="std std-ref">section</span></a>.</p>
</dd>
</dl>
<p>The dataloader you return will not be reloaded unless you set
<code class="xref py py-paramref docutils literal notranslate"><span class="pre">reload_dataloaders_every_n_epochs</span></code> to
a positive integer.</p>
<p>For data processing use the following pattern:</p>
<blockquote>
<div><ul class="simple">
<li><p>download in <code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></p></li>
<li><p>process and split in <code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></p></li>
</ul>
</div></blockquote>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>do not assign state in prepare_data</p>
</div>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># single dataloader</span>
<span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;/path/to/mnist/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span>
                    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">loader</span>

<span class="c1"># multiple dataloaders, return as list</span>
<span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">mnist</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">cifar</span> <span class="o">=</span> <span class="n">CIFAR</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">mnist_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">mnist</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="n">cifar_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">cifar</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="c1"># each batch will be a list of tensors: [batch_mnist, batch_cifar]</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">mnist_loader</span><span class="p">,</span> <span class="n">cifar_loader</span><span class="p">]</span>

<span class="c1"># multiple dataloader, return as dict</span>
<span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">mnist</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">cifar</span> <span class="o">=</span> <span class="n">CIFAR</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">mnist_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">mnist</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="n">cifar_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">cifar</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="c1"># each batch will be a dict of tensors: {&#39;mnist&#39;: batch_mnist, &#39;cifar&#39;: batch_cifar}</span>
    <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;mnist&#39;</span><span class="p">:</span> <span class="n">mnist_loader</span><span class="p">,</span> <span class="s1">&#39;cifar&#39;</span><span class="p">:</span> <span class="n">cifar_loader</span><span class="p">}</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="val-dataloader">
<h4>val_dataloader<a class="headerlink" href="#val-dataloader" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">val_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Implement one or multiple PyTorch DataLoaders for validation.</p>
<p>The dataloader you return will not be reloaded unless you set
<code class="xref py py-paramref docutils literal notranslate"><span class="pre">reload_dataloaders_every_n_epochs</span></code> to
a positive integer.</p>
<p>It’s recommended that all data downloads and preparation happen in <code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code>.</p>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">validate()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware
There is no need to set it yourself.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> or a sequence of them specifying validation samples.</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">val_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;/path/to/mnist/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">loader</span>

<span class="c1"># can also return multiple dataloaders</span>
<span class="k">def</span> <span class="nf">val_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">loader_a</span><span class="p">,</span> <span class="n">loader_b</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">loader_n</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need a validation dataset and a <code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code>, you don’t need to
implement this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the case where you return multiple validation dataloaders, the <code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code>
will have an argument <code class="docutils literal notranslate"><span class="pre">dataloader_idx</span></code> which matches the order here.</p>
</div>
</dd></dl>

</section>
<section id="test-dataloader">
<h4>test_dataloader<a class="headerlink" href="#test-dataloader" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">test_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Implement one or multiple PyTorch DataLoaders for testing.</p>
<p>For data processing use the following pattern:</p>
<blockquote>
<div><ul class="simple">
<li><p>download in <code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></p></li>
<li><p>process and split in <code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></p></li>
</ul>
</div></blockquote>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>do not assign state in prepare_data</p>
</div>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">test()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> or a sequence of them specifying testing samples.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;/path/to/mnist/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span>
                    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">loader</span>

<span class="c1"># can also return multiple dataloaders</span>
<span class="k">def</span> <span class="nf">test_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">loader_a</span><span class="p">,</span> <span class="n">loader_b</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">loader_n</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need a test dataset and a <code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code>, you don’t need to implement
this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the case where you return multiple test dataloaders, the <code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code>
will have an argument <code class="docutils literal notranslate"><span class="pre">dataloader_idx</span></code> which matches the order here.</p>
</div>
</dd></dl>

</section>
<section id="predict-dataloader">
<h4>predict_dataloader<a class="headerlink" href="#predict-dataloader" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">predict_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Implement one or multiple PyTorch DataLoaders for prediction.</p>
<p>It’s recommended that all data downloads and preparation happen in <code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code>.</p>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">predict()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware
There is no need to set it yourself.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> or a sequence of them specifying prediction samples.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the case where you return multiple prediction dataloaders, the <code class="xref py py-meth docutils literal notranslate"><span class="pre">predict_step()</span></code>
will have an argument <code class="docutils literal notranslate"><span class="pre">dataloader_idx</span></code> which matches the order here.</p>
</div>
</dd></dl>

</section>
<section id="on-train-dataloader">
<h4>on_train_dataloader<a class="headerlink" href="#on-train-dataloader" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_train_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Called before requesting the train dataloader.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_train_dataloader()</span></code> is deprecated and will be removed in v1.7.0.
Please use <code class="xref py py-meth docutils literal notranslate"><span class="pre">train_dataloader()</span></code> directly.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-val-dataloader">
<h4>on_val_dataloader<a class="headerlink" href="#on-val-dataloader" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_val_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Called before requesting the val dataloader.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_val_dataloader()</span></code> is deprecated and will be removed in v1.7.0.
Please use <code class="xref py py-meth docutils literal notranslate"><span class="pre">val_dataloader()</span></code> directly.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-test-dataloader">
<h4>on_test_dataloader<a class="headerlink" href="#on-test-dataloader" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_test_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Called before requesting the test dataloader.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_test_dataloader()</span></code> is deprecated and will be removed in v1.7.0.
Please use <code class="xref py py-meth docutils literal notranslate"><span class="pre">test_dataloader()</span></code> directly.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="on-predict-dataloader">
<h4>on_predict_dataloader<a class="headerlink" href="#on-predict-dataloader" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_predict_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Called before requesting the predict dataloader.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_predict_dataloader()</span></code> is deprecated and will be removed in v1.7.0.
Please use <code class="xref py py-meth docutils literal notranslate"><span class="pre">predict_dataloader()</span></code> directly.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="transfer-batch-to-device">
<h4>transfer_batch_to_device<a class="headerlink" href="#transfer-batch-to-device" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">transfer_batch_to_device</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Override this hook if your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code> returns tensors wrapped in a custom
data structure.</p>
<p>The data types listed below (and any arbitrary nesting of them) are supported out of the box:</p>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code> or anything that implements <cite>.to(…)</cite></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">list</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torchtext.data.batch.Batch</span></code></p></li>
</ul>
<p>For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, …).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This hook should only transfer the data and not modify it, nor should it move the data to
any other device than the one passed in as argument (unless you know what you are doing).
To check the current state of execution of this hook you can use
<code class="docutils literal notranslate"><span class="pre">self.trainer.training/testing/validating/predicting</span></code> so that you can
add different logic as per your requirement.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This hook only runs on single GPU training and DDP (no data-parallel).
Data-Parallel support will come in near future.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.transfer_batch_to_device.params.batch"></span><strong>batch</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.transfer_batch_to_device.params.batch">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – A batch of data that needs to be transferred to a new device.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.transfer_batch_to_device.params.device"></span><strong>device</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.transfer_batch_to_device.params.device">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">device</span></code>) – The target device as defined in PyTorch.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.transfer_batch_to_device.params.dataloader_idx"></span><strong>dataloader_idx</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.transfer_batch_to_device.params.dataloader_idx">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The index of the dataloader to which the batch belongs.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A reference to the data on the new device.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">transfer_batch_to_device</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">CustomBatch</span><span class="p">):</span>
        <span class="c1"># move all tensors in your custom data structure to the device</span>
        <span class="n">batch</span><span class="o">.</span><span class="n">samples</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">samples</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">batch</span><span class="o">.</span><span class="n">targets</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">targets</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">dataloader_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># skip device transfer for the first dataloader or anything you wish</span>
        <span class="k">pass</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">transfer_batch_to_device</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">batch</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>MisconfigurationException</strong> – If using data-parallel, <code class="docutils literal notranslate"><span class="pre">Trainer(strategy='dp')</span></code>.</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">move_data_to_device()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">apply_to_collection()</span></code></p></li>
</ul>
</div>
</dd></dl>

</section>
<section id="on-before-batch-transfer">
<h4>on_before_batch_transfer<a class="headerlink" href="#on-before-batch-transfer" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_before_batch_transfer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Override to alter or apply batch augmentations to your batch before it is transferred to the device.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To check the current state of execution of this hook you can use
<code class="docutils literal notranslate"><span class="pre">self.trainer.training/testing/validating/predicting</span></code> so that you can
add different logic as per your requirement.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This hook only runs on single GPU training and DDP (no data-parallel).
Data-Parallel support will come in near future.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.on_before_batch_transfer.params.batch"></span><strong>batch</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.on_before_batch_transfer.params.batch">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – A batch of data that needs to be altered or augmented.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.on_before_batch_transfer.params.dataloader_idx"></span><strong>dataloader_idx</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.on_before_batch_transfer.params.dataloader_idx">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The index of the dataloader to which the batch belongs.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A batch of data</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_before_batch_transfer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">batch</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>MisconfigurationException</strong> – If using data-parallel, <code class="docutils literal notranslate"><span class="pre">Trainer(strategy='dp')</span></code>.</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_after_batch_transfer()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">transfer_batch_to_device()</span></code></p></li>
</ul>
</div>
</dd></dl>

</section>
<section id="on-after-batch-transfer">
<h4>on_after_batch_transfer<a class="headerlink" href="#on-after-batch-transfer" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">on_after_batch_transfer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Override to alter or apply batch augmentations to your batch after it is transferred to the device.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To check the current state of execution of this hook you can use
<code class="docutils literal notranslate"><span class="pre">self.trainer.training/testing/validating/predicting</span></code> so that you can
add different logic as per your requirement.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This hook only runs on single GPU training and DDP (no data-parallel).
Data-Parallel support will come in near future.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.on_after_batch_transfer.params.batch"></span><strong>batch</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.on_after_batch_transfer.params.batch">¶</a> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – A batch of data that needs to be altered or augmented.</p></li>
<li><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.on_after_batch_transfer.params.dataloader_idx"></span><strong>dataloader_idx</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.on_after_batch_transfer.params.dataloader_idx">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The index of the dataloader to which the batch belongs.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A batch of data</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_after_batch_transfer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gpu_transforms</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">batch</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>MisconfigurationException</strong> – If using data-parallel, <code class="docutils literal notranslate"><span class="pre">Trainer(strategy='dp')</span></code>.</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_before_batch_transfer()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">transfer_batch_to_device()</span></code></p></li>
</ul>
</div>
</dd></dl>

</section>
<section id="add-to-queue">
<h4>add_to_queue<a class="headerlink" href="#add-to-queue" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">add_to_queue</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">queue</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_lightning/core/lightning.html#LightningModule.add_to_queue"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Appends the <code class="xref py py-attr docutils literal notranslate"><span class="pre">trainer.callback_metrics</span></code> dictionary to the given queue. To avoid issues with memory
sharing, we cast the data to numpy.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.add_to_queue.params.queue"></span><strong>queue</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.add_to_queue.params.queue">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">_FakeQueue</span></code>) – the instance of the queue to append the data.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span>This method was deprecated in v1.5 and will be removed in v1.7.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="get-from-queue">
<h4>get_from_queue<a class="headerlink" href="#get-from-queue" title="Permalink to this headline">¶</a></h4>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">LightningModule.</span></span><span class="sig-name descname"><span class="pre">get_from_queue</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">queue</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytorch_lightning/core/lightning.html#LightningModule.get_from_queue"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Retrieve the <code class="xref py py-attr docutils literal notranslate"><span class="pre">trainer.callback_metrics</span></code> dictionary from the given queue. To preserve consistency,
we cast back the data to <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><span class="target" id="pytorch_lightning.core.lightning.LightningModule.get_from_queue.params.queue"></span><strong>queue</strong><a class="paramlink headerlink reference internal" href="#pytorch_lightning.core.lightning.LightningModule.get_from_queue.params.queue">¶</a> (<code class="xref py py-class docutils literal notranslate"><span class="pre">_FakeQueue</span></code>) – the instance of the queue from where to get the data.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span>This method was deprecated in v1.5 and will be removed in v1.7.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
</section>
</section>
</section>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="trainer.html" class="btn btn-neutral float-right" title="Trainer" accesskey="n" rel="next">Next <img src="../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="../levels/expert.html" class="btn btn-neutral" title="Expert skills" accesskey="p" rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright Copyright (c) 2018-2022, William Falcon et al...

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">LightningModule</a><ul>
<li><a class="reference internal" href="#starter-example">Starter Example</a></li>
<li><a class="reference internal" href="#training">Training</a><ul>
<li><a class="reference internal" href="#training-loop">Training Loop</a></li>
<li><a class="reference internal" href="#train-epoch-level-metrics">Train Epoch-level Metrics</a></li>
<li><a class="reference internal" href="#train-epoch-level-operations">Train Epoch-level Operations</a></li>
<li><a class="reference internal" href="#training-with-dataparallel">Training with DataParallel</a></li>
</ul>
</li>
<li><a class="reference internal" href="#validation">Validation</a><ul>
<li><a class="reference internal" href="#validation-loop">Validation Loop</a></li>
<li><a class="reference internal" href="#validation-epoch-level-metrics">Validation Epoch-level Metrics</a></li>
<li><a class="reference internal" href="#validating-with-dataparallel">Validating with DataParallel</a></li>
</ul>
</li>
<li><a class="reference internal" href="#testing">Testing</a><ul>
<li><a class="reference internal" href="#test-loop">Test Loop</a></li>
</ul>
</li>
<li><a class="reference internal" href="#inference">Inference</a><ul>
<li><a class="reference internal" href="#prediction-loop">Prediction Loop</a></li>
<li><a class="reference internal" href="#inference-in-research">Inference in Research</a></li>
<li><a class="reference internal" href="#inference-in-production">Inference in Production</a></li>
</ul>
</li>
<li><a class="reference internal" href="#child-modules">Child Modules</a></li>
<li><a class="reference internal" href="#lightningmodule-api">LightningModule API</a><ul>
<li><a class="reference internal" href="#methods">Methods</a><ul>
<li><a class="reference internal" href="#all-gather">all_gather</a></li>
<li><a class="reference internal" href="#configure-callbacks">configure_callbacks</a></li>
<li><a class="reference internal" href="#configure-optimizers">configure_optimizers</a></li>
<li><a class="reference internal" href="#forward">forward</a></li>
<li><a class="reference internal" href="#freeze">freeze</a></li>
<li><a class="reference internal" href="#log">log</a></li>
<li><a class="reference internal" href="#log-dict">log_dict</a></li>
<li><a class="reference internal" href="#lr-schedulers">lr_schedulers</a></li>
<li><a class="reference internal" href="#manual-backward">manual_backward</a></li>
<li><a class="reference internal" href="#optimizers">optimizers</a></li>
<li><a class="reference internal" href="#print">print</a></li>
<li><a class="reference internal" href="#predict-step">predict_step</a></li>
<li><a class="reference internal" href="#save-hyperparameters">save_hyperparameters</a></li>
<li><a class="reference internal" href="#toggle-optimizer">toggle_optimizer</a></li>
<li><a class="reference internal" href="#test-step">test_step</a></li>
<li><a class="reference internal" href="#test-step-end">test_step_end</a></li>
<li><a class="reference internal" href="#test-epoch-end">test_epoch_end</a></li>
<li><a class="reference internal" href="#to-onnx">to_onnx</a></li>
<li><a class="reference internal" href="#to-torchscript">to_torchscript</a></li>
<li><a class="reference internal" href="#training-step">training_step</a></li>
<li><a class="reference internal" href="#training-step-end">training_step_end</a></li>
<li><a class="reference internal" href="#training-epoch-end">training_epoch_end</a></li>
<li><a class="reference internal" href="#unfreeze">unfreeze</a></li>
<li><a class="reference internal" href="#untoggle-optimizer">untoggle_optimizer</a></li>
<li><a class="reference internal" href="#validation-step">validation_step</a></li>
<li><a class="reference internal" href="#validation-step-end">validation_step_end</a></li>
<li><a class="reference internal" href="#validation-epoch-end">validation_epoch_end</a></li>
</ul>
</li>
<li><a class="reference internal" href="#properties">Properties</a><ul>
<li><a class="reference internal" href="#current-epoch">current_epoch</a></li>
<li><a class="reference internal" href="#device">device</a></li>
<li><a class="reference internal" href="#global-rank">global_rank</a></li>
<li><a class="reference internal" href="#global-step">global_step</a></li>
<li><a class="reference internal" href="#hparams">hparams</a></li>
<li><a class="reference internal" href="#logger">logger</a></li>
<li><a class="reference internal" href="#loggers">loggers</a></li>
<li><a class="reference internal" href="#local-rank">local_rank</a></li>
<li><a class="reference internal" href="#precision">precision</a></li>
<li><a class="reference internal" href="#trainer">trainer</a></li>
<li><a class="reference internal" href="#prepare-data-per-node">prepare_data_per_node</a></li>
<li><a class="reference internal" href="#automatic-optimization">automatic_optimization</a></li>
<li><a class="reference internal" href="#example-input-array">example_input_array</a></li>
<li><a class="reference internal" href="#truncated-bptt-steps">truncated_bptt_steps</a></li>
</ul>
</li>
<li><a class="reference internal" href="#hooks">Hooks</a><ul>
<li><a class="reference internal" href="#backward">backward</a></li>
<li><a class="reference internal" href="#on-before-backward">on_before_backward</a></li>
<li><a class="reference internal" href="#on-after-backward">on_after_backward</a></li>
<li><a class="reference internal" href="#on-before-zero-grad">on_before_zero_grad</a></li>
<li><a class="reference internal" href="#on-fit-start">on_fit_start</a></li>
<li><a class="reference internal" href="#on-fit-end">on_fit_end</a></li>
<li><a class="reference internal" href="#on-load-checkpoint">on_load_checkpoint</a></li>
<li><a class="reference internal" href="#on-save-checkpoint">on_save_checkpoint</a></li>
<li><a class="reference internal" href="#load-from-checkpoint">load_from_checkpoint</a></li>
<li><a class="reference internal" href="#on-hpc-save">on_hpc_save</a></li>
<li><a class="reference internal" href="#on-hpc-load">on_hpc_load</a></li>
<li><a class="reference internal" href="#on-train-start">on_train_start</a></li>
<li><a class="reference internal" href="#on-train-end">on_train_end</a></li>
<li><a class="reference internal" href="#on-validation-start">on_validation_start</a></li>
<li><a class="reference internal" href="#on-validation-end">on_validation_end</a></li>
<li><a class="reference internal" href="#on-test-batch-start">on_test_batch_start</a></li>
<li><a class="reference internal" href="#on-test-batch-end">on_test_batch_end</a></li>
<li><a class="reference internal" href="#on-test-epoch-start">on_test_epoch_start</a></li>
<li><a class="reference internal" href="#on-test-epoch-end">on_test_epoch_end</a></li>
<li><a class="reference internal" href="#on-test-start">on_test_start</a></li>
<li><a class="reference internal" href="#on-test-end">on_test_end</a></li>
<li><a class="reference internal" href="#on-predict-batch-start">on_predict_batch_start</a></li>
<li><a class="reference internal" href="#on-predict-batch-end">on_predict_batch_end</a></li>
<li><a class="reference internal" href="#on-predict-epoch-start">on_predict_epoch_start</a></li>
<li><a class="reference internal" href="#on-predict-epoch-end">on_predict_epoch_end</a></li>
<li><a class="reference internal" href="#on-predict-start">on_predict_start</a></li>
<li><a class="reference internal" href="#on-predict-end">on_predict_end</a></li>
<li><a class="reference internal" href="#on-train-batch-start">on_train_batch_start</a></li>
<li><a class="reference internal" href="#on-train-batch-end">on_train_batch_end</a></li>
<li><a class="reference internal" href="#on-train-epoch-start">on_train_epoch_start</a></li>
<li><a class="reference internal" href="#on-train-epoch-end">on_train_epoch_end</a></li>
<li><a class="reference internal" href="#on-validation-batch-start">on_validation_batch_start</a></li>
<li><a class="reference internal" href="#on-validation-batch-end">on_validation_batch_end</a></li>
<li><a class="reference internal" href="#on-validation-epoch-start">on_validation_epoch_start</a></li>
<li><a class="reference internal" href="#on-validation-epoch-end">on_validation_epoch_end</a></li>
<li><a class="reference internal" href="#on-post-move-to-device">on_post_move_to_device</a></li>
<li><a class="reference internal" href="#configure-sharded-model">configure_sharded_model</a></li>
<li><a class="reference internal" href="#on-validation-model-eval">on_validation_model_eval</a></li>
<li><a class="reference internal" href="#on-validation-model-train">on_validation_model_train</a></li>
<li><a class="reference internal" href="#on-test-model-eval">on_test_model_eval</a></li>
<li><a class="reference internal" href="#on-test-model-train">on_test_model_train</a></li>
<li><a class="reference internal" href="#on-before-optimizer-step">on_before_optimizer_step</a></li>
<li><a class="reference internal" href="#configure-gradient-clipping">configure_gradient_clipping</a></li>
<li><a class="reference internal" href="#optimizer-step">optimizer_step</a></li>
<li><a class="reference internal" href="#optimizer-zero-grad">optimizer_zero_grad</a></li>
<li><a class="reference internal" href="#prepare-data">prepare_data</a></li>
<li><a class="reference internal" href="#setup">setup</a></li>
<li><a class="reference internal" href="#tbptt-split-batch">tbptt_split_batch</a></li>
<li><a class="reference internal" href="#teardown">teardown</a></li>
<li><a class="reference internal" href="#train-dataloader">train_dataloader</a></li>
<li><a class="reference internal" href="#val-dataloader">val_dataloader</a></li>
<li><a class="reference internal" href="#test-dataloader">test_dataloader</a></li>
<li><a class="reference internal" href="#predict-dataloader">predict_dataloader</a></li>
<li><a class="reference internal" href="#on-train-dataloader">on_train_dataloader</a></li>
<li><a class="reference internal" href="#on-val-dataloader">on_val_dataloader</a></li>
<li><a class="reference internal" href="#on-test-dataloader">on_test_dataloader</a></li>
<li><a class="reference internal" href="#on-predict-dataloader">on_predict_dataloader</a></li>
<li><a class="reference internal" href="#transfer-batch-to-device">transfer_batch_to_device</a></li>
<li><a class="reference internal" href="#on-before-batch-transfer">on_before_batch_transfer</a></li>
<li><a class="reference internal" href="#on-after-batch-transfer">on_after_batch_transfer</a></li>
<li><a class="reference internal" href="#add-to-queue">add_to_queue</a></li>
<li><a class="reference internal" href="#get-from-queue">get_from_queue</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
         <script src="../_static/jquery.js"></script>
         <script src="../_static/underscore.js"></script>
         <script src="../_static/doctools.js"></script>
         <script src="../_static/clipboard.min.js"></script>
         <script src="../_static/copybutton.js"></script>
         <script src="../_static/copybutton.js"></script>
         <script>let toggleHintShow = 'Click to show';</script>
         <script>let toggleHintHide = 'Click to hide';</script>
         <script>let toggleOpenOnPrint = 'true';</script>
         <script src="../_static/togglebutton.js"></script>
         <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Best practices', 'Optional Extensions', 'Tutorials', 'API References', 'Bolts', 'Examples', 'Partner Domain Frameworks', 'Community'];
</script>



  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <!--
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch-lightning.rtfd.io/en/latest">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch-lightning.readthedocs.io/en/latest/#tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch-lightning.readthedocs.io/en/latest/#community-examples">View Resources</a>
        </div>
        -->
      </div>
    </div>
  </div>

  <!--
  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch-lightning.rtfd.io/en/latest/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch-lightning.rtfd.io/en/latest/">PyTorch</a></li>
            <li><a href="https://pytorch-lightning.readthedocs.io/en/latest/starter/introduction.html">Get Started</a></li>
            <li><a href="https://pytorch-lightning.rtfd.io/en/latest/">Features</a></li>
            <li><a href="">Ecosystem</a></li>
            <li><a href="https://www.pytorchlightning.ai/blog">Blog</a></li>
            <li><a href="https://github.com/PyTorchLightning/pytorch-lightning/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch-lightning.readthedocs.io/en/latest/#community-examples">Resources</a></li>
            <li><a href="https://pytorch-lightning.readthedocs.io/en/latest/#tutorials">Tutorials</a></li>
            <li><a href="https://pytorch-lightning.rtfd.io/en/latest">Docs</a></li>
            <li><a href="https://www.pytorchlightning.ai/community" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/PyTorchLightning/pytorch-lightning/issues" target="_blank">Github Issues</a></li>
            <li><a href="" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Stay Connected</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/PyTorchLightnin" target="_blank" class="twitter"></a>
            <a href="" target="_blank" class="youtube"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>
  -->

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. Read PyTorch Lightning's <a href="https://pytorchlightning.ai/privacy-policy">Privacy Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch-lightning.rtfd.io/en/latest/" aria-label="PyTorch Lightning"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <!-- <li>
            <a href="https://pytorch-lightning.readthedocs.io/en/latest/starter/introduction.html">Get Started</a>
          </li>

          <li>
            <a href="https://www.pytorchlightning.ai/blog">Blog</a>
          </li>

          <li class="resources-mobile-menu-title">
            Ecosystem
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch-lightning.readthedocs.io/en/stable/">PyTorch Lightning</a>
            </li>

            <li>
              <a href="https://torchmetrics.readthedocs.io/en/stable/">TorchMetrics</a>
            </li>

            <li>
              <a href="https://lightning-flash.readthedocs.io/en/stable/">Lightning Flash</a>
            </li>

            <li>
              <a href="https://lightning-transformers.readthedocs.io/en/stable/">Lightning Transformers</a>
            </li>

            <li>
              <a href="https://lightning-bolts.readthedocs.io/en/stable/">Lightning Bolts</a>
            </li>
          </ul> -->

          <!--<li class="resources-mobile-menu-title">
            Resources
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch-lightning.readthedocs.io/en/latest/#community-examples">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch-lightning.rtfd.io/en/latest/">About</a>
            </li>

            <li>
              <a href="">Models (Beta)</a>
            </li>

            <li>
              <a href="https://www.pytorchlightning.ai/community">Community</a>
            </li>

            <li>
              <a href="https://github.com/PyTorchLightning/pytorch-lightning/discussions">Forums</a>
            </li>
          </ul>-->

          <!-- <li>
            <a href="https://github.com/PyTorchLightning/pytorch-lightning">Github</a>
          </li> -->

          <!-- <li>
            <a href="https://www.grid.ai/">Grid.ai</a>
          </li> -->
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>

 </body>
</html>